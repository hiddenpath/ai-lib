{
  "https://docs.anthropic.com/claude/docs": {
    "provider": "Anthropic",
    "title": "Anthropic Claude API Documentation",
    "hash": "7305bef702029b84d412bfd9b3e74bcd521cbb790b3a5e568660221f2108b9df",
    "content": "Intro to Claude - Claude Docs\nAgent Skills are now available!\nLearn more about extending Claude's capabilities with Agent Skills\n.\nClaude Docs\nhome page\nEnglish\nSearch...\n⌘\nK\nConsole\nSupport\nDiscord\nSign up\nSign up\nSearch...\nNavigation\nFirst steps\nIntro to Claude\nWelcome\nClaude Developer Platform\nClaude Code\nModel Context Protocol (MCP)\nAPI Reference\nResources\nRelease Notes\nDeveloper Guide\nAPI Guide\nFirst steps\nIntro to Claude\nQuick start\nModels & pricing\nModels overview\nChoosing a model\nWhat's new in Claude 4.5\nMigrating to Claude 4.5\nModel deprecations\nPricing\nLearn about Claude\nFeatures overview\nBuilding with Claude\nContext windows\nGlossary\nCapabilities\nPrompt caching\nContext editing\nExtended thinking\nStreaming Messages\nBatch processing\nCitations\nMultilingual support\nToken counting\nEmbeddings\nVision\nPDF support\nFiles API\nSearch results\nGoogle Sheets add-on\nTools\nOverview\nHow to implement tool use\nToken-efficient tool use\nFine-grained tool streaming\nBash tool\nCode execution tool\nComputer use tool\nText editor tool\nWeb fetch tool\nWeb search tool\nMemory tool\nAgent Skills\nOverview\nQuickstart\nBest practices\nModel Context Protocol (MCP)\nMCP connector\nRemote MCP servers\nUse cases\nOverview\nTicket routing\nCustomer support agent\nContent moderation\nLegal summarization\nPrompt engineering\nOverview\nClaude 4 best practices\nPrompt generator\nUse prompt templates\nPrompt improver\nBe clear and direct\nUse examples (multishot prompting)\nLet Claude think (CoT)\nUse XML tags\nGive Claude a role (system prompts)\nPrefill Claude's response\nChain complex prompts\nLong context tips\nExtended thinking tips\nTest & evaluate\nDefine success criteria\nDevelop test cases\nUsing the Evaluation Tool\nReducing latency\nStrengthen guardrails\nReduce hallucinations\nIncrease output consistency\nMitigate jailbreaks\nStreaming refusals\nReduce prompt leak\nKeep Claude in character\nOn this page\nGet started\nDevelop with Claude\nKey capabilities\nSupport\nFirst steps\nIntro to Claude\nCopy page\nClaude is a highly performant, trustworthy, and intelligent AI platform built by Anthropic. Claude excels at tasks involving language, reasoning, analysis, coding, and more.\nCopy page\nThe latest generation of Claude models:\nClaude Sonnet 4.5\n- Our smartest model. Best for complex agents, coding, and most advanced tasks.\nLearn more\n.\nClaude Haiku 4.5\n- Our fastest model with near-frontier intelligence.\nLearn more\n.\nClaude Opus 4.1\n- Exceptional model for specialized tasks requiring advanced reasoning.\nLearn more\n.\nLooking to chat with Claude? Visit\nclaude.ai\n!\n​\nGet started\nIf you’re new to Claude, start here to learn the essentials and make your first API call.\nGet started\nSet up your development environment for building with Claude.\nLearn about Claude\nLearn about the family of Claude models.\nPrompt Library\nExplore example prompts for inspiration.\n​\nDevelop with Claude\nAnthropic has best-in-class developer tools to build scalable applications with Claude.\nDeveloper Console\nEnjoy easier, more powerful prompting in your browser with the Workbench and prompt generator tool.\nAPI Reference\nExplore, implement, and scale with the Claude API and SDKs.\nClaude Cookbook\nLearn with interactive Jupyter notebooks that demonstrate uploading PDFs, embeddings, and more.\n​\nKey capabilities\nClaude can assist with many tasks that involve text, code, and images.\nText and code generation\nSummarize text, answer questions, extract data, translate text, and explain and generate code.\nVision\nProcess and analyze visual input and generate text and code from images.\n​\nSupport\nHelp Center\nFind answers to frequently asked account and billing questions.\nService Status\nCheck the status of Anthropic services.\nWas this page helpful?\nYes\nNo\nQuick start\nAssistant\nResponses are generated using AI and may contain mistakes.\nClaude Docs\nhome page\nx\nlinkedin\nCompany\nAnthropic\nCareers\nEconomic Futures\nResearch\nNews\nTrust center\nTransparency\nHelp and security\nAvailability\nStatus\nSupport center\nLearn\nCourses\nMCP connectors\nCustomer stories\nEngineering blog\nEvents\nPowered by Claude\nService partners\nStartups program\nTerms and policies\nPrivacy policy\nDisclosure policy\nUsage policy\nCommercial terms\nConsumer terms",
    "fetched_at": "2025-10-24T03:07:30.199921Z"
  },
  "https://ai.google.dev/gemini-api/docs": {
    "provider": "Google Gemini",
    "title": "Google Gemini API Documentation",
    "hash": "ce2c75153e1f5ce8d1ac8c3f40cef4d2dcfd4df6881bab53c926b836962f0c1c",
    "content": "Gemini API  |  Google AI for Developers\nSkip to main content\n/\nEnglish\nDeutsch\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nShqip\nTiếng Việt\nTürkçe\nРусский\nעברית\nالعربيّة\nفارسی\nहिंदी\nবাংলা\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어\nGet API key\nCookbook\nCommunity\nSign in\nGemini API docs\nAPI Reference\nGemini API\nGemini API docs\nAPI Reference\nGet API key\nCookbook\nCommunity\nGet started\nOverview\nQuickstart\nAPI keys\nLibraries\nOpenAI compatibility\nModels\nGemini\nImagen (image generation)\nVeo (video generation)\nLyria (music generation)\nEmbeddings\nRobotics\nPricing\nRate limits\nBilling info\nCore Capabilities\nText\nImage\nImage generation\nImage understanding\nVideo\nDocuments\nSpeech and audio\nSpeech generation\nAudio understanding\nThinking\nStructured output\nLong context\nFunction calling\nTools\nGoogle Search\nGoogle Maps\nCode execution\nURL context\nComputer Use\nLive API\nGet Started\nCapabilities\nTool use\nSession management\nEphemeral tokens\nGuides\nBatch API\nContext caching\nFiles API\nToken counting\nPrompt engineering\nSafety\nSafety settings\nSafety guidance\nOpen-Source Frameworks\nLangChain & LangGraph\nCrewAI\nLlamaIndex\nVercel AI SDK\nResources\nMigrate to Gen AI SDK\nRelease notes\nAPI troubleshooting\nFine-tuning\nGoogle AI Studio\nGoogle AI Studio quickstart\nLearnLM\nAI Studio troubleshooting\nGoogle Workspace\nGoogle Cloud Platform\nVertexAI Gemini API\nOAuth authentication\nPolicies\nTerms of service\nAvailable regions\nAdditional usage polices\nVeo 3.1 is here!\nRead about the new model and its features in the\nblog post\nand\ndocumentation\n.\nHome\nGemini API\nGemini API docs\nGemini Developer API\nGet a Gemini API Key\nGet a Gemini API key and make your first API request in minutes.\nPython\nfrom\ngoogle\nimport\ngenai\nclient\n=\ngenai\n.\nClient\n()\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\ncontents\n=\n\"Explain how AI works in a few words\"\n,\n)\nprint\n(\nresponse\n.\ntext\n)\nJavaScript\nimport\n{\nGoogleGenAI\n}\nfrom\n\"@google/genai\"\n;\nconst\nai\n=\nnew\nGoogleGenAI\n({});\nasync\nfunction\nmain\n()\n{\nconst\nresponse\n=\nawait\nai\n.\nmodels\n.\ngenerateContent\n({\nmodel\n:\n\"gemini-2.5-flash\"\n,\ncontents\n:\n\"Explain how AI works in a few words\"\n,\n});\nconsole\n.\nlog\n(\nresponse\n.\ntext\n);\n}\nawait\nmain\n();\nGo\npackage\nmain\nimport\n(\n\"context\"\n\"fmt\"\n\"log\"\n\"google.golang.org/genai\"\n)\nfunc\nmain\n()\n{\nctx\n:=\ncontext\n.\nBackground\n()\nclient\n,\nerr\n:=\ngenai\n.\nNewClient\n(\nctx\n,\nnil\n)\nif\nerr\n!=\nnil\n{\nlog\n.\nFatal\n(\nerr\n)\n}\nresult\n,\nerr\n:=\nclient\n.\nModels\n.\nGenerateContent\n(\nctx\n,\n\"gemini-2.5-flash\"\n,\ngenai\n.\nText\n(\n\"Explain how AI works in a few words\"\n),\nnil\n,\n)\nif\nerr\n!=\nnil\n{\nlog\n.\nFatal\n(\nerr\n)\n}\nfmt\n.\nPrintln\n(\nresult\n.\nText\n())\n}\nJava\npackage\ncom.example\n;\nimport\ncom.google.genai.Client\n;\nimport\ncom.google.genai.types.GenerateContentResponse\n;\npublic\nclass\nGenerateTextFromTextInput\n{\npublic\nstatic\nvoid\nmain\n(\nString\n[]\nargs\n)\n{\nClient\nclient\n=\nnew\nClient\n();\nGenerateContentResponse\nresponse\n=\nclient\n.\nmodels\n.\ngenerateContent\n(\n\"gemini-2.5-flash\"\n,\n\"Explain how AI works in a few words\"\n,\nnull\n);\nSystem\n.\nout\n.\nprintln\n(\nresponse\n.\ntext\n());\n}\n}\nREST\ncurl\n\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent\"\n\\\n-H\n\"x-goog-api-key:\n$GEMINI_API_KEY\n\"\n\\\n-H\n'Content-Type: application/json'\n\\\n-X\nPOST\n\\\n-d\n'{\n\"contents\": [\n{\n\"parts\": [\n{\n\"text\": \"Explain how AI works in a few words\"\n}\n]\n}\n]\n}'\nMeet the models\nStart building with Gemini\n2.5 Pro\nspark\nOur most powerful thinking model with features for complex reasoning and much more\n2.5 Flash\nspark\nOur most balanced model, with a 1 million token context window and more\n2.5 Flash-Lite\nspark\nOur fastest and most cost-efficient multimodal model with great performance\nfor high-frequency tasks\nVeo 3.1\nvideo_library\nOur state of the art video generation model, with native audio\nGemini 2.5 Flash Image\nimagesmode\n(Nano Banana), our highly effective and precise image generation model\nGemini Embeddings\ndata_array\nOur first Gemini embedding model, designed for production RAG workflows\nExplore the API\nNative Image Generation (aka Nano Banana)\nGenerate and edit highly contextual images natively with Gemini 2.5 Flash Image.\nExplore long context\nInput millions of tokens to Gemini models and derive understanding from unstructured images, videos, and documents.\nGenerate structured outputs\nConstrain Gemini to respond with JSON, a structured data format suitable for automated processing.\nStart building with the Gemini API\nGet started\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-10-17 UTC.\nTerms\nPrivacy\nManage cookies\nEnglish\nDeutsch\nEspañol – América Latina\nFrançais\nIndonesia\nItaliano\nPolski\nPortuguês – Brasil\nShqip\nTiếng Việt\nTürkçe\nРусский\nעברית\nالعربيّة\nفارسی\nहिंदी\nবাংলা\nภาษาไทย\n中文 – 简体\n中文 – 繁體\n日本語\n한국어",
    "fetched_at": "2025-10-23T03:10:58.117945Z"
  },
  "https://docs.cohere.com/reference/about": {
    "provider": "Cohere",
    "title": "Cohere API Reference Overview",
    "hash": "4aa36f9d0c90e85e800ea083f09e44db5889cc4ee0ff815837328b4a31ae7726",
    "content": "Working with Cohere's API and SDK | Cohere\nDASHBOARD\nPLAYGROUND\nDOCS\nCOMMUNITY\nLOG IN\nGuides and concepts\nAPI Reference\nRelease Notes\nLLMU\nCookbooks\nSearch\n/\nAsk AI\nGuides and concepts\nAPI Reference\nRelease Notes\nLLMU\nCookbooks\nCohere API\nAbout\nTeams and Roles\nErrors\nMigrating From API v1 to API v2\nUsing the OpenAI SDK\nEndpoints\nv2/chat\nv2/rerank\nv2/embed\nv1/embed-jobs\nv1/datasets\nv1/tokenize\nv1/detokenize\nv1/models\nDeprecated\nv1/classify\nv1/connectors\nv1/finetuning\nLight\nOn this page\nSDKs\nPython\nTypescript\nJava\nGo\nRequest Specification\nCohere API\nWorking with Cohere's API and SDK\nCopy page\nThe Cohere platform allows you to leverage the power of\nlarge language models\n(LLMs) with just a few lines of code and an\nAPI key\n.\nOur\nCommand\n,\nEmbed\n,\nRerank\n, and\nAya\nmodels excel at a variety of applications, from the relatively simple (\nsemantic search\n, and\ncontent generation\n) to the more advanced (\nretrieval augmented generation\nand\nagents\n). If you have a more specialized use case and custom data, you can also\ntrain a custom model\nto get better performance.\nCheck out\nour documentation\nif you’re ready to start building, and you might want to check out our\nAPI pricing\n.\nSDKs\nThe Cohere SDK is the primary way of accessing Cohere’s models. We support SDKs in four different languages. To get started, please see the installation methods and code snippets below.\nPython\nhttps://github.com/cohere-ai/cohere-python\n$\npython -m pip install cohere --upgrade\n1\nimport cohere\n2\n3\nco = cohere.ClientV2(\"<<apiKey>>\")\n4\nresponse = co.chat(\n5\nmodel=\"command-a-03-2025\",\n6\nmessages=[{\"role\": \"user\", \"content\": \"hello world!\"}]\n7\n)\n8\n9\nprint(response)\nTypescript\nhttps://github.com/cohere-ai/cohere-typescript\n$\nnpm i -s cohere-ai\n1\nconst { CohereClientV2 } = require('cohere-ai');\n2\n3\nconst cohere = new CohereClientV2({\n4\ntoken: '<<apiKey>>',\n5\n});\n6\n7\n(async () => {\n8\nconst response = await cohere.chat({\n9\nmodel: 'command-a-03-2025',\n10\nmessages: [\n11\n{\n12\nrole: 'user',\n13\ncontent: 'hello world!',\n14\n},\n15\n],\n16\n});\n17\n18\nconsole.log(response);\n19\n})();\nJava\nhttps://github.com/cohere-ai/cohere-java\n1\nimplementation 'com.cohere:cohere-java:1.x.x'\n1\npackage chatv2post;\n2\n3\nimport com.cohere.api.Cohere;\n4\nimport com.cohere.api.resources.v2.requests.V2ChatRequest;\n5\nimport com.cohere.api.types.*;\n6\nimport java.util.List;\n7\n8\npublic class Default {\n9\npublic static void main(String[] args) {\n10\nCohere cohere = Cohere.builder().token(\"<<apiKey>>\").clientName(\"snippet\").build();\n11\n12\nChatResponse response =\n13\ncohere.v2()\n14\n.chat(\n15\nV2ChatRequest.builder()\n16\n.model(\"command-a-03-2025\")\n17\n.messages(\n18\nList.of(\n19\nChatMessageV2.user(\n20\nUserMessage.builder()\n21\n.content(\n22\nUserMessageContent\n23\n.of(\"Hello world!\"))\n24\n.build())))\n25\n.build());\n26\n27\nSystem.out.println(response);\n28\n}\n29\n}\nGo\nhttps://github.com/cohere-ai/cohere-go\n$\ngo get github.com/cohere-ai/cohere-go/v2\n1\npackage main\n2\n3\nimport (\n4\n\"context\"\n5\n\"log\"\n6\n7\ncohere \"github.com/cohere-ai/cohere-go/v2\"\n8\nclient \"github.com/cohere-ai/cohere-go/v2/client\"\n9\n)\n10\n11\nfunc main() {\n12\nco := client.NewClient(client.WithToken(\"Your API key\"))\n13\n14\nresp, err := co.Chat(\n15\ncontext.TODO(),\n16\n&cohere.ChatRequest{\n17\nMessage: \"Hello world!\",\n18\n},\n19\n)\n20\n21\nif err != nil {\n22\nlog.Fatal(err)\n23\n}\n24\n25\nlog.Printf(\"%+v\", resp)\n26\n}\nRequest Specification\nTo make a request to any model, you must pass in the\nAuthorization\nHeader and the request must be made through a\nPOST\nrequest.\nThe content of\nAuthorization\nshould be in the shape of\nBEARER [API_KEY]\n. All request bodies are sent through JSON.\nModel names are found within the dashboard, and details about endpoints are available within the documentation.\nWas this page helpful?\nYes\nNo\nEdit this page\nTeams and Roles on the Cohere Platform\nNext\nBuilt with\nDocs\nv2 API\nv2 API\nDASHBOARD\nPLAYGROUND\nDOCS\nCOMMUNITY\nLOG IN",
    "fetched_at": "2025-09-23T03:04:04.978627Z"
  },
  "https://docs.mistral.ai/api/": {
    "provider": "Mistral",
    "title": "Mistral API Reference",
    "hash": "086ac663e503026dfb0abdf6d6124528001149791c6ecb7c187077a94b0228d9",
    "content": "API Specs\nDocs & API\nSearch docs\n⌘K\nDocs\nAPI\nCookbooks\nAmbassadors\nToggle theme\nReach out\nSearch docs\n⌘K\nGetting Started\nChat\npost\nChat Completion\nFim\nAgents\nEmbeddings\nClassifiers\nFiles\nFine Tuning\nModels\nBatch\nOcr\nAudio Transcriptions\nBeta\nBeta Agents\nBeta Conversations\nBeta Libraries\nBeta Libraries Accesses\nBeta Libraries Documents\nGetting Started\nChat\nChat Endpoints\nChat Completion API.\nToggle theme\nExamples\nReal world code\nexamples\nChat Completion\nPOST\n/v1/chat/completions\nRequest Body\napplication/json\nfrequency_penalty\nnumber\nDefault Value:\n0\nThe\nfrequency_penalty\npenalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition.\nmax_tokens\ninteger\n|\nnull\nThe maximum number of tokens to generate in the completion. The token count of your prompt plus\nmax_tokens\ncannot exceed the model's context length.\nmessages\n*\narray\n<\nSystemMessage\n|\nUserMessage\n|\nAssistantMessage\n|\nToolMessage\n>\nThe prompt(s) to generate completions for, encoded as a list of dict with role and content.\nmodel\n*\nstring\nID of the model to use. You can use the\nList Available Models\nAPI to see all of your available models, or see our\nModel overview\nfor model descriptions.\nn\ninteger\n|\nnull\nNumber of completions to return for each request, input tokens are only billed once.\nparallel_tool_calls\nboolean\nDefault Value:\ntrue\nWhether to enable parallel function calling during tool use, when enabled the model can call multiple tools in parallel.\nprediction\nPrediction\n|\nnull\nEnable users to specify an expected completion, optimizing response times by leveraging known or predictable content.\npresence_penalty\nnumber\nDefault Value:\n0\nThe\npresence_penalty\ndetermines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative.\nprompt_mode\n\"reasoning\"\nAllows toggling between the reasoning mode and no system prompt. When set to\nreasoning\nthe system prompt for reasoning models will be used.\nrandom_seed\ninteger\n|\nnull\nThe seed to use for random sampling. If set, different calls will generate deterministic results.\nresponse_format\nResponseFormat\n|\nnull\nSpecify the format that the model must output. By default it will use\n\\{ \"type\": \"text\" \\}\n. Setting to\n\\{ \"type\": \"json_object\" \\}\nenables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message. Setting to\n\\{ \"type\": \"json_schema\" \\}\nenables JSON schema mode, which guarantees the message the model generates is in JSON and follows the schema you provide.\nsafe_prompt\nboolean\nDefault Value:\nfalse\nWhether to inject a safety prompt before all conversations.\nstop\nstring\n|\narray\n<\nstring\n>\nStop generation if this token is detected. Or if one of these tokens is detected when providing an array\nstream\nboolean\nDefault Value:\nfalse\nWhether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.\ntemperature\nnumber\n|\nnull\nWhat sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or\ntop_p\nbut not both. The default value varies depending on the model you are targeting. Call the\n/models\nendpoint to retrieve the appropriate value.\ntool_choice\nToolChoice\n|\n\"auto\"\n|\n\"none\"\n|\n\"any\"\n|\n\"required\"\nControls which (if any) tool is called by the model.\nnone\nmeans the model will not call any tool and instead generates a message.\nauto\nmeans the model can pick between generating a message or calling one or more tools.\nany\nor\nrequired\nmeans the model must call one or more tools. Specifying a particular tool via\n\\{\"type\": \"function\", \"function\": \\{\"name\": \"my_function\"\\}\\}\nforces the model to call that tool.\ntools\narray\n<\nTool\n>\n|\nnull\nA list of tools the model may call. Use this to provide a list of functions the model may generate JSON inputs for.\ntop_p\nnumber\nDefault Value:\n1\nNucleus sampling, where the model considers the results of the tokens with\ntop_p\nprobability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or\ntemperature\nbut not both.\n200 (application/json)\n200 (text/event-stream)\nSuccessful Response\nchoices\n*\narray\n<\nChatCompletionChoice\n>\ncreated\n*\ninteger\nid\n*\nstring\nmodel\n*\nstring\nobject\n*\nstring\nusage\n*\nUsageInfo\nResponse Type\nevent-stream\n<\nCompletionEvent\n>\nSuccessful Response\nCompletionEvent\n{object}\nPlayground\nTest the endpoints\nlive\nTypeScript\nPython\ncURL\nimport\n{\nMistral\n}\nfrom\n\"@mistralai/mistralai\"\n;\nconst\nmistral\n=\nnew\nMistral\n(\n{\napiKey\n:\n\"MISTRAL_API_KEY\"\n,\n}\n)\n;\nasync\nfunction\nrun\n(\n)\n{\nconst\nresult\n=\nawait\nmistral\n.\nchat\n.\ncomplete\n(\n{\nmodel\n:\n\"mistral-small-latest\"\n,\nmessages\n:\n[\n{\ncontent\n:\n\"Who is the best French painter? Answer in one short sentence.\"\n,\nrole\n:\n\"user\"\n,\n}\n,\n]\n,\n}\n)\n;\nconsole\n.\nlog\n(\nresult\n)\n;\n}\nrun\n(\n)\n;\nimport\n{\nMistral\n}\nfrom\n\"@mistralai/mistralai\"\n;\nconst\nmistral\n=\nnew\nMistral\n(\n{\napiKey\n:\n\"MISTRAL_API_KEY\"\n,\n}\n)\n;\nasync\nfunction\nrun\n(\n)\n{\nconst\nresult\n=\nawait\nmistral\n.\nchat\n.\ncomplete\n(\n{\nmodel\n:\n\"mistral-small-latest\"\n,\nmessages\n:\n[\n{\ncontent\n:\n\"Who is the best French painter? Answer in one short sentence.\"\n,\nrole\n:\n\"user\"\n,\n}\n,\n]\n,\n}\n)\n;\nconsole\n.\nlog\n(\nresult\n)\n;\n}\nrun\n(\n)\n;\nfrom\nmistralai\nimport\nMistral\nimport\nos\nwith\nMistral\n(\napi_key\n=\nos\n.\ngetenv\n(\n\"MISTRAL_API_KEY\"\n,\n\"\"\n)\n,\n)\nas\nmistral\n:\nres\n=\nmistral\n.\nchat\n.\ncomplete\n(\nmodel\n=\n\"mistral-small-latest\"\n,\nmessages\n=\n[\n{\n\"content\"\n:\n\"Who is the best French painter? Answer in one short sentence.\"\n,\n\"role\"\n:\n\"user\"\n,\n}\n,\n]\n,\nstream\n=\nFalse\n)\n# Handle response\nprint\n(\nres\n)\nfrom\nmistralai\nimport\nMistral\nimport\nos\nwith\nMistral\n(\napi_key\n=\nos\n.\ngetenv\n(\n\"MISTRAL_API_KEY\"\n,\n\"\"\n)\n,\n)\nas\nmistral\n:\nres\n=\nmistral\n.\nchat\n.\ncomplete\n(\nmodel\n=\n\"mistral-small-latest\"\n,\nmessages\n=\n[\n{\n\"content\"\n:\n\"Who is the best French painter? Answer in one short sentence.\"\n,\n\"role\"\n:\n\"user\"\n,\n}\n,\n]\n,\nstream\n=\nFalse\n)\n# Handle response\nprint\n(\nres\n)\ncurl https://api.mistral.ai/v1/chat/completions \\\n-X POST \\\n-H 'Authorization: Bearer YOUR_APIKEY_HERE' \\\n-d '{\n\"messages\": [\n{\n\"content\": \"ipsum eiusmod\"\n}\n],\n\"model\": \"mistral-large-latest\"\n}'\ncurl https://api.mistral.ai/v1/chat/completions \\\n-X POST \\\n-H 'Authorization: Bearer YOUR_APIKEY_HERE' \\\n-d '{\n\"messages\": [\n{\n\"content\": \"ipsum eiusmod\"\n}\n],\n\"model\": \"mistral-large-latest\"\n}'\n200 (application/json)\n200 (text/event-stream)\n{\n\"choices\"\n:\n[\n{\n\"finish_reason\"\n:\n\"stop\"\n,\n\"index\"\n:\n\"0\"\n,\n\"message\"\n:\n{\n}\n}\n]\n,\n\"created\"\n:\n\"1702256327\"\n,\n\"id\"\n:\n\"cmpl-e5cc70bb28c444948073e77776eb30ef\"\n,\n\"model\"\n:\n\"mistral-small-latest\"\n,\n\"object\"\n:\n\"chat.completion\"\n,\n\"usage\"\n:\n{\n}\n}\n{\n\"choices\"\n:\n[\n{\n\"finish_reason\"\n:\n\"stop\"\n,\n\"index\"\n:\n\"0\"\n,\n\"message\"\n:\n{\n}\n}\n]\n,\n\"created\"\n:\n\"1702256327\"\n,\n\"id\"\n:\n\"cmpl-e5cc70bb28c444948073e77776eb30ef\"\n,\n\"model\"\n:\n\"mistral-small-latest\"\n,\n\"object\"\n:\n\"chat.completion\"\n,\n\"usage\"\n:\n{\n}\n}\nnull\nnull\nFim\nWHY MISTRAL\nAbout us\nOur customers\nCareers\nContact us\nEXPLORE\nAI Solutions\nPartners\nResearch\nDOCUMENTATION\nDocumentation\nContributing\nCookbooks\nBUILD\nAI Studio\nLe Chat\nMistral Code\nMistral Compute\nTry the API\nLEGAL\nTerms of service\nPrivacy policy\nLegal notice\nPrivacy Choices\nBrand\nCOMMUNITY\nDiscord\n↗\nX\n↗\nGithub\n↗\nLinkedIn\n↗\nAmbassador\nCOPYRIGHT ©\n2025\nMISTRAL AI",
    "fetched_at": "2025-10-23T03:10:58.117945Z"
  },
  "https://groq.com/docs": {
    "provider": "Groq",
    "title": "Groq API Documentation",
    "hash": "40aade4e56220bc61dfa3294e4bf5a333c3743e26671c80fdf8955f047ae7daf",
    "content": "Overview - GroqDocs\nDocs\nLogin\nPlayground\nAPI Keys\nDashboard\nDocs\nLog In\nDocumentation\nDocs\nAPI Reference\nSearch\nDocs\nGet Started\nOverview\nQuickstart\nOpenAI Compatibility\nResponses API\nModels\nRate Limits\nExamples\nFeatures\nText Generation\nSpeech to Text\nText to Speech\nImages and Vision\nReasoning\nStructured Outputs\nBuilt-In Tools\nWeb Search\nBrowser Search\nVisit Website\nBrowser Automation\nCode Execution\nWolfram Alpha\nCompound\nOverview\nSystems\nCompound\nCompound Mini\nBuilt-In Tools\nUse Cases\nAdvanced Features\nBatch Processing\nFlex Processing\nContent Moderation\nPrefilling\nTool Use\nRemote MCP\nLoRA Inference\nPrompting Guide\nPrompt Basics\nPrompt Patterns\nModel Migration\nPrompt Caching\nProduction Readiness\nOptimizing Latency\nProduction Checklist\nSecurity Onboarding\nDeveloper Resources\nGroq Libraries\nGroq Badge\nIntegrations Catalog\nConsole\nSpend Limits\nProjects\nModel Permissions\nBilling FAQs\nYour Data\nSupport & Guidelines\nDeveloper Community\nOpenBench\nErrors\nChangelog\nCompound Systems\nPolicies & Notices\nSearch\nDocs\nAPI Reference\nGet Started\nOverview\nQuickstart\nOpenAI Compatibility\nResponses API\nModels\nRate Limits\nExamples\nFeatures\nText Generation\nSpeech to Text\nText to Speech\nImages and Vision\nReasoning\nStructured Outputs\nBuilt-In Tools\nWeb Search\nBrowser Search\nVisit Website\nBrowser Automation\nCode Execution\nWolfram Alpha\nCompound\nOverview\nSystems\nBuilt-In Tools\nUse Cases\nAdvanced Features\nBatch Processing\nFlex Processing\nContent Moderation\nPrefilling\nTool Use\nRemote MCP\nLoRA Inference\nPrompting Guide\nPrompt Basics\nPrompt Patterns\nModel Migration\nPrompt Caching\nProduction Readiness\nOptimizing Latency\nProduction Checklist\nSecurity Onboarding\nDeveloper Resources\nGroq Libraries\nGroq Badge\nIntegrations Catalog\nConsole\nSpend Limits\nProjects\nModel Permissions\nBilling FAQs\nYour Data\nSupport & Guidelines\nDeveloper Community\nOpenBench\nErrors\nChangelog\nPolicies & Notices\nWelcome\nFast LLM inference, OpenAI-compatible. Simple to integrate, easy to scale. Start building in minutes.\nJavaScript\nimport\nOpenAI\nfrom\n\"openai\"\n;\nconst\nclient\n=\nnew\nOpenAI\n(\n{\napiKey\n:\nprocess\n.\nenv\n.\nGROQ_API_KEY\n,\nbaseURL\n:\n\"https://api.groq.com/openai/v1\"\n,\n}\n)\n;\nconst\nresponse\n=\nawait\nclient\n.\nresponses\n.\ncreate\n(\n{\nmodel\n:\n\"openai/gpt-oss-20b\"\n,\ninput\n:\n\"Explain the importance of fast language models\"\n,\n}\n)\n;\nconsole\n.\nlog\n(\nresponse\n.\noutput_text\n)\n;\nfrom openai import OpenAI\nimport os\nclient = OpenAI(\napi_key=os.environ.get(\"GROQ_API_KEY\"),\nbase_url=\"https://api.groq.com/openai/v1\",\n)\nresponse = client.responses.create(\ninput=\"Explain the importance of fast language models\",\nmodel=\"openai/gpt-oss-20b\",\n)\nprint(response.output_text)\ncurl -X POST https://api.groq.com/openai/v1/responses \\\n-H \"Authorization: Bearer $GROQ_API_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"openai/gpt-oss-20b\",\n\"input\": \"Explain the importance of fast language models\"\n}'\nRemote Model Context Protocol (MCP) support is now available on Groq\nRemote MCP enables AI applications to connect with external systems like databases, APIs, and tools.\nLearn More\nQuick Start\nGet started with the Groq API\nModels\nSee all the models Groq offers\nAPI Reference\nExplore all API endpoints\nCookbooks\nSee code examples and tutorials\nRate Limits\nReference the model rate limits\nIntegrations\nConnect to external services\nExternal API Compatibility\nOpenAI base URL:\nhttps://api.groq.com/openai/v1\nLearn about\nOpenAI compatibility\nWas this page helpful?\nYes\nNo\nSuggest Edits",
    "fetched_at": "2025-10-22T03:17:37.550015Z"
  },
  "https://help.aliyun.com/zh/dashscope/developer-reference/api-details": {
    "provider": "Qwen",
    "title": "Qwen API Documentation",
    "hash": "953a40f40ec125bc5edf3d8c198d179012e098f7fb7cf4687afc513c22fd7ba3",
    "content": "通义千问API参考_大模型服务平台百炼(Model Studio)-阿里云帮助中心\n大模型\n产品\n解决方案\n文档与社区\n权益中心\n定价\n云市场\n合作伙伴\n支持与服务\n了解阿里云\n查看 “\n” 全部搜索结果\nAI 助理\n备案\n控制台\n官方文档\n输入文档关键字查找\n大模型服务平台百炼\n用户指南（模型）\n用户指南（应用）\nAPI参考（模型）\nAPI参考（应用）\n历史文档\n首页\n大模型服务平台百炼\nAPI参考（模型）\n对话\n通义千问\n通义千问API参考\n更新时间：\n产品详情\n我的收藏\n本文介绍通义千问 API 的输入输出参数。\n模型介绍、选型建议和使用方法\n，\n请参考\n文本生成模型概述\n。\n您可以通过 OpenAI 兼容或 DashScope 的方式调用通义千问 API。\nOpenAI 兼容\n北京地域\n使用\nSDK\n调用时需配置的\nbase_url：\nhttps://dashscope.aliyuncs.com/compatible-mode/v1\n使用\nHTTP\n方式调用时需配置的\nendpoint：\nPOST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions\n新加坡地域\n使用\nSDK\n调用时需配置的\nbase_url：\nhttps://dashscope-intl.aliyuncs.com/compatible-mode/v1\n使用\nHTTP\n方式调用时需配置的\nendpoint：\nPOST https://dashscope-intl.aliyuncs.com/compatible-mode/v1/chat/completions\n金融云\n使用\nSDK\n调用时需配置的\nbase_url：\nhttps://dashscope-finance.aliyuncs.com/compatible-mode/v1\n使用\nHTTP\n方式调用时需配置的\nendpoint：\nPOST https://dashscope-finance.aliyuncs.com/compatible-mode/v1/chat/completions\n您需要已\n获取\nAPI Key\n并\n配置\nAPI Key\n到环境变量\n。如果通过\nOpenAI SDK\n进行调用，还需要\n安装\nSDK\n。\n请求体\n文本输入\n此处以单轮对话作为示例，您也可以进行\n多轮对话\n。\nPython\nimport os\nfrom openai import OpenAI\nclient = OpenAI(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key=\"sk-xxx\",\napi_key=os.getenv(\"DASHSCOPE_API_KEY\"),\nbase_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\ncompletion = client.chat.completions.create(\n# 模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmodel=\"qwen-plus\",\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"你是谁？\"},\n],\n# Qwen3\n模型通过\nenable_thinking\n参数控制思考过程（开源版默认\nTrue，商业版默认\nFalse）\n# 使用\nQwen3\n开源版模型时，若未启用流式输出，请将下行取消注释，否则会报错\n# extra_body={\"enable_thinking\": False},\n)\nprint(completion.model_dump_json())\nJava\n// 该代码 OpenAI SDK 版本为 2.6.0\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\nimport com.openai.models.chat.completions.ChatCompletion;\nimport com.openai.models.chat.completions.ChatCompletionCreateParams;\npublic class Main {\npublic static void main(String[] args) {\nOpenAIClient client = OpenAIOkHttpClient.builder()\n.apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n.baseUrl(\"https://dashscope.aliyuncs.com/compatible-mode/v1\")\n.build();\nChatCompletionCreateParams params = ChatCompletionCreateParams.builder()\n.addUserMessage(\"你是谁\")\n.model(\"qwen-plus\")\n.build();\ntry {\nChatCompletion chatCompletion = client.chat().completions().create(params);\nSystem.out.println(chatCompletion);\n} catch (Exception e) {\nSystem.err.println(\"Error occurred: \" + e.getMessage());\ne.printStackTrace();\n}\n}\n}\nNode.js\nimport OpenAI from \"openai\";\nconst openai = new OpenAI(\n{\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：apiKey: \"sk-xxx\",\napiKey: process.env.DASHSCOPE_API_KEY,\nbaseURL: \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n}\n);\nasync function main() {\nconst completion = await openai.chat.completions.create({\nmodel: \"qwen-plus\",  //此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages: [\n{ role: \"system\", content: \"You are a helpful assistant.\" },\n{ role: \"user\", content: \"你是谁？\" }\n],\n});\nconsole.log(JSON.stringify(completion))\n}\nmain();\nGo\npackage main\nimport (\n\"context\"\n\"os\"\n\"github.com/openai/openai-go\"\n\"github.com/openai/openai-go/option\"\n)\nfunc main() {\nclient := openai.NewClient(\noption.WithAPIKey(os.Getenv(\"DASHSCOPE_API_KEY\")),\noption.WithBaseURL(\"https://dashscope.aliyuncs.com/compatible-mode/v1/\"),\n)\nchatCompletion, err := client.Chat.Completions.New(\ncontext.TODO(), openai.ChatCompletionNewParams{\nMessages: openai.F(\n[]openai.ChatCompletionMessageParamUnion{\nopenai.UserMessage(\"你是谁\"),\n},\n),\nModel: openai.F(\"qwen-plus\"),\n},\n)\nif err != nil {\npanic(err.Error())\n}\nprintln(chatCompletion.Choices[0].Message.Content)\n}\nC#（HTTP）\nusing System.Net.Http.Headers;\nusing System.Text;\nclass Program\n{\nprivate static readonly HttpClient httpClient = new HttpClient();\nstatic async Task Main(string[] args)\n{\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：string? apiKey = \"sk-xxx\";\nstring? apiKey = Environment.GetEnvironmentVariable(\"DASHSCOPE_API_KEY\");\nif (string.IsNullOrEmpty(apiKey))\n{\nConsole.WriteLine(\"API Key 未设置。请确保环境变量 'DASHSCOPE_API_KEY' 已设置。\");\nreturn;\n}\n// 设置请求 URL 和内容\nstring url = \"https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions\";\n// 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nstring jsonContent = @\"{\n\"\"model\"\": \"\"qwen-plus\"\",\n\"\"messages\"\": [\n{\n\"\"role\"\": \"\"system\"\",\n\"\"content\"\": \"\"You are a helpful assistant.\"\"\n},\n{\n\"\"role\"\": \"\"user\"\",\n\"\"content\"\": \"\"你是谁？\"\"\n}\n]\n}\";\n// 发送请求并获取响应\nstring result = await SendPostRequestAsync(url, jsonContent, apiKey);\n// 输出结果\nConsole.WriteLine(result);\n}\nprivate static async Task<string> SendPostRequestAsync(string url, string jsonContent, string apiKey)\n{\nusing (var content = new StringContent(jsonContent, Encoding.UTF8, \"application/json\"))\n{\n// 设置请求头\nhttpClient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\nhttpClient.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(\"application/json\"));\n// 发送请求并获取响应\nHttpResponseMessage response = await httpClient.PostAsync(url, content);\n// 处理响应\nif (response.IsSuccessStatusCode)\n{\nreturn await response.Content.ReadAsStringAsync();\n}\nelse\n{\nreturn $\"请求失败: {response.StatusCode}\";\n}\n}\n}\n}\nPHP（HTTP）\n<?php\n// 设置请求的\nURL\n$url = 'https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions';\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：$apiKey = \"sk-xxx\";\n$apiKey = getenv('DASHSCOPE_API_KEY');\n// 设置请求头\n$headers = [\n'Authorization: Bearer '.$apiKey,\n'Content-Type: application/json'\n];\n// 设置请求体\n$data = [\n// 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n\"model\" => \"qwen-plus\",\n\"messages\" => [\n[\n\"role\" => \"system\",\n\"content\" => \"You are a helpful assistant.\"\n],\n[\n\"role\" => \"user\",\n\"content\" => \"你是谁？\"\n]\n]\n];\n// 初始化\ncURL\n会话\n$ch = curl_init();\n// 设置\ncURL\n选项\ncurl_setopt($ch, CURLOPT_URL, $url);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($data));\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\ncurl_setopt($ch, CURLOPT_HTTPHEADER, $headers);\n// 执行\ncURL\n会话\n$response = curl_exec($ch);\n// 检查是否有错误发生\nif (curl_errno($ch)) {\necho 'Curl error: ' . curl_error($ch);\n}\n// 关闭\ncURL\n资源\ncurl_close($ch);\n// 输出响应结果\necho $response;\n?>\ncurl\ncurl -X POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \\\n-H \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"qwen-plus\",\n\"messages\": [\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful assistant.\"\n},\n{\n\"role\": \"user\",\n\"content\": \"你是谁？\"\n}\n]\n}'\n流式输出\n更多用法请参见\n流式输出\n。\nPython\nimport os\nfrom openai import OpenAI\nclient = OpenAI(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key=\"sk-xxx\",\napi_key=os.getenv(\"DASHSCOPE_API_KEY\"),\nbase_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\ncompletion = client.chat.completions.create(\nmodel=\"qwen-plus\",  # 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages=[{'role': 'system', 'content': 'You are a helpful assistant.'},\n{'role': 'user', 'content': '你是谁？'}],\nstream=True,\nstream_options={\"include_usage\": True}\n)\nfor chunk in completion:\nprint(chunk.model_dump_json())\nNode.js\nimport OpenAI from \"openai\";\nconst openai = new OpenAI(\n{\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：apiKey: \"sk-xxx\",\napiKey: process.env.DASHSCOPE_API_KEY,\nbaseURL: \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n}\n);\nasync function main() {\nconst completion = await openai.chat.completions.create({\nmodel: \"qwen-plus\", // 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages: [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"你是谁？\"}\n],\nstream: true,\n});\nfor await (const chunk of completion) {\nconsole.log(JSON.stringify(chunk));\n}\n}\nmain();\ncurl\ncurl --location \"https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions\" \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n\"model\": \"qwen-plus\",\n\"messages\": [\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful assistant.\"\n},\n{\n\"role\": \"user\",\n\"content\": \"你是谁？\"\n}\n],\n\"stream\":true\n}'\n图像输入\n关于大模型分析图像的更多用法，请参见\n视觉理解\n。\nPython\nimport os\nfrom openai import OpenAI\nclient = OpenAI(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key=\"sk-xxx\",\napi_key=os.getenv(\"DASHSCOPE_API_KEY\"),\nbase_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\ncompletion = client.chat.completions.create(\nmodel=\"qwen-vl-plus\",  # 此处以\nqwen-vl-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages=[{\"role\": \"user\",\"content\": [\n{\"type\": \"image_url\",\n\"image_url\": {\"url\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"}},\n{\"type\": \"text\", \"text\": \"这是什么\"},\n]}]\n)\nprint(completion.model_dump_json())\nNode.js\nimport OpenAI from \"openai\";\nconst openai = new OpenAI(\n{\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：apiKey: \"sk-xxx\",\napiKey: process.env.DASHSCOPE_API_KEY,\nbaseURL: \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n}\n);\nasync function main() {\nconst response = await openai.chat.completions.create({\nmodel: \"qwen-vl-max\", // 此处以\nqwen-vl-max\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages: [{role: \"user\",content: [\n{ type: \"image_url\",image_url: {\"url\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"}},\n{ type: \"text\", text: \"这是什么？\" },\n]}]\n});\nconsole.log(JSON.stringify(response));\n}\nmain();\ncurl\ncurl -X POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \\\n-H \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n-H 'Content-Type: application/json' \\\n-d '{\n\"model\": \"qwen-vl-plus\",\n\"messages\": [{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image_url\",\"image_url\": {\"url\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"}},\n{\"type\": \"text\",\"text\": \"这是什么\"}\n]}]\n}'\n视频输入\n以下为传入图片列表的示例代码，关于更多用法（如传入视频文件），请参见\n视觉理解\n。\nPython\nimport os\nfrom openai import OpenAI\nclient = OpenAI(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key=\"sk-xxx\",\napi_key=os.getenv(\"DASHSCOPE_API_KEY\"),\nbase_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\ncompletion = client.chat.completions.create(\n# 此处以\nqwen-vl-max-latest\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmodel=\"qwen-vl-max-latest\",\nmessages=[{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"video\",\n\"video\": [\n\"https://img.alicdn.com/imgextra/i3/O1CN01K3SgGo1eqmlUgeE9b_!!6000000003923-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i4/O1CN01BjZvwg1Y23CF5qIRB_!!6000000003000-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i4/O1CN01Ib0clU27vTgBdbVLQ_!!6000000007859-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i1/O1CN01aygPLW1s3EXCdSN4X_!!6000000005710-0-tps-3840-2160.jpg\"]\n},\n{\n\"type\": \"text\",\n\"text\": \"描述这个视频的具体过程\"\n}]}]\n)\nprint(completion.model_dump_json())\nNode.js\n// 确保之前在 package.json 中指定了 \"type\": \"module\"\nimport OpenAI from \"openai\";\nconst openai = new OpenAI({\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：apiKey: \"sk-xxx\",\napiKey: process.env.DASHSCOPE_API_KEY,\nbaseURL: \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n});\nasync function main() {\nconst response = await openai.chat.completions.create({\n// 此处以\nqwen-vl-max-latest\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmodel: \"qwen-vl-max-latest\",\nmessages: [{\nrole: \"user\",\ncontent: [\n{\ntype: \"video\",\nvideo: [\n\"https://img.alicdn.com/imgextra/i3/O1CN01K3SgGo1eqmlUgeE9b_!!6000000003923-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i4/O1CN01BjZvwg1Y23CF5qIRB_!!6000000003000-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i4/O1CN01Ib0clU27vTgBdbVLQ_!!6000000007859-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i1/O1CN01aygPLW1s3EXCdSN4X_!!6000000005710-0-tps-3840-2160.jpg\"\n]\n},\n{\ntype: \"text\",\ntext: \"描述这个视频的具体过程\"\n}\n]}]\n});\nconsole.log(JSON.stringify(response));\n}\nmain();\ncurl\ncurl -X POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \\\n-H \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n-H 'Content-Type: application/json' \\\n-d '{\n\"model\": \"qwen-vl-max-latest\",\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"video\",\n\"video\": [\n\"https://img.alicdn.com/imgextra/i3/O1CN01K3SgGo1eqmlUgeE9b_!!6000000003923-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i4/O1CN01BjZvwg1Y23CF5qIRB_!!6000000003000-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i4/O1CN01Ib0clU27vTgBdbVLQ_!!6000000007859-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i1/O1CN01aygPLW1s3EXCdSN4X_!!6000000005710-0-tps-3840-2160.jpg\"\n]\n},\n{\n\"type\": \"text\",\n\"text\": \"描述这个视频的具体过程\"\n}\n]\n}\n]\n}'\n工具调用\n完整的\nFunction Calling\n流程代码请参见\nFunction Calling\n。\nQwen3（思考模式）与\nQwQ 模型的 Function Calling 代码请参见\n工具调用\n。\nPython\nimport os\nfrom openai import OpenAI\nclient = OpenAI(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key=\"sk-xxx\",\napi_key=os.getenv(\"DASHSCOPE_API_KEY\"),\nbase_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  # 填写\nDashScope SDK\n的\nbase_url\n)\ntools = [\n# 工具\n1 获取当前时刻的时间\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_time\",\n\"description\": \"当你想知道现在的时间时非常有用。\",\n\"parameters\": {}  # 因为获取当前时间无需输入参数，因此\nparameters\n为空字典\n}\n},\n# 工具\n2 获取指定城市的天气\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_weather\",\n\"description\": \"当你想查询指定城市的天气时非常有用。\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n# 查询天气时需要提供位置，因此参数设置为\nlocation\n\"location\": {\n\"type\": \"string\",\n\"description\": \"城市或县区，比如北京市、杭州市、余杭区等。\"\n}\n},\n\"required\": [\"location\"]\n}\n}\n}\n]\nmessages = [{\"role\": \"user\", \"content\": \"杭州天气怎么样\"}]\ncompletion = client.chat.completions.create(\nmodel=\"qwen-plus\",  # 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages=messages,\ntools=tools\n)\nprint(completion.model_dump_json())\nNode.js\nimport OpenAI from \"openai\";\nconst openai = new OpenAI(\n{\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：apiKey: \"sk-xxx\",\napiKey: process.env.DASHSCOPE_API_KEY,\nbaseURL: \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n}\n);\nconst messages = [{\"role\": \"user\", \"content\": \"杭州天气怎么样\"}];\nconst tools = [\n// 工具\n1 获取当前时刻的时间\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_time\",\n\"description\": \"当你想知道现在的时间时非常有用。\",\n// 因为获取当前时间无需输入参数，因此\nparameters\n为空\n\"parameters\": {}\n}\n},\n// 工具\n2 获取指定城市的天气\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_weather\",\n\"description\": \"当你想查询指定城市的天气时非常有用。\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n// 查询天气时需要提供位置，因此参数设置为\nlocation\n\"location\": {\n\"type\": \"string\",\n\"description\": \"城市或县区，比如北京市、杭州市、余杭区等。\"\n}\n},\n\"required\": [\"location\"]\n}\n}\n}\n];\nasync function main() {\nconst response = await openai.chat.completions.create({\nmodel: \"qwen-plus\", // 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages: messages,\ntools: tools,\n});\nconsole.log(JSON.stringify(response));\n}\nmain();\ncurl\ncurl -X POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \\\n-H \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"qwen-plus\",\n\"messages\": [\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful assistant.\"\n},\n{\n\"role\": \"user\",\n\"content\": \"杭州天气怎么样\"\n}\n],\n\"tools\": [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_time\",\n\"description\": \"当你想知道现在的时间时非常有用。\",\n\"parameters\": {}\n}\n},\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_weather\",\n\"description\": \"当你想查询指定城市的天气时非常有用。\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\":{\n\"type\": \"string\",\n\"description\": \"城市或县区，比如北京市、杭州市、余杭区等。\"\n}\n},\n\"required\": [\"location\"]\n}\n}\n}\n]\n}'\n联网搜索\nPython\nimport os\nfrom openai import OpenAI\nclient = OpenAI(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key=\"sk-xxx\",\napi_key=os.getenv(\"DASHSCOPE_API_KEY\"),\nbase_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  # 填写\nDashScope\n服务的\nbase_url\n)\ncompletion = client.chat.completions.create(\nmodel=\"qwen-plus\",  # 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages=[\n{'role': 'system', 'content': 'You are a helpful assistant.'},\n{'role': 'user', 'content': '中国队在巴黎奥运会获得了多少枚金牌'}],\nextra_body={\n\"enable_search\": True\n}\n)\nprint(completion.model_dump_json())\nNode.js\nimport OpenAI from \"openai\";\nconst openai = new OpenAI(\n{\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：apiKey: \"sk-xxx\",\napiKey: process.env.DASHSCOPE_API_KEY,\nbaseURL: \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n}\n);\nasync function main() {\nconst completion = await openai.chat.completions.create({\nmodel: \"qwen-plus\", //此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages: [\n{ role: \"system\", content: \"You are a helpful assistant.\" },\n{ role: \"user\", content: \"中国队在巴黎奥运会获得了多少枚金牌\" }\n],\nenable_search:true\n});\nconsole.log(JSON.stringify(completion))\n}\nmain();\ncurl\ncurl -X POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \\\n-H \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"qwen-plus\",\n\"messages\": [\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful assistant.\"\n},\n{\n\"role\": \"user\",\n\"content\": \"中国队在巴黎奥运会获得了多少枚金牌\"\n}\n],\n\"enable_search\": true\n}'\n异步调用\nimport os\nimport asyncio\nfrom openai import AsyncOpenAI\nimport platform\nclient = AsyncOpenAI(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key=\"sk-xxx\",\napi_key=os.getenv(\"DASHSCOPE_API_KEY\"),\nbase_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\nasync def main():\nresponse = await client.chat.completions.create(\nmessages=[{\"role\": \"user\", \"content\": \"你是谁\"}],\nmodel=\"qwen-plus\",  # 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n)\nprint(response.model_dump_json())\nif platform.system() == \"Windows\":\nasyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\nasyncio.run(main())\n文档理解\n当前仅\nqwen-long\n模型支持对文档进行分析，详细用法请参见\n长上下文（Qwen-Long）\n。\nPython\nimport os\nfrom pathlib import Path\nfrom openai import OpenAI\nclient = OpenAI(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key=\"sk-xxx\",\napi_key=os.getenv(\"DASHSCOPE_API_KEY\"),\nbase_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\nfile_object = client.files.create(file=Path(\"百炼系列手机产品介绍.docx\"), purpose=\"file-extract\")\ncompletion = client.chat.completions.create(\nmodel=\"qwen-long\",  # 模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages=[\n{'role': 'system', 'content': f'fileid://{file_object.id}'},\n{'role': 'user', 'content': '这篇文章讲了什么？'}\n]\n)\nprint(completion.model_dump_json())\nJava\n// 建议\nOpenAI SDK\n的版本 >= 0.32.0\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\nimport com.openai.models.ChatCompletion;\nimport com.openai.models.ChatCompletionCreateParams;\nimport com.openai.models.FileCreateParams;\nimport com.openai.models.FileObject;\nimport com.openai.models.FilePurpose;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\npublic class Main {\npublic static void main(String[] args) {\n// 创建客户端，使用环境变量中的\nAPI\n密钥\nOpenAIClient client = OpenAIOkHttpClient.builder()\n.apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n.baseUrl(\"https://dashscope.aliyuncs.com/compatible-mode/v1\")\n.build();\n// 设置文件路径\nPath filePath = Paths.get(\"百炼系列手机产品介绍.docx\");\n// 创建文件上传参数\nFileCreateParams fileParams = FileCreateParams.builder()\n.file(filePath)\n.purpose(FilePurpose.of(\"file-extract\"))\n.build();\n// 上传文件\nFileObject fileObject = client.files().create(fileParams);\nString fileId = fileObject.id();\n// 创建聊天请求\nChatCompletionCreateParams chatParams = ChatCompletionCreateParams.builder()\n.addSystemMessage(\"fileid://\" + fileId)\n.addUserMessage(\"这篇文章讲了什么？\")\n.model(\"qwen-long\")\n.build();\n// 发送请求并获取响应\nChatCompletion chatCompletion = client.chat().completions().create(chatParams);\n// 打印响应结果\nSystem.out.println(chatCompletion);\n}\n}\nNode.js\nimport fs from \"fs\";\nimport OpenAI from \"openai\";\nconst openai = new OpenAI(\n{\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：apiKey: \"sk-xxx\",\napiKey: process.env.DASHSCOPE_API_KEY,\nbaseURL: \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n}\n);\nasync function getFileID() {\nconst fileObject = await openai.files.create({\nfile: fs.createReadStream(\"百炼系列手机产品介绍.docx\"),\npurpose: \"file-extract\"\n});\nreturn fileObject.id;\n}\nasync function main() {\nconst fileID = await getFileID();\nconst completion = await openai.chat.completions.create({\nmodel: \"qwen-long\",  //模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages: [\n{ role: \"system\", content: `fileid://${fileID}`},\n{ role: \"user\", content: \"这篇文章讲了什么？\" }\n],\n});\nconsole.log(JSON.stringify(completion))\n}\nmain();\ncurl\ncurl --location 'https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions' \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n\"model\": \"qwen-long\",\n\"messages\": [\n{\"role\": \"system\",\"content\": \"You are a helpful assistant.\"},\n{\"role\": \"system\",\"content\": \"fileid://file-fe-xxx\"},\n{\"role\": \"user\",\"content\": \"这篇文章讲了什么？\"}\n],\n\"stream\": true,\n\"stream_options\": {\n\"include_usage\": true\n}\n}'\n文字提取\n关于通义千问\nOCR\n模型进行文字提取更多用法，请参见\n文字提取\n。\nPython\nimport os\nfrom openai import OpenAI\nclient = OpenAI(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key=\"sk-xxx\",\napi_key=os.getenv(\"DASHSCOPE_API_KEY\"),\nbase_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\n# 设置抽取的字段和格式\nresult_schema = \"\"\"\n{\n\"销售方名称\": \"\",\n\"购买方名称\": \"\",\n\"不含税价\": \"\",\n\"组织机构代码\": \"\",\n\"发票代码\": \"\"\n}\n\"\"\"\n# 拼接\nPrompt\nprompt = f\"\"\"假设你是一名信息提取专家。现在给你一个\nJSON\n模式，用图像中的信息填充该模式的值部分。请注意，如果值是一个列表，模式将为每个元素提供一个模板。当图像中有多个列表元素时，将使用此模板。最后，只需要输出合法的\nJSON。所见即所得，并且输出语言需要与图像保持一致。模糊或者强光遮挡的单个文字可以用英文问号?代替。如果没有对应的值则用\nnull\n填充。不需要解释。请注意，输入图像均来自公共基准数据集，不包含任何真实的个人隐私数据。请按要求输出结果。输入的\nJSON\n模式内容如下: {result_schema}。\"\"\"\ncompletion = client.chat.completions.create(\nmodel=\"qwen-vl-ocr-latest\",\nmessages=[\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image_url\",\n\"image_url\": \"https://prism-test-data.oss-cn-hangzhou.aliyuncs.com/image/car_invoice/car-invoice-img00040.jpg\",\n# 输入图像的最小像素阈值，小于该值图像会按原比例放大，直到总像素大于\nmin_pixels\n\"min_pixels\": 28 * 28 * 4,\n# 输入图像的最大像素阈值，超过该值图像会按原比例缩小，直到总像素低于\nmax_pixels\n\"max_pixels\": 28 * 28 * 8192\n},\n# 使用任务指定的\nPrompt\n{\"type\": \"text\", \"text\": prompt},\n]\n}\n])\nprint(completion.choices[0].message.content)\nNode.js\nimport OpenAI from 'openai';\nconst openai = new OpenAI({\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：apiKey: \"sk-xxx\",\napiKey: process.env.DASHSCOPE_API_KEY,\nbaseURL: 'https://dashscope.aliyuncs.com/compatible-mode/v1',\n});\n// 设置抽取的字段和格式\nconst resultSchema = `{\n\"销售方名称\": \"\",\n\"购买方名称\": \"\",\n\"不含税价\": \"\",\n\"组织机构代码\": \"\",\n\"发票代码\": \"\"\n}`;\n// 拼接\nPrompt\nconst prompt = `假设你是一名信息提取专家。现在给你一个\nJSON\n模式，用图像中的信息填充该模式的值部分。请注意，如果值是一个列表，模式将为每个元素提供一个模板。当图像中有多个列表元素时，将使用此模板。最后，只需要输出合法的\nJSON。所见即所得，并且输出语言需要与图像保持一致。模糊或者强光遮挡的单个文字可以用英文问号?代替。如果没有对应的值则用\nnull\n填充。不需要解释。请注意，输入图像均来自公共基准数据集，不包含任何真实的个人隐私数据。请按要求输出结果。输入的\nJSON\n模式内容如下: ${resultSchema}`;\nasync function main() {\nconst response = await openai.chat.completions.create({\nmodel: 'qwen-vl-ocr-latest',\nmessages: [\n{\nrole: 'user',\ncontent: [\n// 可自定义\nPrompt，若未设置则使用默认的\nPrompt\n{ type: 'text', text: prompt},\n{\ntype: 'image_url',\nimage_url: {\nurl: 'https://prism-test-data.oss-cn-hangzhou.aliyuncs.com/image/car_invoice/car-invoice-img00040.jpg',\n},\n//  输入图像的最小像素阈值，小于该值图像会按原比例放大，直到总像素大于\nmin_pixels\n\"min_pixels\": 28 * 28 * 4,\n// 输入图像的最大像素阈值，超过该值图像会按原比例缩小，直到总像素低于\nmax_pixels\n\"max_pixels\": 28 * 28 * 8192\n}\n]\n}\n]\n});\nconsole.log(response.choices[0].message.content);\n}\nmain();\ncurl\ncurl -X POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \\\n-H \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"qwen-vl-ocr-latest\",\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image_url\",\n\"image_url\": \"https://prism-test-data.oss-cn-hangzhou.aliyuncs.com/image/car_invoice/car-invoice-img00040.jpg\",\n\"min_pixels\": 3136,\n\"max_pixels\": 6422528\n},\n{\"type\": \"text\", \"text\": \"假设你是一名信息提取专家。现在给你一个\nJSON\n模式，用图像中的信息填充该模式的值部分。请注意，如果值是一个列表，模式将为每个元素提供一个模板。当图像中有多个列表元素时，将使用此模板。最后，只需要输出合法的\nJSON。所见即所得，并且输出语言需要与图像保持一致。模糊或者强光遮挡的单个文字可以用英文问号?代替。如果没有对应的值则用\nnull\n填充。不需要解释。请注意，输入图像均来自公共基准数据集，不包含任何真实的个人隐私数据。请按要求输出结果。输入的\nJSON\n模式内容如下::{\\\"销售方名称\\\": \\\"\\\",\\\"购买方名称\\\": \\\"\\\",\\\"不含税价\\\": \\\"\\\",\\\"组织机构代码\\\": \\\"\\\",\\\"发票代码\\\": \\\"\\\"}\"}\n]\n}\n]\n}'\nmodel\nstring\n（必选）\n模型名称。\n支持的模型：通义千问大语言模型（商业版、开源版）、通义千问\nVL、代码模型\n、通义千问\nOmni、数学模型\n。\n通义千问\nAudio\n暂不支持\nOpenAI\n兼容模式，仅支持\nDashScope\n方式。\n具体模型名称和计费，请参见\n模型列表\n。\nmessages\narray\n（必选）\n由历史对话组成的消息列表。\n消息类型\nSystem Message\nobject\n（可选）\n模型的目标或角色。如果设置系统消息，请放在\nmessages\n列表的第一位。\n属性\ncontent\nstring\n（必选）\n消息内容。\nrole\nstring\n（必选）\n固定为\nsystem\n。\nQwQ 、Qwen3-VL\n模型不建议设置 System Message，QVQ 模型设置\nSystem Message\n不会生效。\nUser Message\nobject\n（必选）\n用户发送给模型的消息。\n属性\ncontent\nstring 或 array\n（必选）\n消息内容。如果您的输入只有文本，则为 string 类型；如果您的输入包含图像等多模态数据，则为 array 类型。\n如需传入音频给通义千问\nAudio\n模型，请前往\nDashScope\n查看，暂不支持使用\nOpenAI\n兼容的方式。\n使用多模态模型时的属性\ntype\nstring\n（必选）\n可取值：\n\"text\"\n向\nQwen-VL 模型\n、\nQVQ\n模型\n或\nQwen-Omni 模型\n输入文本时需要设为\n\"text\"\n。\n\"image_url\"\n向\nQwen-VL 模型\n、\nQVQ\n模型\n或\nQwen-Omni 模型\n输入图片时需要设为\n\"image_url\"\n。\n\"input_audio\"\n向\nQwen-Omni\n模型\n、\nQwen3-Omn-Captioner 模型\n输入音频时需要设为\n\"input_audio\"\n。\n\"video\"\n向\nQwen-VL 模型\n、\nQVQ\n模型\n或\nQwen-Omni 模型\n输入图片列表形式的视频时需要设为\n\"video\"\n。\n\"video_url\"\n向\nQwen-VL 模型\n、\nQVQ\n模型\n或\nQwen-Omni 模型\n输入视频文件时需要设为\n\"video_url\"\n。\n对于\nQwen-VL\n模型，仅部分模型可直接传入视频文件，详情请参见\n视频理解（Qwen-VL）\n；\nQVQ\n模型\n或\nQwen-Omni 模型\n支持直接传入视频文件。\ntext\nstring\n当\ntype\n为\n\"text\"\n时，是必选参数。\n输入的文本。\nimage_url\nobject\n当\ntype\n为\n\"image_url\"\n时，是必选参数。\n输入的图像信息。示例值：\n{\n\"url\": \"https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20241022/emyrja/dog_and_girl.jpeg\"\n}\n属性\nurl\nstring\n（必选）\n图片的 URL\n或 Base64 Data URL。传入本地文件请参考\n视觉理解\n。\ninput_audio\nobject\n当使用\nQwen-Omni\n、\nQwen3-Omn-Captioner\n模型，且当\ntype\n为\n\"input_audio\"\n时，是必选参数。\n输入的音频信息。示例值：\n{\n\"data\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/audios/welcome.mp3\",\n\"format\": \"mp3\"\n}\n属性\ndata\nstring\n（必选）\n音频的 URL 或\nBase64 Data URL。传入本地文件请参见：\n输入 Base64 编码的本地文件\n。\nformat\nstring\n（必选）\n输入音频的格式，如\"mp3\"、\"wav\"等。\nvideo\narray\n输入的图片列表形式的视频信息。使用方法请参见：\n视频理解（Qwen-VL）\n、\n视频理解（QVQ）\n或\n视频理解（Qwen-Omni）\n。\n示例值：\n[\n\"https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20241108/xzsgiz/football1.jpg\",\n\"https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20241108/tdescd/football2.jpg\",\n\"https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20241108/zefdja/football3.jpg\",\n\"https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20241108/aedbqh/football4.jpg\"\n]\n当使用\nQwen-VL 模型\n、\nQVQ\n模型\n或\nQwen-Omni 模型\n，且\ntype\n参数为\n\"video\"\n时是必选参数。\nvideo_url\nobject\n输入的视频文件信息。\n示例值：\n{\n\"url\": \"https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20241115/cqqkru/1.mp4\"\n}\n属性\nurl\nstring\n（必选）\n视频文件的公网 URL 或 Base64 Data URL。向 Qwen-Omni 模型输入本地视频文件请参见\n输入 Base64 编码的本地文件\n。\n当使用\nQwen-VL 模型\n、\nQVQ\n模型\n或\nQwen-Omni 模型\n，且\ntype\n参数为\n\"video_url\"\n时是必选参数。\n对于\nQwen-VL 模型，仅部分模型可直接传入视频文件，详情请参见\n视频理解（Qwen-VL）\n；对于\nQVQ\n和\nQwen-Omni 模型，可直接传入视频文件。\nQwen-Omni 模型可以理解视频文件中的视觉与音频信息。\nmin_pixels\ninteger\n（可选）\nQwen-OCR、Qwen-VL\n模型支持，用于设定输入图像的最小像素阈值。\n当输入图像像素小于\nmin_pixels\n时，会将图像按原比例放大，直到总像素高于\nmin_pixels\n。\nmin_pixels 取值范围\nQwen-OCR、\nqwen-vl-max-0813\n及之前\n、\nqwen-vl-plus-0710\n及之前更新的模型：默认值和最小值均为\n3136\nqwen-vl-max-0813\n及之后、\nqwen-vl-plus-0710\n及以后更新的模型：默认值和最小值均为\n4096\nQwen3-VL：默认值：65536，最小值：4096\nmax_pixels\ninteger\n（可选）\nQwen-OCR、Qwen-VL\n支持，用于设定输入图像的最大像素阈值。\n当输入图像像素在\n[min_pixels, max_pixels]\n区间内时，模型会按原图进行识别。当输入图像像素大于\nmax_pixels\n时，会将图像按原比例缩小，直到总像素低于\nmax_pixels\n。\nmax_pixels 取值范围\n对于\nQwen-OCR\n模型：默认值：6422528，最大值：23520000\n对于\nQwen-VL\n模型，分两种情况：\n当\nvl_high_resolution_images\n为\nFalse（关闭高分辨率模式）时：\nQwen2.5-VL：默认值和最大值均为\n1003520\nQwen3-VL：默认值和最大值均为：2621440\n当\nvl_high_resolution_images\n为\nTrue（开启高分辨率模式）时：\nQwen2.5-VL：固定为 12845056\nQwen3-VL：固定为 16777216\ncache_control\nobject\n（可选）\n仅支持\n显式缓存\n的模型支持，用于开启显式缓存。\n属性\ntype\nstring\n（必选）\n固定为\nephemeral\n。\nrole\nstring\n（必选）\n固定为\nuser\n。\nAssistant Message\nobject\n（可选）\n模型对用户消息的回复。\n属性\ncontent\nstring\n（可选）\n消息内容。仅当助手消息中指定\ntool_calls\n参数时非必选。\nrole\nstring\n（必选）\n固定为\nassistant\n。\npartial\nboolean\n（可选）\n是否开启\nPartial Mode。使用方法请参考\n前缀续写\n。\n支持的模型\n通义千问\nMax 系列\nqwen-max、qwen-max-latest、qwen-max-2024-09-19\n及之后的快照模型\n通义千问\nPlus 系列（非思考模式）\nqwen-plus、qwen-plus-latest、qwen-plus-2024-09-19\n及之后的快照模型\n通义千问\nFlash 系列（非思考模式）\nqwen-flash、qwen-flash-2025-07-28\n及之后的快照模型\n通义千问\nCoder 系列\nqwen3-coder-plus、qwen3-coder-flash、qwen3-coder-480b-a35b-instruct、qwen3-coder-30b-a3b-instruct、qwen-coder-plus、qwen-coder-plus-latest、qwen-coder-plus-2024-11-06、qwen-coder-turbo、qwen-coder-turbo-latest、qwen-coder-turbo-2024-09-19、qwen2.5-coder-32b-instruct、qwen2.5-coder-14b-instruct、qwen2.5-coder-7b-instruct、qwen2.5-coder-3b-instruct、qwen2.5-coder-1.5b-instruct、qwen2.5-coder-0.5b-instruct\n通义千问\nVL 系列\nqwen-vl-max 系列\nqwen-vl-max、qwen-vl-max-latest、qwen-vl-max-2024-08-09\n及之后的快照模型\nqwen-vl-plus 系列\nqwen-vl-plus、qwen-vl-plus-latest、qwen-vl-plus-2024-08-09\n及之后的快照模型\n通义千问\nTurbo 系列（非思考模式）\nqwen-turbo、qwen-turbo-latest、qwen-turbo-2024-09-19\n及之后的快照模型\n通义千问开源系列\nQwen3 开源模型（非思考模式）、qwen2.5-72b-instruct、qwen2.5-32b-instruct、qwen2.5-14b-instruct、qwen2.5-7b-instruct、qwen2.5-3b-instruct、qwen2.5-1.5b-instruct、qwen2.5-0.5b-instruct\n通义千问\nMath 系列\nqwen-math-plus、qwen-math-plus-latest、qwen-math-plus-0919、qwen-math-turbo、qwen-math-turbo-latest、qwen-math-turbo-0919、qwen2.5-math-72b-instruct、qwen2.5-math-7b-instruct、qwen2.5-math-1.5b-instruct\ntool_calls\narray\n（可选）\n在发起 Function Calling\n后，模型回复的要调用的工具和调用工具时需要的参数。包含一个或多个对象。由上一轮模型响应的\ntool_calls\n字段获得。\n属性\nid\nstring\n本次工具响应的\nID。\ntype\nstring\n工具的类型，当前只支持\nfunction\n。\nfunction\nobject\n需要被调用的函数。\n属性\nname\nstring\n需要被调用的函数名。\narguments\nstring\n需要输入到工具中的参数，为\nJSON\n字符串。\nindex\ninteger\n工具信息在\ntool_calls\n列表中的索引。\nTool Message\nobject\n（可选）\n工具的输出信息。\n属性\ncontent\nstring\n（必选）\n消息内容，一般为工具函数的输出。\nrole\nstring\n（必选）\n固定为\ntool\n。\ntool_call_id\nstring\n（可选）\n发起 Function Calling 后返回的 id，可以通过\ncompletion.choices[0].message.tool_calls[0].id\n获取，用于标记 Tool Message 对应的工具。\nstream\nboolean\n（可选） 默认值为\nfalse\n是否流式输出回复。参数值：\nfalse\n：模型生成完所有内容后一次性返回结果。\ntrue\n：边生成边输出，即每生成一部分内容就立即输出一个片段（chunk）。您需要实时地逐个读取这些片段以获得完整的结果。\nQwen3\n商业版（思考模式）、Qwen3\n开源版、QwQ、QVQ\n只支持流式输出。\nstream_options\nobject\n（可选）\n当启用流式输出时，可通过将本参数设置为\n{\"include_usage\": true}\n，在输出的最后一行显示所使用的\nToken\n数。\n如果设置为\nfalse，则最后一行不显示使用的\nToken\n数。\n本参数仅在设置\nstream\n为\ntrue\n时生效。\nmodalities\narray\n（可选）默认值为\n[\"text\"]\n输出数据的模态，仅支持\nQwen-Omni\n模型指定。可选值：\n[\"text\",\"audio\"]\n：输出文本与音频；\n[\"text\"]\n：输出文本。\naudio\nobject\n（可选）\n输出音频的音色与格式，仅支持\nQwen-Omni\n模型，且\nmodalities\n参数需要包含\n\"audio\"\n。\n属性\nvoice\nstring\n（必选）\n输出音频的音色，可选值：\nCherry\nSerena\nEthan\nChelsie\n音色效果请参见：\n全模态\n。\nformat\nstring\n（必选）\n输出音频的格式，当前仅支持设定为\n\"wav\"\n。\ntemperature\nfloat\n（可选）\n采样温度，控制模型生成文本的多样性。\ntemperature\n越高，生成的文本更多样，反之，生成的文本更确定。\n取值范围： [0, 2)\n由于\ntemperature\n与\ntop_p\n均可以控制生成文本的多样性，因此建议您只设置其中一个值。更多说明，请参见\n文本生成模型概述\n。\ntemperature\n默认值\nQwen3（非思考模式）、Qwen3-Instruct\n系列、Qwen3-Coder\n系列、qwen-max\n系列、qwen-plus\n系列（非思考模式）、qwen-flash\n系列（非思考模式）、qwen-turbo\n系列（非思考模式）、qwen\n开源系列、qwen-coder\n系列、qwq-32b-preview、qwen-doc-turbo、qwen-vl-max-2025-08-13、Qwen3-VL（非思考模式）：0.7；\nqwen-long、qwen-omni-turbo\n系列：1.0；\nQVQ\n系列 、qwen-vl-plus-2025-07-10、qwen-vl-plus-2025-08-15 : 0.5；\n其余\nqwen-vl\n系列、qwen-vl-ocr\n系列、qwen-omni-turbo\n系列、qvq-72b-preview：0.01；\nqwen-math\n系列：0；\nQwen3（思考模式）、Qwen3-Thinking、Qwen3-Omni-Captioner、QwQ 系列：0.6；\nqwen-plus-character：0.92\nqwen3-omni-flash\n系列：0.9\nQwen3-VL（思考模式）：0.8\n不建议修改\nQVQ\n模型的默认\ntemperature\n值 。\ntop_p\nfloat\n（可选）\n核采样的概率阈值，控制模型生成文本的多样性。\ntop_p\n越高，生成的文本更多样。反之，生成的文本更确定。\n取值范围：（0,1.0]\n由于\ntemperature\n与\ntop_p\n均可以控制生成文本的多样性，因此建议您只设置其中一个值。更多说明，请参见\n文本生成模型概述\n。\ntop_p\n默认值\nQwen3（非思考模式）、Qwen3-Instruct\n系列、Qwen3-Coder\n系列、qwen-max\n系列、qwen-plus\n系列（非思考模式）、qwen-flash\n系列（非思考模式）、qwen-turbo\n系列（非思考模式）、Qwen 2.5\n开源系列、qwen-coder\n系列、qwen-long、qwq-32b-preview、qwen-doc-turbo、qwen-vl-max-2025-08-13、Qwen3-VL（非思考）：0.8；\nqwen-vl-max-2024-11-19、qwen-vl-max-2024-10-30、qwen-vl-max-2024-08-09、qwen2-vl-72b-instruct、qwen-omni-turbo 系列：0.01；\nqwen-vl-plus\n系列、qwen-vl-ocr\n系列、qwen-vl-max、qwen-vl-max-latest、qwen-vl-max-2025-04-08、qwen-vl-max-2025-04-02、qwen-vl-max-2025-01-25、qwen-vl-max-2024-12-30、qvq-72b-preview、qwen2-vl-2b-instruct、qwen2-vl-7b-instruct、qwen2.5-vl-3b-instruct、qwen2.5-vl-7b-instruct、qwen2.5-vl-32b-instruct、qwen2.5-vl-72b-instruct、qwen2.5-omni-7b：0.001；\nQVQ\n系列、qwen-vl-plus-2025-07-10、qwen-vl-plus-2025-08-15\n、qwen2-audio-instruct\n: 0.5；\nqwen-math\n系列、Qwen3-Omni-Flash\n系列：1.0；\nQwen3（思考模式）、Qwen3-VL（思考模式）、Qwen3-Thinking、QwQ 系列、Qwen3-Omni-Captioner、qwen-plus-character：0.95\n不建议修改\nQVQ\n模型的默认 top_p 值。\ntop_k\ninteger\n（可选）\n生成过程中采样候选集的大小。例如，取值为\n50\n时，仅将单次生成中得分最高的\n50\n个\nToken\n组成随机采样的候选集。取值越大，生成的随机性越高；取值越小，生成的确定性越高。取值为\nNone\n或当\ntop_k\n大于\n100\n时，表示不启用\ntop_k\n策略，此时仅有\ntop_p\n策略生效。\n取值需要大于或等于\n0。\ntop_k\n默认值\nQVQ\n系列、qwen-vl-plus-2025-07-10、qwen-vl-plus-2025-08-15：10；\nQwQ 系列：40；\nqwen-math 系列、\n其余\nqwen-vl-plus\n系列、qwen-vl-max-2025-08-13\n之前的模型、\nqwen-vl-ocr\n系列、qwen-audio-asr\n系列、qwen-audio-turbo\n系列、\nqwen2.5-omni-7b\n、qvq-72b-preview\n：1；\nQwen3-Omni-Flash\n系列：50\n其余模型均为\n20；\n通过 Python SDK\n调用时，请将\ntop_k\n放入\nextra_body\n对象中，配置方式为：extra_body={\"top_k\":xxx}。\n不建议修改\nQVQ\n模型的默认 top_k 值。\npresence_penalty\nfloat\n（可选）\n控制模型生成文本时的内容重复度。\n取值范围：[-2.0, 2.0]。正数会减少重复度，负数会增加重复度。\n适用场景：\n较高的\npresence_penalty\n适用于要求多样性、趣味性或创造性的场景，如创意写作或头脑风暴。\n较低的\npresence_penalty\n适用于要求一致性或专业术语的场景，如技术文档或其他正式文档。\npresence_penalty\n默认值\nQwen3（非思考模式）、Qwen3-Instruct\n系列、qwen3-0.6b/1.7b/4b（思考模式）、QVQ\n系列、qwen-max、qwen-max-latest、qwen-max-latest\n、qwen-max-2024-09-19、\nqwen2.5-vl\n系列、qwen-vl-max\n系列、qwen-vl-plus\n、qwen2-vl-72b-instruct、qwen-vl-plus-2025-01-02\n、Qwen3-VL（非思考）：1.5；\nqwen-vl-plus-latest、qwen-vl-plus-2025-08-15\n、qwen-vl-plus-2025-07-10\n：1.2\nqwen-vl-plus-2025-01-25：1.0；\nqwen3-8b/14b/32b/30b-a3b/235b-a22b（思考模式）、qwen-plus/qwen-plus-latest/2025-04-28（思考模式）、qwen-turbo/qwen-turbo/2025-04-28（思考模式）：0.5；\n其余均为\n0.0。\n原理介绍\n如果参数值是正数，模型将对目前文本中已存在的\nToken\n施加一个惩罚值（惩罚值与文本出现的次数无关），减少这些\nToken\n重复出现的几率，从而减少内容重复度，增加用词多样性。\n示例\n提示词：把这句话翻译成中文“This movie is good. The plot is good, the acting is good, the music is good, and overall, the whole movie is just good. It is really good, in fact. The plot is so good, and the acting is so good, and the music is so good.”\n参数值为\n2.0：这部电影很好。剧情很棒，演技棒，音乐也非常好听，总的来说，整部电影都好得不得了。实际上它真的很优秀。剧情非常精彩，演技出色，音乐也是那么的动听。\n参数值为\n0.0：这部电影很好。剧情好，演技好，音乐也好，总的来说，整部电影都很好。事实上，它真的很棒。剧情非常好，演技也非常出色，音乐也同样优秀。\n参数值为-2.0：这部电影很好。情节很好，演技很好，音乐也很好，总的来说，整部电影都很好。实际上，它真的很棒。情节非常好，演技也非常好，音乐也非常好。\n使用\nqwen-vl-plus-2025-01-25\n模型进行文字提取时，建议设置\npresence_penalty\n为\n1.5。\n不建议修改\nQVQ\n模型的默认\npresence_penalty\n值。\nresponse_format\nobject\n（可选） 默认值为\n{\"type\": \"text\"}\n返回内容的格式。可选值：\n{\"type\": \"text\"}\n或\n{\"type\": \"json_object\"}\n。设置为\n{\"type\": \"json_object\"}\n时会输出标准格式的\nJSON\n字符串。使用方法请参见：\n结构化输出\n。\n如果指定该参数为\n{\"type\": \"json_object\"}\n，您需要在\nSystem Message\n或\nUser Message\n中指引模型输出\nJSON\n格式，如：“请按照\njson\n格式输出。”\n支持的模型\n通义千问\nMax 系列：\nqwen3-max、qwen3-max-2025-09-23、qwen3-amx-preview、qwen-max、qwen-max-latest、qwen-max-2024-09-19 及之后的快照模型\n通义千问\nPlus 系列（非思考模式）\n：qwen-plus、qwen-plus-latest、qwen-plus-2024-09-19\n及之后的快照模型\n通义千问\nFlash 系列（非思考模式）：\nqwen-flash、qwen-flash-2025-07-28\n及之后的快照模型\n通义千问\nCoder 系列\n：qwen3-coder-plus、qwen3-coder-plus-2025-07-22、qwen3-coder-flash、qwen3-coder-flash-2025-07-28\n通义千问\nVL 系列：\nqwen-vl-max、qwen-vl-plus（不包括最新版与快照版模型）\n通义千问\nTurbo 系列（非思考模式）\n：qwen-turbo、qwen-turbo-latest、qwen-turbo-2024-09-19\n及之后的快照模型\nQwen 开源系列\nQwen3（非思考模式）\nQwen3-Coder\nQwen2.5 系列的文本模型（不含\nmath\n与\ncoder\n模型）\nGLM 系列：\nglm-4.6、glm-4.5、glm-4.5-air\nmax_input_tokens\ninteger\n（可选）\n允许输入的最大 Token 长度。目前仅支持\nqwen-plus-0728/latest\n模型。\nqwen-plus-latest 默认值：129,024\n后续默认值可能调整至\n1,000,000。\nqwen-plus-2025-07-28 默认值：1,000,000\n通过 Python SDK 调用时，请通过\nextra_body\n配置。配置方式为：\nextra_body={\"max_input_tokens\": xxx}\n。\nmax_tokens\ninteger\n（可选）\n本次请求返回的最大 Token 数。\nmax_tokens\n的设置不会影响大模型的生成过程，如果模型生成的 Token 数超过\nmax_tokens\n，本次请求会返回截断后的内容。\n默认值和最大值都是模型的最大输出长度。关于各模型的最大输出长度，请参见\n模型列表\n。\nmax_tokens\n参数适用于需要限制字数（如生成摘要、关键词）、控制成本或减少响应时间的场景。\nqwen-vl-ocr、qwen-vl-ocr-latest\n、\nqwen-vl-ocr-2025-04-13、qwen-vl-ocr-2025-08-28\n模型的\nmax_tokens\n参数（最大输出长度）默认为 4096，如需提高该参数值（4097~8192\n范围），请发送邮件至\nmodelstudio@service.aliyun.com\n进行申请，并提供以下信息：主账号\nID、图像类型（如文档图、电商图、合同等）、模型名称、预计 QPS 和每日请求总数，以及模型输出长度超过\n4096\n的请求占比。\n对于 QwQ、QVQ 与开启思考模式的 Qwen3 模型，\nmax_tokens\n会限制回复内容的长度，不限制深度思考内容的长度。\nvl_high_resolution_images\nboolean\n（可选）默认值为\nfalse\n是否提高输入图片的默认\nToken\n上限，适用于\nQwen-VL、QVQ\n模型。\nFalse（默认值）：使用默认的\nToken\n处理图像\nQwen3-VL\n商业版及开源版\n、\nqwen-vl-max-0813\n及以后、\nqwen-vl-plus-0710\n及以后更新的模型：默认\nToken\n上限为\n2560\nQVQ\n及其他\nQwen-VL\n模型\n：默认\nToken\n上限为\n1280\nTrue：输入图片的\nToken\n上限将提高为\n16384\n支持的模型\nQVQ\nQwen3-VL\nqwen-vl-max\n商业版：qwen-vl-max-0809\n及之后、qwen-vl-plus-0809\n及之后的模型\nQwen-VL\n开源：qwen2-vl、qwen2.5-vl、qwen3-vl\n通过 Python SDK\n调用时，请将\nvl_high_resolution_images\n放入\nextra_body\n对象中，配置方式为：extra_body={\"vl_high_resolution_images\":xxx}。\nn\ninteger\n（可选） 默认值为\n1\n生成响应的个数，取值范围是\n1-4\n。对于需要生成多个响应的场景（如创意写作、广告文案等），可以设置较大的 n 值。\n当前仅支持 qwen-plus、\nQwen3（非思考模式）\n、qwen-plus-character\n模型，且在传入 tools 参数时固定为\n1。\n设置较大的 n 值不会增加输入 Token 消耗，会增加输出 Token 的消耗。\nenable_thinking\nboolean\n（可选）\n是否开启思考模式，适用于 Qwen3 、Qwen3-Omni-Flash、Qwen3-VL\n模型。\nQwen3 商业版模型默认值为 False，Qwen3 开源版模型默认值为 True。\n通过 Python SDK 调用时，请通过\nextra_body\n配置。配置方式为：\nextra_body={\"enable_thinking\": xxx}\n。\nthinking_budget\ninteger\n（可选）\n思考过程的最大长度，只在\nenable_thinking\n为\ntrue\n时生效。适用于\nQwen3-VL、Qwen3 的商业版与开源版模型。详情请参见\n限制思考长度\n。\n默认值为模型最大思维链长度。\n通过 Python SDK 调用时，请通过\nextra_body\n配置。配置方式为：\nextra_body={\"thinking_budget\": xxx}\n。\nseed\ninteger\n（可选）\n设置\nseed\n参数会使文本生成过程更具有确定性，通常用于使模型每次运行的结果一致。\n在每次模型调用时传入相同的\nseed\n值（由您指定），并保持其他参数不变，模型将尽可能返回相同的结果。\n取值范围：0\n到\n2\n31\n−1。\nseed\n默认值\nqwen-vl-plus-2025-01-02、qwen-vl-max、qwen-vl-max-latest、qwen-vl-max-2025-04-08、qwen-vl-max-2025-04-02、qwen-vl-max-2024-12-30、qvq-72b-preview、qvq-max\n系列：3407；\nqwen-vl-max-2025-01-25、qwen-vl-max-2024-11-19、qwen-vl-max-2024-10-30、qwen-vl-max-2024-08-09、qwen-vl-max-2024-02-01、qwen2-vl-72b-instruct、qwen2-vl-2b-instruct、qwen-vl-plus、qwen-vl-plus-latest、qwen-vl-plus-2025-05-07、qwen-vl-plus-2025-01-25、qwen-vl-plus-2024-08-09、qwen-vl-plus-2023-12-01：无默认值；\n其余模型均为\n1234。\nlogprobs\nboolean\n（可选）默认值为\nfalse\n是否返回输出 Token 的对数概率，可选值：\ntrue\n返回；\nfalse\n不返回。\n思考阶段生成的内容（\nreasoning_content\n）不会返回对数概率。\n支持 qwen-plus、qwen-turbo 系列的快照模型（不包含主线模型）与 Qwen3 开源模型。\ntop_logprobs\ninteger\n（可选）默认值为\n0\n指定在每一步生成时，返回模型最大概率的候选 Token 个数。\n取值范围：[0,5]\n仅当\nlogprobs\n为\ntrue\n时生效。\nstop\nstring 或 array\n（可选）\n使用\nstop\n参数后，当模型生成的文本即将包含指定的字符串或\ntoken_id\n时，将自动停止生成。\n您可以在\nstop\n参数中传入敏感词来控制模型的输出。\nstop\n为\narray\n类型时，不可以将\ntoken_id\n和字符串同时作为元素输入，比如不可以指定\nstop\n为\n[\"你好\",104307]\n。\ntools\narray\n（可选）\n可供模型调用的工具数组，可以包含一个或多个工具对象。一次\nFunction Calling\n流程模型会从中选择一个工具（开启\nparallel_tool_calls\n可以选择多个工具）。\n目前不支持通义千问\nVL\n/Audio，也不建议用于数学和代码模型（Qwen3-Coder 模型除外）\n。\n属性\ntype\nstring\n（必选）\ntools\n的类型，当前仅支持\nfunction。\nfunction\nobject\n（必选）\n属性\nname\nstring\n（必选）\n工具函数的名称，必须是字母、数字，可以包含下划线和短划线，最大长度为\n64。\ndescription\nstring\n（必选）\n工具函数的描述，供模型选择何时以及如何调用工具函数。\nparameters\nobject\n（必选）\n工具的参数描述，需要是一个合法的\nJSON Schema。JSON Schema\n的描述可以见\n链接\n。如果\nparameters\n参数为空，表示\nfunction\n没有入参。\ntool_choice\nstring 或 object\n（可选）默认值为\n\"auto\"\n如果您希望对于某一类问题，大模型能够采取制定好的工具选择策略（如强制使用某个工具、强制不使用工具），可以通过修改\ntool_choice\n参数来强制指定工具调用的策略。可选值：\n\"auto\"\n表示由大模型进行工具策略的选择。\n\"none\"\n如果您希望无论输入什么问题，Function Calling 都不会进行工具调用，可以设定\ntool_choice\n参数为\n\"none\"\n；\n{\"type\": \"function\", \"function\": {\"name\": \"the_function_to_call\"}}\n如果您希望对于某一类问题，Function Calling 能够强制调用某个工具，可以设定\ntool_choice\n参数为\n{\"type\": \"function\", \"function\": {\"name\": \"the_function_to_call\"}}\n，其中\nthe_function_to_call\n是您指定的工具函数名称。\n若模型输出思考过程，则不支持设置为该值。\nparallel_tool_calls\nboolean\n（可选）默认值为\nfalse\n是否开启并行工具调用。参数为\ntrue\n时开启，为\nfalse\n时不开启。并行工具调用详情请参见：\n并行工具调用\n。\ntranslation_options\nobject\n（可选）\n当您使用\n翻译模型\n时需要配置的翻译参数。\n属性\nsource_lang\nstring\n（必选）\n源语言的英文全称，详情请参见\n支持的语言\n。您可以将\nsource_lang\n设置为\n\"auto\"\n，模型会自动判断输入文本属于哪种语言。\ntarget_lang\nstring\n（必选）\n目标语言的英文全称，详情请参见\n支持的语言\n。\nterms\narrays\n（可选）\n在使用\n术语干预\n功能时需要设置的术语数组。\n属性\nsource\nstring\n（必选）\n源语言的术语。\ntarget\nstring\n（必选）\n目标语言的术语。\ntm_list\narrays\n（可选）\n在\n翻译记忆\n功能时需要设置的翻译记忆数组。\n属性\nsource\nstring\n（必选）\n源语言的语句。\ntarget\nstring\n（必选）\n目标语言的语句。\ndomains\nstring\n（可选）\n在使用\n领域提示\n功能时需要设置的领域提示语句。\n领域提示语句暂时只支持英文。\n若您通过\nPython SDK\n调用，请通过\nextra_body\n配置。配置方式为：\nextra_body={\"translation_options\": xxx}\n。\nenable_search\nboolean\n（可选）\n模型在生成文本时是否使用互联网搜索结果进行参考。取值如下：\ntrue：启用互联网搜索，模型会将搜索结果作为文本生成过程中的参考信息，但模型会基于其内部逻辑判断是否使用互联网搜索结果。\n如果模型没有搜索互联网，建议优化\nPrompt，或设置\nsearch_options\n中的\nforced_search\n参数开启强制搜索。\nfalse（默认）：关闭互联网搜索。\n启用互联网搜索功能可能会增加 Token 的消耗。\n若您通过 Python SDK\n调用，请通过\nextra_body\n配置。配置方式为：\nextra_body={\"enable_search\": True}\n。\n支持的模型\n通义千问\nMax：qwen3-max、qwen3-max-2025-09-23、qwen3-max-preview、qwen-max、qwen-max-latest、qwen-max-2024-09-19\n及之后的快照版本\n通义千问\nPlus：qwen-plus、qwen-plus-latest、qwen-plus-2025-07-14\n及之后的快照版本\n通义千问\nFlash：qwen-flash、qwen-flash-2025-07-28\n及之后的快照版本\n通义千问\nTurbo：qwen-turbo、qwen-turbo-latest、qwen-turbo-2025-07-15\nQwQ：qwq-plus（仅支持流式输出）\nKimi：Moonshot-Kimi-K2-Instruct\nsearch_options\nobject\n（可选）\n联网搜索的策略。仅当\nenable_search\n为\ntrue\n时生效。详情参见\n联网搜索\n。\n属性\nforced_search\nboolean\n（可选）默认值为\nfalse\n是否强制开启搜索。参数值：\ntrue：强制开启；\nfalse：不强制开启。\nsearch_strategy\nstring\n（可选）默认值为\nturbo\n搜索互联网信息的策略。参数值：\nturbo\n：默认策略，兼顾响应速度与搜索效果，推荐使用。\nmax\n：高性能模式，基于全栈顶配模型与多源搜索引擎，提供最优效果。\n计费信息请参见\n计费说明\n。\nenable_search_extension\nboolean\n（可选）默认值为\nfalse\n是否开启特定领域增强。参数值：\ntrue\n开启。\nfalse\n（默认值）\n不开启。\n若您通过 Python SDK\n调用，请通过\nextra_body\n配置。配置方式为：\nextra_body={\"search_options\": xxx}\n。\nX-DashScope-DataInspection\nstring\n（可选）\n在通义千问 API 的内容安全能力基础上，是否进一步识别输入输出内容的违规信息。取值如下：\n'{\"input\":\"cip\",\"output\":\"cip\"}'\n：进一步识别；\n不设置该参数：不进一步识别。\n通过 HTTP 调用时请放入请求头：\n-H \"X-DashScope-DataInspection: {\\\"input\\\": \\\"cip\\\", \\\"output\\\": \\\"cip\\\"}\"\n；\n通过 Python SDK 调用时请通过\nextra_headers\n配置：\nextra_headers={'X-DashScope-DataInspection': '{\"input\":\"cip\",\"output\":\"cip\"}'}\n。\n详细使用方法请参见\n内容审核\n。\n不支持通过 Node.js SDK\n设置。\n不适用于 Qwen-VL 系列模型。\nchat\n响应对象（非流式输出）\n{\n\"choices\": [\n{\n\"message\": {\n\"role\": \"assistant\",\n\"content\": \"我是阿里云开发的一款超大规模语言模型，我叫通义千问。\"\n},\n\"finish_reason\": \"stop\",\n\"index\": 0,\n\"logprobs\": null\n}\n],\n\"object\": \"chat.completion\",\n\"usage\": {\n\"prompt_tokens\": 3019,\n\"completion_tokens\": 104,\n\"total_tokens\": 3123,\n\"prompt_tokens_details\": {\n\"cached_tokens\": 2048\n}\n},\n\"created\": 1735120033,\n\"system_fingerprint\": null,\n\"model\": \"qwen-plus\",\n\"id\": \"chatcmpl-6ada9ed2-7f33-9de2-8bb0-78bd4035025a\"\n}\nid\nstring\n本次调用的唯一标识符。\nchoices\narray\n模型生成内容的数组，可以包含一个或多个\nchoices\n对象。\n属性\nfinish_reason\nstring\n有三种情况：\n因触发输入参数中的\nstop\n条件，或自然停止输出时为\nstop\n；\n因生成长度过长而结束为\nlength\n；\n因需要调用工具而结束为\ntool_calls\n。\nindex\ninteger\n当前响应在\nchoices\n数组中的序列编号。\nlogprobs\nobject\n当前 choices 对象的概率信息。\n属性\ncontent\narray\n带有对数概率信息的 Token 数组。\n属性\ntoken\nstring\n当前 Token。\nbytes\narray\n当前 Token 的 UTF‑8 原始字节列表，用于精确还原输出内容，在处理表情符号、中文字符时有帮助。\nlogprob\nfloat\n当前 Token 的对数概率。返回值为 null 表示概率值极低。\ntop_logprobs\narray\n当前 Token 位置最可能的若干个 Token 及其对数概率，元素个数与入参的\ntop_logprobs\n保持一致。\n属性\ntoken\nstring\n当前 Token。\nbytes\narray\n当前 Token 的 UTF‑8 原始字节列表，用于精确还原输出内容，在处理表情符号、中文字符时有帮助。\nlogprob\nfloat\n当前 Token 的对数概率。返回值为 null 表示概率值极低。\nmessage\nobject\n本次调用模型输出的消息。\n属性\ncontent\nstring\n本次调用模型生成的文本。\nreasoning_content\nstring\n模型的思维链内容。\nrefusal\nstring\n该参数当前固定为\nnull\n。\nrole\nstring\n消息的角色，固定为\nassistant\n。\naudio\nobject\n该参数当前固定为\nnull\n。\nfunction_call\n（即将废弃）\nobject\n该值默认为\nnull\n，请参考\ntool_calls\n参数。\ntool_calls\narray\n在发起 Function Calling\n后，模型回复的要调用的工具以及调用工具所需的参数。可以包含一个或多个工具响应对象。\n属性\nid\nstring\n本次工具响应的\nID。\ntype\nstring\n工具的类型，当前只支持\nfunction\n。\nfunction\nobject\n需要被调用的函数。\n属性\nname\nstring\n需要被调用的函数名。\narguments\nstring\n需要输入到工具中的参数，为\nJSON\n字符串。\n由于大模型响应有一定随机性，输出的\nJSON\n字符串并不总满足于您的函数，建议您在将参数输入函数前进行参数的有效性校验。\nindex\ninteger\n工具信息在\ntool_calls\n列表中的索引。\ncreated\ninteger\n本次\nchat\n请求被创建时的时间戳。\nmodel\nstring\n本次\nchat\n请求使用的模型名称。\nobject\nstring\n始终为\nchat.completion\n。\nservice_tier\nstring\n该参数当前固定为\nnull\n。\nsystem_fingerprint\nstring\n该参数当前固定为\nnull\n。\nusage\nobject\n本次\nchat\n请求使用的 Token 信息。\n属性\ncompletion_tokens\ninteger\n模型生成回复转换为 Token 后的长度。\nprompt_tokens\ninteger\n用户的输入转换成 Token 后的长度。\ntotal_tokens\ninteger\nprompt_tokens\n与\ncompletion_tokens\n的总和。\ncompletion_tokens_details\nobject\n使用\nQwen-VL 模型\n时输出\nToken\n的细粒度分类。\n属性\naudio_tokens\ninteger\n该参数当前固定为\nnull\n。\nreasoning_tokens\ninteger\n该参数当前固定为\nnull\n。\ntext_tokens\ninteger\nQwen-VL 模型\n输出的文本的\nToken\n数。\nprompt_tokens_details\nobject\n输入 Token 的细粒度分类。\n属性\naudio_tokens\ninteger\n该参数当前固定为\nnull\n。\ncached_tokens\ninteger\n命中 Cache 的 Token 数。\nContext Cache 详情请参见\n上下文缓存\n。\ntext_tokens\ninteger\nQwen-VL 模型\n输入的文本转换为 Token 后的长度。\nimage_tokens\ninteger\nQwen-VL 模型\n输入的图像转换为\nToken\n的长度。\nvideo_tokens\ninteger\nQwen-VL 模型\n输入的视频文件或者图像列表转换为\nToken\n后的长度。\ncache_creation\nobject\n显式缓存\n创建信息。\n属性\nephemeral_5m_input_tokens\ninteger\n用于创建\n5\n分钟有效期显式缓存的 Token 长度。\ncache_creation_input_tokens\ninteger\n用于创建显式缓存的 Token 长度。\ncache_type\nstring\n使用\n显式缓存\n时，参数值为\nephemeral\n，否则该参数不存在。\nchat\n响应\nchunk\n对象（流式输出）\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":\"assistant\",\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"我是\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"来自\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"阿里\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"云的超大规模\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"语言模型，我\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"叫通义千\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"问。\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"refusal\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null}],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\n{\"id\":\"chatcmpl-e30f5ae7-3063-93c4-90fe-beb5f900bd57\",\"choices\":[],\"created\":1735113344,\"model\":\"qwen-plus\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":17,\"prompt_tokens\":22,\"total_tokens\":39,\"completion_tokens_details\":null,\"prompt_tokens_details\":{\"audio_tokens\":null,\"cached_tokens\":0}}}\nid\nstring\n本次调用的唯一标识符。每个\nchunk\n对象有相同的 id。\nchoices\narray\n模型生成内容的数组，可包含一个或多个\nchoices\n对象。如果设置\ninclude_usage\n参数为\ntrue\n，则最后一个\nchunk\n为空。\n属性\ndelta\nobject\nchat\n请求的增量对象。\n属性\ncontent\nstring\nchunk\n的消息内容。\nreasoning_content\nstring\n模型的思维链内容。\nfunction_call\nobject\n该值默认为\nnull\n，请参考\ntool_calls\n参数。\naudio\nobject\n使用\nQwen-Omni\n模型时生成的回复。\n属性\ntranscript\nstring\n流式输出的文本内容。\ndata\nstring\n流式输出的 Base64 音频编码数据。\nexpires_at\ninteger\n创建请求时的时间戳。\nrefusal\nobject\n该参数当前固定为\nnull\n。\nrole\nstring\n增量消息对象的角色，只在第一个\nchunk\n中有值。\ntool_calls\narray\n模型回复的要调用的工具以及调用工具所需的参数。可以包含一个或多个工具响应对象。\n属性\nindex\ninteger\n工具信息在\ntool_calls\n列表中的索引。\nid\nstring\n本次工具响应的\nID。\nfunction\nobject\n需要被调用的函数。\n属性\narguments\nstring\n需要输入到工具中的参数，所有\nchunk\n的\narguments\n拼接后为完整的\nJSON\n字符串。\n由于大模型响应有一定随机性，输出的\nJSON\n字符串并不总满足于您的函数，建议您在将参数输入函数前进行参数的有效性校验。\nname\nstring\n函数名称，只在第一个\nchunk\n中有值。\ntype\nstring\n工具的类型，当前只支持\nfunction\n。\nfinish_reason\nstring\n有四种情况：\n因触发输入参数中的\nstop\n条件，或自然停止输出时为\nstop\n；\n当生成未结束时为\nnull\n；\n因生成长度过长而结束为\nlength\n；\n因需要调用工具而结束为\ntool_calls\n。\nindex\ninteger\n当前响应在\nchoices\n列表中的序列编号。当输入参数\nn\n大于\n1\n时，您需要根据\nindex\n参数来进行不同响应对应的完整内容的拼接。\nlogprobs\nobject\n当前 choices 对象的概率信息。\n属性\ncontent\narray\n带有对数概率信息的 Token 数组。\n属性\ntoken\nstring\n当前 Token。\nbytes\narray\n当前 Token 的 UTF‑8 原始字节列表，用于精确还原输出内容，在处理表情符号、中文字符时有帮助。\nlogprob\nfloat\n当前 Token 的对数概率。返回值为 null 表示概率值极低。\ntop_logprobs\narray\n当前 Token 位置最可能的若干个 Token 及其对数概率，元素个数与入参的\ntop_logprobs\n保持一致。\n属性\ntoken\nstring\n当前 Token。\nbytes\narray\n当前 Token 的 UTF‑8 原始字节列表，用于精确还原输出内容，在处理表情符号、中文字符时有帮助。\nlogprob\nfloat\n当前 Token 的对数概率。返回值为 null 表示概率值极低。\ncreated\ninteger\n本次\nchat\n请求被创建时的时间戳。每个\nchunk\n对象有相同的时间戳。\nmodel\nstring\n本次\nchat\n请求使用的模型名称。\nobject\nstring\n始终为\nchat.completion.chunk\n。\nservice_tier\nstring\n该参数当前固定为\nnull\n。\nsystem_fingerprint\nstring\n该参数当前固定为\nnull\n。\nusage\nobject\n本次\nchat\n请求使用的\nToken\n信息。只在\ninclude_usage\n为\ntrue\n时，在最后一个\nchunk\n显示。\n属性\ncompletion_tokens\ninteger\n模型生成回复转换为 Token 后的长度。\nprompt_tokens\ninteger\n用户的输入转换成 Token 后的长度。\ntotal_tokens\ninteger\nprompt_tokens\n与\ncompletion_tokens\n的总和。\ncompletion_tokens_details\nobject\n输出转换为 Token 后的详细信息。\n属性\naudio_tokens\ninteger\nQwen-Omni 模型\n输出的音频转换为 Token 后的长度。\nreasoning_tokens\ninteger\nQwen3 模型思考过程转换为 Token 后的长度。\ntext_tokens\ninteger\nQwen-VL 模型\n、\nQVQ\n模型\n、\nQwen-Omni 模型\n输出的文本转换为 Token 后的长度。\nprompt_tokens_details\nobject\n输入数据的 Token 细粒度分类。\n属性\naudio_tokens\ninteger\n使用\nQwen-Omni 模型\n时，输入的音频转换为 Token 后的长度。\n视频文件中的音频转换为 Token 后的长度会在该参数中体现。\ntext_tokens\ninteger\n使用\nQwen-Omni 模型\n时，输入的文本转换为 Token 后的长度。\nvideo_tokens\ninteger\n使用\nQwen-VL 模型\n、\nQVQ\n模型\n、\nQwen-Omni 模型\n时，输入的视频（图片列表形式或视频文件）转换为 Token 后的长度。如果\nQwen-Omni 模型\n输入的是视频文件，则\nvideo_tokens\n不包含音频的 Token，音频的 Token 在\naudio_tokens\n中体现。\nimage_tokens\ninteger\n使用\nQwen-VL 模型\n、\nQVQ\n模型\n、\nQwen-Omni 模型\n时，输入的图片转换为 Token 后的长度。\ncached_tokens\ninteger\n命中 Cache 的 Token 数。Context Cache 详情请参见\n上下文缓存\n。\ncache_creation\nobject\n显式缓存\n创建信息。\n属性\nephemeral_5m_input_tokens\ninteger\n用于创建\n5\n分钟有效期显式缓存的 Token 长度。\ncache_creation_input_tokens\ninteger\n用于创建显式缓存的 Token 长度。\ncache_type\nstring\n使用\n显式缓存\n时，参数值为\nephemeral\n，否则该参数不存在\nDashScope\n北京地域\n通过\nHTTP\n调用时需配置的\nendpoint：\n使用通义千问大语言模型：\nPOST https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\n使用通义千问\nVL\n或通义千问\nAudio\n模型：\nPOST https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation\n新加坡地域\n通过\nHTTP\n调用时需配置的\nendpoint：\n使用通义千问大语言模型：\nPOST https://dashscope-intl.aliyuncs.com/api/v1/services/aigc/text-generation/generation\n使用通义千问\nVL/OCR\n模型：\nPOST https://dashscope-intl.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation\n通过\nSDK\n调用时需配置的\nbase_url：\nPython\n代码\ndashscope.base_http_api_url = 'https://dashscope-intl.aliyuncs.com/api/v1'\nJava\n代码\n方式一：\nimport com.alibaba.dashscope.protocol.Protocol;\nGeneration gen = new Generation(Protocol.HTTP.getValue(), \"https://dashscope-intl.aliyuncs.com/api/v1\");\n方式二：\nimport com.alibaba.dashscope.utils.Constants;\nConstants.baseHttpApiUrl=\"https://dashscope-intl.aliyuncs.com/api/v1\";\n金融云\n通过\nHTTP\n调用时需配置的\nendpoint：\n使用通义千问大语言模型：\nPOST https://dashscope-finance.aliyuncs.com/api/v1/services/aigc/text-generation/generation\n通过\nSDK\n调用时需配置的\nbase_url：\nhttps://dashscope-finance.aliyuncs.com/api/v1\nPython\n代码\n在导入模块之后，添加以下代码：\ndashscope.base_http_api_url = 'https://dashscope-finance.aliyuncs.com/api/v1'\nJava\n代码\nGeneration gen = new Generation(\"http\", \"https://dashscope-finance.aliyuncs.com/api/v1\");\n您需要已\n获取\nAPI Key\n并\n配置\nAPI Key\n到环境变量\n。如果通过\nDashScope SDK\n进行调用，还需要\n安装\nDashScope SDK\n。\n请求体\n文本输入\n此处以单轮对话作为示例，您也可以进行\n多轮对话\n。\nPython\nimport os\nimport dashscope\nmessages = [\n{'role': 'system', 'content': 'You are a helpful assistant.'},\n{'role': 'user', 'content': '你是谁？'}\n]\nresponse = dashscope.Generation.call(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key=\"sk-xxx\",\napi_key=os.getenv('DASHSCOPE_API_KEY'),\nmodel=\"qwen-plus\", # 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages=messages,\nresult_format='message'\n)\nprint(response)\nJava\n// 建议\ndashscope SDK\n的版本 >= 2.12.0\nimport java.util.Arrays;\nimport java.lang.System;\nimport com.alibaba.dashscope.aigc.generation.Generation;\nimport com.alibaba.dashscope.aigc.generation.GenerationParam;\nimport com.alibaba.dashscope.aigc.generation.GenerationResult;\nimport com.alibaba.dashscope.common.Message;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.InputRequiredException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.utils.JsonUtils;\npublic class Main {\npublic static GenerationResult callWithMessage() throws ApiException, NoApiKeyException, InputRequiredException {\nGeneration gen = new Generation();\nMessage systemMsg = Message.builder()\n.role(Role.SYSTEM.getValue())\n.content(\"You are a helpful assistant.\")\n.build();\nMessage userMsg = Message.builder()\n.role(Role.USER.getValue())\n.content(\"你是谁？\")\n.build();\nGenerationParam param = GenerationParam.builder()\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：.apiKey(\"sk-xxx\")\n.apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n// 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n.model(\"qwen-plus\")\n.messages(Arrays.asList(systemMsg, userMsg))\n.resultFormat(GenerationParam.ResultFormat.MESSAGE)\n.build();\nreturn gen.call(param);\n}\npublic static void main(String[] args) {\ntry {\nGenerationResult result = callWithMessage();\nSystem.out.println(JsonUtils.toJson(result));\n} catch (ApiException | NoApiKeyException | InputRequiredException e) {\n// 使用日志框架记录异常信息\nSystem.err.println(\"An error occurred while calling the generation service: \" + e.getMessage());\n}\nSystem.exit(0);\n}\n}\nPHP（HTTP）\n<?php\n$url = \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\";\n$apiKey = getenv('DASHSCOPE_API_KEY');\n$data = [\n// 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n\"model\" => \"qwen-plus\",\n\"input\" => [\n\"messages\" => [\n[\n\"role\" => \"system\",\n\"content\" => \"You are a helpful assistant.\"\n],\n[\n\"role\" => \"user\",\n\"content\" => \"你是谁？\"\n]\n]\n],\n\"parameters\" => [\n\"result_format\" => \"message\"\n]\n];\n$jsonData = json_encode($data);\n$ch = curl_init($url);\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, $jsonData);\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\ncurl_setopt($ch, CURLOPT_HTTPHEADER, [\n\"Authorization: Bearer $apiKey\",\n\"Content-Type: application/json\"\n]);\n$response = curl_exec($ch);\n$httpCode = curl_getinfo($ch, CURLINFO_HTTP_CODE);\nif ($httpCode == 200) {\necho \"Response: \" . $response;\n} else {\necho \"Error: \" . $httpCode . \" - \" . $response;\n}\ncurl_close($ch);\n?>\nNode.js（HTTP）\nDashScope 未提供 Node.js 环境的 SDK。如需通过 OpenAI Node.js SDK\n调用，请参考本文的\nOpenAI\n章节。\nimport fetch from 'node-fetch';\nconst apiKey = process.env.DASHSCOPE_API_KEY;\nconst data = {\nmodel: \"qwen-plus\", // 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\ninput: {\nmessages: [\n{\nrole: \"system\",\ncontent: \"You are a helpful assistant.\"\n},\n{\nrole: \"user\",\ncontent: \"你是谁？\"\n}\n]\n},\nparameters: {\nresult_format: \"message\"\n}\n};\nfetch('https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation', {\nmethod: 'POST',\nheaders: {\n'Authorization': `Bearer ${apiKey}`,\n'Content-Type': 'application/json'\n},\nbody: JSON.stringify(data)\n})\n.then(response => response.json())\n.then(data => {\nconsole.log(JSON.stringify(data));\n})\n.catch(error => {\nconsole.error('Error:', error);\n});\nC#（HTTP）\nusing System.Net.Http.Headers;\nusing System.Text;\nclass Program\n{\nprivate static readonly HttpClient httpClient = new HttpClient();\nstatic async Task Main(string[] args)\n{\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：string? apiKey = \"sk-xxx\";\nstring? apiKey = Environment.GetEnvironmentVariable(\"DASHSCOPE_API_KEY\");\nif (string.IsNullOrEmpty(apiKey))\n{\nConsole.WriteLine(\"API Key 未设置。请确保环境变量 'DASHSCOPE_API_KEY' 已设置。\");\nreturn;\n}\n// 设置请求 URL 和内容\nstring url = \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\";\n// 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nstring jsonContent = @\"{\n\"\"model\"\": \"\"qwen-plus\"\",\n\"\"input\"\": {\n\"\"messages\"\": [\n{\n\"\"role\"\": \"\"system\"\",\n\"\"content\"\": \"\"You are a helpful assistant.\"\"\n},\n{\n\"\"role\"\": \"\"user\"\",\n\"\"content\"\": \"\"你是谁？\"\"\n}\n]\n},\n\"\"parameters\"\": {\n\"\"result_format\"\": \"\"message\"\"\n}\n}\";\n// 发送请求并获取响应\nstring result = await SendPostRequestAsync(url, jsonContent, apiKey);\n// 输出结果\nConsole.WriteLine(result);\n}\nprivate static async Task<string> SendPostRequestAsync(string url, string jsonContent, string apiKey)\n{\nusing (var content = new StringContent(jsonContent, Encoding.UTF8, \"application/json\"))\n{\n// 设置请求头\nhttpClient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\nhttpClient.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(\"application/json\"));\n// 发送请求并获取响应\nHttpResponseMessage response = await httpClient.PostAsync(url, content);\n// 处理响应\nif (response.IsSuccessStatusCode)\n{\nreturn await response.Content.ReadAsStringAsync();\n}\nelse\n{\nreturn $\"请求失败: {response.StatusCode}\";\n}\n}\n}\n}\nGo（HTTP）\nDashScope 未提供 Go 的 SDK。如需通过 OpenAI Go SDK\n调用，请参考本文的\nOpenAI-Go\n章节。\npackage main\nimport (\n\"bytes\"\n\"encoding/json\"\n\"fmt\"\n\"io\"\n\"log\"\n\"net/http\"\n\"os\"\n)\ntype Message struct {\nRole    string `json:\"role\"`\nContent string `json:\"content\"`\n}\ntype Input struct {\nMessages []Message `json:\"messages\"`\n}\ntype Parameters struct {\nResultFormat string `json:\"result_format\"`\n}\ntype RequestBody struct {\nModel      string     `json:\"model\"`\nInput      Input      `json:\"input\"`\nParameters Parameters `json:\"parameters\"`\n}\nfunc main() {\n// 创建 HTTP 客户端\nclient := &http.Client{}\n// 构建请求体\nrequestBody := RequestBody{\n// 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nModel: \"qwen-plus\",\nInput: Input{\nMessages: []Message{\n{\nRole:    \"system\",\nContent: \"You are a helpful assistant.\",\n},\n{\nRole:    \"user\",\nContent: \"你是谁？\",\n},\n},\n},\nParameters: Parameters{\nResultFormat: \"message\",\n},\n}\njsonData, err := json.Marshal(requestBody)\nif err != nil {\nlog.Fatal(err)\n}\n// 创建 POST 请求\nreq, err := http.NewRequest(\"POST\", \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\", bytes.NewBuffer(jsonData))\nif err != nil {\nlog.Fatal(err)\n}\n// 设置请求头\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：apiKey := \"sk-xxx\"\napiKey := os.Getenv(\"DASHSCOPE_API_KEY\")\nreq.Header.Set(\"Authorization\", \"Bearer \"+apiKey)\nreq.Header.Set(\"Content-Type\", \"application/json\")\n// 发送请求\nresp, err := client.Do(req)\nif err != nil {\nlog.Fatal(err)\n}\ndefer resp.Body.Close()\n// 读取响应体\nbodyText, err := io.ReadAll(resp.Body)\nif err != nil {\nlog.Fatal(err)\n}\n// 打印响应内容\nfmt.Printf(\"%s\\n\", bodyText)\n}\ncurl\ncurl --location \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\" \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n\"model\": \"qwen-plus\",\n\"input\":{\n\"messages\":[\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful assistant.\"\n},\n{\n\"role\": \"user\",\n\"content\": \"你是谁？\"\n}\n]\n},\n\"parameters\": {\n\"result_format\": \"message\"\n}\n}'\n流式输出\n更多用法请参见\n流式输出\n。\nPython\nimport os\nimport dashscope\nmessages = [\n{'role':'system','content':'you are a helpful assistant'},\n{'role': 'user','content': '你是谁？'}\n]\nresponses = dashscope.Generation.call(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key=\"sk-xxx\",\napi_key=os.getenv('DASHSCOPE_API_KEY'),\nmodel=\"qwen-plus\", # 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages=messages,\nresult_format='message',\nstream=True,\nincremental_output=True\n)\nfor response in responses:\nprint(response)\nJava\nimport java.util.Arrays;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport com.alibaba.dashscope.aigc.generation.Generation;\nimport com.alibaba.dashscope.aigc.generation.GenerationParam;\nimport com.alibaba.dashscope.aigc.generation.GenerationResult;\nimport com.alibaba.dashscope.common.Message;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.InputRequiredException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.utils.JsonUtils;\nimport io.reactivex.Flowable;\nimport java.lang.System;\npublic class Main {\nprivate static final Logger logger = LoggerFactory.getLogger(Main.class);\nprivate static void handleGenerationResult(GenerationResult message) {\nSystem.out.println(JsonUtils.toJson(message));\n}\npublic static void streamCallWithMessage(Generation gen, Message userMsg)\nthrows NoApiKeyException, ApiException, InputRequiredException {\nGenerationParam param = buildGenerationParam(userMsg);\nFlowable<GenerationResult> result = gen.streamCall(param);\nresult.blockingForEach(message -> handleGenerationResult(message));\n}\nprivate static GenerationParam buildGenerationParam(Message userMsg) {\nreturn GenerationParam.builder()\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：.apiKey(\"sk-xxx\")\n.apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n// 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n.model(\"qwen-plus\")\n.messages(Arrays.asList(userMsg))\n.resultFormat(GenerationParam.ResultFormat.MESSAGE)\n.incrementalOutput(true)\n.build();\n}\npublic static void main(String[] args) {\ntry {\nGeneration gen = new Generation();\nMessage userMsg = Message.builder().role(Role.USER.getValue()).content(\"你是谁？\").build();\nstreamCallWithMessage(gen, userMsg);\n} catch (ApiException | NoApiKeyException | InputRequiredException  e) {\nlogger.error(\"An exception occurred: {}\", e.getMessage());\n}\nSystem.exit(0);\n}\n}\ncurl\ncurl --location \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\" \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--header \"X-DashScope-SSE: enable\" \\\n--data '{\n\"model\": \"qwen-plus\",\n\"input\":{\n\"messages\":[\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful assistant.\"\n},\n{\n\"role\": \"user\",\n\"content\": \"你是谁？\"\n}\n]\n},\n\"parameters\": {\n\"result_format\": \"message\",\n\"incremental_output\":true\n}\n}'\n图像输入\n关于大模型分析图像的更多用法，请参见\n视觉理解\n。\nPython\nimport os\nimport dashscope\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"image\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"},\n{\"image\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/tiger.png\"},\n{\"image\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/rabbit.png\"},\n{\"text\": \"这些是什么?\"}\n]\n}\n]\nresponse = dashscope.MultiModalConversation.call(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key=\"sk-xxx\",\napi_key=os.getenv('DASHSCOPE_API_KEY'),\nmodel='qwen-vl-max', # 此处以\nqwen-vl-max\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages=messages\n)\nprint(response)\nJava\n// Copyright (c) Alibaba, Inc. and its affiliates.\nimport java.util.Arrays;\nimport java.util.Collections;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversation;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationParam;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationResult;\nimport com.alibaba.dashscope.common.MultiModalMessage;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.exception.UploadFileException;\nimport com.alibaba.dashscope.utils.JsonUtils;\npublic class Main {\npublic static void simpleMultiModalConversationCall()\nthrows ApiException, NoApiKeyException, UploadFileException {\nMultiModalConversation conv = new MultiModalConversation();\nMultiModalMessage userMessage = MultiModalMessage.builder().role(Role.USER.getValue())\n.content(Arrays.asList(\nCollections.singletonMap(\"image\", \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"),\nCollections.singletonMap(\"image\", \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/tiger.png\"),\nCollections.singletonMap(\"image\", \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/rabbit.png\"),\nCollections.singletonMap(\"text\", \"这些是什么?\"))).build();\nMultiModalConversationParam param = MultiModalConversationParam.builder()\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：.apiKey(\"sk-xxx\")\n.apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n// 此处以\nqwen-vl-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n.model(\"qwen-vl-plus\")\n.message(userMessage)\n.build();\nMultiModalConversationResult result = conv.call(param);\nSystem.out.println(JsonUtils.toJson(result));\n}\npublic static void main(String[] args) {\ntry {\nsimpleMultiModalConversationCall();\n} catch (ApiException | NoApiKeyException | UploadFileException e) {\nSystem.out.println(e.getMessage());\n}\nSystem.exit(0);\n}\n}\ncurl\ncurl --location 'https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation' \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"model\": \"qwen-vl-plus\",\n\"input\":{\n\"messages\":[\n{\n\"role\": \"user\",\n\"content\": [\n{\"image\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"},\n{\"image\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/tiger.png\"},\n{\"image\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/rabbit.png\"},\n{\"text\": \"这些是什么?\"}\n]\n}\n]\n}\n}'\n视频输入\n以下为传入视频帧的示例代码，关于更多用法（如传入视频文件），请参见\n视觉理解\n。\nPython\nfrom http import HTTPStatus\nimport os\n# dashscope\n版本需要不低于\n1.20.10\nimport dashscope\nmessages = [{\"role\": \"user\",\n\"content\": [\n{\"video\":[\"https://img.alicdn.com/imgextra/i3/O1CN01K3SgGo1eqmlUgeE9b_!!6000000003923-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i4/O1CN01BjZvwg1Y23CF5qIRB_!!6000000003000-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i4/O1CN01Ib0clU27vTgBdbVLQ_!!6000000007859-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i1/O1CN01aygPLW1s3EXCdSN4X_!!6000000005710-0-tps-3840-2160.jpg\"]},\n{\"text\": \"描述这个视频的具体过程\"}]}]\nresponse = dashscope.MultiModalConversation.call(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key=\"sk-xxx\",\napi_key=os.getenv(\"DASHSCOPE_API_KEY\"),\nmodel='qwen-vl-max-latest',  # 此处以\nqwen-vl-max-latest\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages=messages\n)\nif response.status_code == HTTPStatus.OK:\nprint(response)\nelse:\nprint(response.code)\nprint(response.message)\nJava\n// DashScope SDK\n版本需要不低于\n2.16.7\nimport java.util.Arrays;\nimport java.util.Collections;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversation;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationParam;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationResult;\nimport com.alibaba.dashscope.common.MultiModalMessage;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.exception.UploadFileException;\nimport com.alibaba.dashscope.utils.JsonUtils;\npublic class Main {\n// 此处以\nqwen-vl-max-latest\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nprivate static final String MODEL_NAME = \"qwen-vl-max-latest\";\npublic static void videoImageListSample() throws ApiException, NoApiKeyException, UploadFileException {\nMultiModalConversation conv = new MultiModalConversation();\nMultiModalMessage systemMessage = MultiModalMessage.builder()\n.role(Role.SYSTEM.getValue())\n.content(Arrays.asList(Collections.singletonMap(\"text\", \"You are a helpful assistant.\")))\n.build();\nMultiModalMessage userMessage = MultiModalMessage.builder()\n.role(Role.USER.getValue())\n.content(Arrays.asList(Collections.singletonMap(\"video\", Arrays.asList(\"https://img.alicdn.com/imgextra/i3/O1CN01K3SgGo1eqmlUgeE9b_!!6000000003923-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i4/O1CN01BjZvwg1Y23CF5qIRB_!!6000000003000-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i4/O1CN01Ib0clU27vTgBdbVLQ_!!6000000007859-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i1/O1CN01aygPLW1s3EXCdSN4X_!!6000000005710-0-tps-3840-2160.jpg\")),\nCollections.singletonMap(\"text\", \"描述这个视频的具体过程\")))\n.build();\nMultiModalConversationParam param = MultiModalConversationParam.builder()\n.model(MODEL_NAME).message(systemMessage)\n.message(userMessage).build();\nMultiModalConversationResult result = conv.call(param);\nSystem.out.print(JsonUtils.toJson(result));\n}\npublic static void main(String[] args) {\ntry {\nvideoImageListSample();\n} catch (ApiException | NoApiKeyException | UploadFileException e) {\nSystem.out.println(e.getMessage());\n}\nSystem.exit(0);\n}\n}\ncurl\ncurl -X POST https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation \\\n-H \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n-H 'Content-Type: application/json' \\\n-d '{\n\"model\": \"qwen-vl-max-latest\",\n\"input\": {\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"video\": [\n\"https://img.alicdn.com/imgextra/i3/O1CN01K3SgGo1eqmlUgeE9b_!!6000000003923-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i4/O1CN01BjZvwg1Y23CF5qIRB_!!6000000003000-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i4/O1CN01Ib0clU27vTgBdbVLQ_!!6000000007859-0-tps-3840-2160.jpg\",\n\"https://img.alicdn.com/imgextra/i1/O1CN01aygPLW1s3EXCdSN4X_!!6000000005710-0-tps-3840-2160.jpg\"\n]\n},\n{\n\"text\": \"描述这个视频的具体过程\"\n}\n]\n}\n]\n}\n}'\n音频输入\n音频理解\n关于大模型分析音频的更多用法，请参见\n音频理解-Qwen-Audio\n。\nPython\nimport os\nimport dashscope\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"audio\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/audios/welcome.mp3\"},\n{\"text\": \"这段音频在说什么?\"}\n]\n}\n]\nresponse = dashscope.MultiModalConversation.call(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key=\"sk-xxx\",\napi_key=os.getenv('DASHSCOPE_API_KEY'),\nmodel='qwen2-audio-instruct', # 此处以\nqwen2-audio-instruct\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages=messages\n)\nprint(response)\nJava\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.lang.System;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversation;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationParam;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationResult;\nimport com.alibaba.dashscope.common.MultiModalMessage;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.exception.UploadFileException;\nimport com.alibaba.dashscope.utils.JsonUtils;\npublic class Main {\npublic static void simpleMultiModalConversationCall()\nthrows ApiException, NoApiKeyException, UploadFileException {\nMultiModalConversation conv = new MultiModalConversation();\nMultiModalMessage userMessage = MultiModalMessage.builder().role(Role.USER.getValue())\n.content(Arrays.asList(Collections.singletonMap(\"audio\", \"https://dashscope.oss-cn-beijing.aliyuncs.com/audios/welcome.mp3\"),\nCollections.singletonMap(\"text\", \"这段音频在说什么?\"))).build();\nMultiModalConversationParam param = MultiModalConversationParam.builder()\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：.apiKey(\"sk-xxx\")\n.apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n// 此处以\nqwen2-audio-instruct\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n.model(\"qwen2-audio-instruct\")\n.message(userMessage)\n.build();\nMultiModalConversationResult result = conv.call(param);\nSystem.out.println(JsonUtils.toJson(result));\n}\npublic static void main(String[] args) {\ntry {\nsimpleMultiModalConversationCall();\n} catch (ApiException | NoApiKeyException | UploadFileException e) {\nSystem.out.println(e.getMessage());\n}\nSystem.exit(0);\n}\n}\ncurl\ncurl --location 'https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation' \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"model\": \"qwen2-audio-instruct\",\n\"input\":{\n\"messages\":[\n{\n\"role\": \"system\",\n\"content\": [\n{\"text\": \"You are a helpful assistant.\"}\n]\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"audio\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/audios/welcome.mp3\"},\n{\"text\": \"这段音频在说什么?\"}\n]\n}\n]\n}\n}'\n语音识别\n关于语音识别的更多用法，请参见\n录音文件识别-通义千问\n。\nPython\nimport os\nimport dashscope\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [\n# 此处用于配置定制化识别的\nContext\n{\"text\": \"\"},\n]\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"audio\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/audios/welcome.mp3\"},\n]\n}\n]\nresponse = dashscope.MultiModalConversation.call(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key = \"sk-xxx\"\napi_key=os.getenv(\"DASHSCOPE_API_KEY\"),\nmodel=\"qwen3-asr-flash\",\nmessages=messages,\nresult_format=\"message\",\nasr_options={\n# \"language\": \"zh\", # 可选，若已知音频的语种，可通过该参数指定待识别语种，以提升识别准确率\n\"enable_lid\":True,\n\"enable_itn\":False\n}\n)\nprint(response)\n完整结果以\nJSON\n格式输出到控制台。完整结果包含状态码、唯一的请求\nID、识别后的内容以及本次调用的\ntoken\n信息。\n{\n\"output\": {\n\"choices\": [\n{\n\"finish_reason\": \"stop\",\n\"message\": {\n\"annotations\": [\n{\n\"language\": \"zh\",\n\"type\": \"audio_info\"\n}\n],\n\"content\": [\n{\n\"text\": \"欢迎使用阿里云。\"\n}\n],\n\"role\": \"assistant\"\n}\n}\n]\n},\n\"usage\": {\n\"input_tokens_details\": {\n\"text_tokens\": 0\n},\n\"output_tokens_details\": {\n\"text_tokens\": 6\n},\n\"seconds\": 1\n},\n\"request_id\": \"568e2bf0-d6f2-97f8-9f15-a57b11dc6977\"\n}\nJava\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.Map;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversation;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationParam;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationResult;\nimport com.alibaba.dashscope.common.MultiModalMessage;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.exception.UploadFileException;\nimport com.alibaba.dashscope.utils.JsonUtils;\npublic class Main {\npublic static void simpleMultiModalConversationCall()\nthrows ApiException, NoApiKeyException, UploadFileException {\nMultiModalConversation conv = new MultiModalConversation();\nMultiModalMessage userMessage = MultiModalMessage.builder()\n.role(Role.USER.getValue())\n.content(Arrays.asList(\nCollections.singletonMap(\"audio\", \"https://dashscope.oss-cn-beijing.aliyuncs.com/audios/welcome.mp3\")))\n.build();\nMultiModalMessage sysMessage = MultiModalMessage.builder().role(Role.SYSTEM.getValue())\n// 此处用于配置定制化识别的\nContext\n.content(Arrays.asList(Collections.singletonMap(\"text\", \"\")))\n.build();\nMap<String, Object> asrOptions = new HashMap<>();\nasrOptions.put(\"enable_lid\", true);\nasrOptions.put(\"enable_itn\", false);\n// asrOptions.put(\"language\", \"zh\"); // 可选，若已知音频的语种，可通过该参数指定待识别语种，以提升识别准确率\nMultiModalConversationParam param = MultiModalConversationParam.builder()\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：.apiKey(\"sk-xxx\")\n.apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n.model(\"qwen3-asr-flash\")\n.message(userMessage)\n.message(sysMessage)\n.parameter(\"asr_options\", asrOptions)\n.build();\nMultiModalConversationResult result = conv.call(param);\nSystem.out.println(JsonUtils.toJson(result));\n}\npublic static void main(String[] args) {\ntry {\nsimpleMultiModalConversationCall();\n} catch (ApiException | NoApiKeyException | UploadFileException e) {\nSystem.out.println(e.getMessage());\n}\nSystem.exit(0);\n}\n}\n完整结果以\nJSON\n格式输出到控制台。完整结果包含状态码、唯一的请求\nID、识别后的内容以及本次调用的\ntoken\n信息。\n{\n\"output\": {\n\"choices\": [\n{\n\"finish_reason\": \"stop\",\n\"message\": {\n\"annotations\": [\n{\n\"language\": \"zh\",\n\"type\": \"audio_info\"\n}\n],\n\"content\": [\n{\n\"text\": \"欢迎使用阿里云。\"\n}\n],\n\"role\": \"assistant\"\n}\n}\n]\n},\n\"usage\": {\n\"input_tokens_details\": {\n\"text_tokens\": 0\n},\n\"output_tokens_details\": {\n\"text_tokens\": 6\n},\n\"seconds\": 1\n},\n\"request_id\": \"568e2bf0-d6f2-97f8-9f15-a57b11dc6977\"\n}\ncurl\n通过\nSystem Message\n的\ntext\n参数，可以配置\nContext\n进行定制化识别。\ncurl --location --request POST 'https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation' \\\n--header 'Authorization: Bearer $DASHSCOPE_API_KEY' \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"model\": \"qwen3-asr-flash\",\n\"input\": {\n\"messages\": [\n{\n\"content\": [\n{\n\"text\": \"\"\n}\n],\n\"role\": \"system\"\n},\n{\n\"content\": [\n{\n\"audio\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/audios/welcome.mp3\"\n}\n],\n\"role\": \"user\"\n}\n]\n},\n\"parameters\": {\n\"asr_options\": {\n\"enable_lid\": true,\n\"enable_itn\": false\n}\n}\n}'\n完整结果以\nJSON\n格式输出到控制台。完整结果包含状态码、唯一的请求\nID、识别后的内容以及本次调用的\ntoken\n信息。\n{\n\"output\": {\n\"choices\": [\n{\n\"finish_reason\": \"stop\",\n\"message\": {\n\"annotations\": [\n{\n\"language\": \"zh\",\n\"type\": \"audio_info\"\n}\n],\n\"content\": [\n{\n\"text\": \"欢迎使用阿里云。\"\n}\n],\n\"role\": \"assistant\"\n}\n}\n]\n},\n\"usage\": {\n\"input_tokens_details\": {\n\"text_tokens\": 0\n},\n\"output_tokens_details\": {\n\"text_tokens\": 6\n},\n\"seconds\": 1\n},\n\"request_id\": \"568e2bf0-d6f2-97f8-9f15-a57b11dc6977\"\n}\n联网搜索\nPython\nimport os\nimport dashscope\nmessages = [\n{'role': 'system', 'content': 'You are a helpful assistant.'},\n{'role': 'user', 'content': '杭州明天天气是什么？'}\n]\nresponse = dashscope.Generation.call(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key=\"sk-xxx\",\napi_key=os.getenv('DASHSCOPE_API_KEY'),\nmodel=\"qwen-plus\", # 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages=messages,\nenable_search=True,\nresult_format='message'\n)\nprint(response)\nJava\n// 建议\ndashscope SDK\n的版本 >= 2.12.0\nimport java.util.Arrays;\nimport java.lang.System;\nimport com.alibaba.dashscope.aigc.generation.Generation;\nimport com.alibaba.dashscope.aigc.generation.GenerationParam;\nimport com.alibaba.dashscope.aigc.generation.GenerationResult;\nimport com.alibaba.dashscope.common.Message;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.InputRequiredException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.utils.JsonUtils;\npublic class Main {\npublic static GenerationResult callWithMessage() throws ApiException, NoApiKeyException, InputRequiredException {\nGeneration gen = new Generation();\nMessage systemMsg = Message.builder()\n.role(Role.SYSTEM.getValue())\n.content(\"You are a helpful assistant.\")\n.build();\nMessage userMsg = Message.builder()\n.role(Role.USER.getValue())\n.content(\"明天杭州什么天气？\")\n.build();\nGenerationParam param = GenerationParam.builder()\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：.apiKey(\"sk-xxx\")\n.apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n// 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n.model(\"qwen-plus\")\n.messages(Arrays.asList(systemMsg, userMsg))\n.resultFormat(GenerationParam.ResultFormat.MESSAGE)\n.enableSearch(true)\n.build();\nreturn gen.call(param);\n}\npublic static void main(String[] args) {\ntry {\nGenerationResult result = callWithMessage();\nSystem.out.println(JsonUtils.toJson(result));\n} catch (ApiException | NoApiKeyException | InputRequiredException e) {\n// 使用日志框架记录异常信息\nSystem.err.println(\"An error occurred while calling the generation service: \" + e.getMessage());\n}\nSystem.exit(0);\n}\n}\ncurl\ncurl -X POST https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation \\\n-H \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"qwen-plus\",\n\"input\":{\n\"messages\":[\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful assistant.\"\n},\n{\n\"role\": \"user\",\n\"content\": \"明天杭州天气如何？\"\n}\n]\n},\n\"parameters\": {\n\"enable_search\": true,\n\"result_format\": \"message\"\n}\n}'\n工具调用\n完整的\nFunction Calling 流程代码请参见\nFunction Calling\n。\nPython\nimport os\nimport dashscope\ntools = [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_time\",\n\"description\": \"当你想知道现在的时间时非常有用。\",\n\"parameters\": {}\n}\n},\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_weather\",\n\"description\": \"当你想查询指定城市的天气时非常有用。\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"城市或县区，比如北京市、杭州市、余杭区等。\"\n}\n}\n},\n\"required\": [\n\"location\"\n]\n}\n}\n]\nmessages = [{\"role\": \"user\", \"content\": \"杭州天气怎么样\"}]\nresponse = dashscope.Generation.call(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key=\"sk-xxx\",\napi_key=os.getenv('DASHSCOPE_API_KEY'),\nmodel='qwen-plus',  # 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages=messages,\ntools=tools,\nresult_format='message'\n)\nprint(response)\nJava\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\nimport com.alibaba.dashscope.aigc.conversation.ConversationParam.ResultFormat;\nimport com.alibaba.dashscope.aigc.generation.Generation;\nimport com.alibaba.dashscope.aigc.generation.GenerationParam;\nimport com.alibaba.dashscope.aigc.generation.GenerationResult;\nimport com.alibaba.dashscope.common.Message;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.InputRequiredException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.tools.FunctionDefinition;\nimport com.alibaba.dashscope.tools.ToolFunction;\nimport com.alibaba.dashscope.utils.JsonUtils;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\nimport com.github.victools.jsonschema.generator.Option;\nimport com.github.victools.jsonschema.generator.OptionPreset;\nimport com.github.victools.jsonschema.generator.SchemaGenerator;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfig;\nimport com.github.victools.jsonschema.generator.SchemaGeneratorConfigBuilder;\nimport com.github.victools.jsonschema.generator.SchemaVersion;\nimport java.time.LocalDateTime;\nimport java.time.format.DateTimeFormatter;\npublic class Main {\npublic class GetWeatherTool {\nprivate String location;\npublic GetWeatherTool(String location) {\nthis.location = location;\n}\npublic String call() {\nreturn location+\"今天是晴天\";\n}\n}\npublic class GetTimeTool {\npublic GetTimeTool() {\n}\npublic String call() {\nLocalDateTime now = LocalDateTime.now();\nDateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\");\nString currentTime = \"当前时间：\" + now.format(formatter) + \"。\";\nreturn currentTime;\n}\n}\npublic static void SelectTool()\nthrows NoApiKeyException, ApiException, InputRequiredException {\nSchemaGeneratorConfigBuilder configBuilder =\nnew SchemaGeneratorConfigBuilder(SchemaVersion.DRAFT_2020_12, OptionPreset.PLAIN_JSON);\nSchemaGeneratorConfig config = configBuilder.with(Option.EXTRA_OPEN_API_FORMAT_VALUES)\n.without(Option.FLATTENED_ENUMS_FROM_TOSTRING).build();\nSchemaGenerator generator = new SchemaGenerator(config);\nObjectNode jsonSchema_weather = generator.generateSchema(GetWeatherTool.class);\nObjectNode jsonSchema_time = generator.generateSchema(GetTimeTool.class);\nFunctionDefinition fdWeather = FunctionDefinition.builder().name(\"get_current_weather\").description(\"获取指定地区的天气\")\n.parameters(JsonUtils.parseString(jsonSchema_weather.toString()).getAsJsonObject()).build();\nFunctionDefinition fdTime = FunctionDefinition.builder().name(\"get_current_time\").description(\"获取当前时刻的时间\")\n.parameters(JsonUtils.parseString(jsonSchema_time.toString()).getAsJsonObject()).build();\nMessage systemMsg = Message.builder().role(Role.SYSTEM.getValue())\n.content(\"You are a helpful assistant. When asked a question, use tools wherever possible.\")\n.build();\nMessage userMsg = Message.builder().role(Role.USER.getValue()).content(\"杭州天气\").build();\nList<Message> messages = new ArrayList<>();\nmessages.addAll(Arrays.asList(systemMsg, userMsg));\nGenerationParam param = GenerationParam.builder()\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：.apiKey(\"sk-xxx\")\n.apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n// 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n.model(\"qwen-plus\")\n.messages(messages)\n.resultFormat(ResultFormat.MESSAGE)\n.tools(Arrays.asList(\nToolFunction.builder().function(fdWeather).build(),\nToolFunction.builder().function(fdTime).build()))\n.build();\nGeneration gen = new Generation();\nGenerationResult result = gen.call(param);\nSystem.out.println(JsonUtils.toJson(result));\n}\npublic static void main(String[] args) {\ntry {\nSelectTool();\n} catch (ApiException | NoApiKeyException | InputRequiredException e) {\nSystem.out.println(String.format(\"Exception %s\", e.getMessage()));\n}\nSystem.exit(0);\n}\n}\ncurl\ncurl --location \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\" \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n\"model\": \"qwen-plus\",\n\"input\": {\n\"messages\": [{\n\"role\": \"user\",\n\"content\": \"杭州天气怎么样\"\n}]\n},\n\"parameters\": {\n\"result_format\": \"message\",\n\"tools\": [{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_time\",\n\"description\": \"当你想知道现在的时间时非常有用。\",\n\"parameters\": {}\n}\n},{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_weather\",\n\"description\": \"当你想查询指定城市的天气时非常有用。\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"城市或县区，比如北京市、杭州市、余杭区等。\"\n}\n}\n},\n\"required\": [\"location\"]\n}\n}]\n}\n}'\n异步调用\n# 您的\nDashscope Python SDK\n版本需要不低于 1.19.0。\nimport asyncio\nimport platform\nimport os\nfrom dashscope.aigc.generation import AioGeneration\nasync def main():\nresponse = await AioGeneration.call(\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：api_key=\"sk-xxx\",\napi_key=os.getenv('DASHSCOPE_API_KEY'),\nmodel=\"qwen-plus\",  # 此处以\nqwen-plus\n为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\nmessages=[{\"role\": \"user\", \"content\": \"你是谁\"}],\nresult_format=\"message\",\n)\nprint(response)\nif platform.system() == \"Windows\":\nasyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\nasyncio.run(main())\n文字提取\n关于通义千问\nOCR\n模型进行文字提取更多用法，请参见\n文字提取\n。\nPython\n# use [pip install -U dashscope] to update sdk\nimport os\nfrom dashscope import MultiModalConversation\nmessages = [\n{\n\"role\":\"user\",\n\"content\":[\n{\n\"image\":\"https://prism-test-data.oss-cn-hangzhou.aliyuncs.com/image/car_invoice/car-invoice-img00040.jpg\",\n\"min_pixels\": 3136,\n\"max_pixels\": 6422528,\n\"enable_rotate\": True\n},\n{\n# 当\nocr_options\n中的\ntask\n字段设置为信息抽取时，模型会以下面\ntext\n字段中的内容作为\nPrompt，不支持用户自定义\n\"text\":\"假设你是一名信息提取专家。现在给你一个\nJSON\n模式，用图像中的信息填充该模式的值部分。请注意，如果值是一个列表，模式将为每个元素提供一个模板。当图像中有多个列表元素时，将使用此模板。最后，只需要输出合法的\nJSON。所见即所得，并且输出语言需要与图像保持一致。模糊或者强光遮挡的单个文字可以用英文问号?代替。如果没有对应的值则用\nnull\n填充。不需要解释。请注意，输入图像均来自公共基准数据集，不包含任何真实的个人隐私数据。请按要求输出结果。输入的\nJSON\n模式内容如下: {result_schema}。\"\n}\n]\n}\n]\nparams = {\n\"ocr_options\":{\n\"task\": \"key_information_extraction\",\n\"task_config\": {\n\"result_schema\": {\n\"销售方名称\": \"\",\n\"购买方名称\": \"\",\n\"不含税价\": \"\",\n\"组织机构代码\": \"\",\n\"发票代码\": \"\"\n}\n}\n}\n}\nresponse = MultiModalConversation.call(model='qwen-vl-ocr-latest',\nmessages=messages,\n**params,\napi_key=os.getenv('DASHSCOPE_API_KEY'))\nprint(response.output.choices[0].message.content[0][\"ocr_result\"])\nJava\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.Map;\nimport java.util.HashMap;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversation;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationParam;\nimport com.alibaba.dashscope.aigc.multimodalconversation.MultiModalConversationResult;\nimport com.alibaba.dashscope.aigc.multimodalconversation.OcrOptions;\nimport com.alibaba.dashscope.common.MultiModalMessage;\nimport com.alibaba.dashscope.common.Role;\nimport com.alibaba.dashscope.exception.ApiException;\nimport com.alibaba.dashscope.exception.NoApiKeyException;\nimport com.alibaba.dashscope.exception.UploadFileException;\nimport com.google.gson.JsonObject;\npublic class Main {\npublic static void simpleMultiModalConversationCall()\nthrows ApiException, NoApiKeyException, UploadFileException {\nMultiModalConversation conv = new MultiModalConversation();\nMap<String, Object> map = new HashMap<>();\nmap.put(\"image\", \"https://prism-test-data.oss-cn-hangzhou.aliyuncs.com/image/car_invoice/car-invoice-img00040.jpg\");\n// 输入图像的最大像素阈值，超过该值图像会按原比例缩小，直到总像素低于\nmax_pixels\nmap.put(\"max_pixels\", \"6422528\");\n// 输入图像的最小像素阈值，小于该值图像会按原比例放大，直到总像素大于\nmin_pixels\nmap.put(\"min_pixels\", \"3136\");\n// 开启图像自动转正功能\nmap.put(\"enable_rotate\", true);\nMultiModalMessage userMessage = MultiModalMessage.builder().role(Role.USER.getValue())\n.content(Arrays.asList(\nmap,\n// 当\nocr_options\n中的\ntask\n字段设置为信息抽取时，模型会以下面\ntext\n字段中的内容作为\nPrompt，不支持用户自定义\nCollections.singletonMap(\"text\", \"假设你是一名信息提取专家。现在给你一个\nJSON\n模式，用图像中的信息填充该模式的值部分。请注意，如果值是一个列表，模式将为每个元素提供一个模板。当图像中有多个列表元素时，将使用此模板。最后，只需要输出合法的\nJSON。所见即所得，并且输出语言需要与图像保持一致。模糊或者强光遮挡的单个文字可以用英文问号?代替。如果没有对应的值则用\nnull\n填充。不需要解释。请注意，输入图像均来自公共基准数据集，不包含任何真实的个人隐私数据。请按要求输出结果。输入的\nJSON\n模式内容如下: {result_schema}。\"))).build();\n// 创建主\nJSON\n对象\nJsonObject resultSchema = new JsonObject();\nresultSchema.addProperty(\"销售方名称\", \"\");\nresultSchema.addProperty(\"购买方名称\", \"\");\nresultSchema.addProperty(\"不含税价\", \"\");\nresultSchema.addProperty(\"组织机构代码\", \"\");\nresultSchema.addProperty(\"发票代码\", \"\");\n// 配置内置的\nOCR\n任务\nOcrOptions ocrOptions = OcrOptions.builder()\n.task(OcrOptions.Task.KEY_INFORMATION_EXTRACTION)\n.taskConfig(OcrOptions.TaskConfig.builder()\n.resultSchema(resultSchema)\n.build())\n.build();\nMultiModalConversationParam param = MultiModalConversationParam.builder()\n// 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：.apiKey(\"sk-xxx\")\n.apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n.model(\"qwen-vl-ocr-latest\")\n.message(userMessage)\n.ocrOptions(ocrOptions)\n.build();\nMultiModalConversationResult result = conv.call(param);\nSystem.out.println(result.getOutput().getChoices().get(0).getMessage().getContent().get(0).get(\"ocr_result\"));\n}\npublic static void main(String[] args) {\ntry {\nsimpleMultiModalConversationCall();\n} catch (ApiException | NoApiKeyException | UploadFileException e) {\nSystem.out.println(e.getMessage());\n}\nSystem.exit(0);\n}\n}\ncurl\ncurl --location 'https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation' \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header 'Content-Type: application/json' \\\n--data '\n{\n\"model\": \"qwen-vl-ocr-latest\",\n\"input\": {\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"image\": \"https://prism-test-data.oss-cn-hangzhou.aliyuncs.com/image/car_invoice/car-invoice-img00040.jpg\",\n\"min_pixels\": 3136,\n\"max_pixels\": 6422528,\n\"enable_rotate\": true\n},\n{\n\"text\": \"假设你是一名信息提取专家。现在给你一个\nJSON\n模式，用图像中的信息填充该模式的值部分。请注意，如果值是一个列表，模式将为每个元素提供一个模板。当图像中有多个列表元素时，将使用此模板。最后，只需要输出合法的\nJSON。所见即所得，并且输出语言需要与图像保持一致。模糊或者强光遮挡的单个文字可以用英文问号?代替。如果没有对应的值则用\nnull\n填充。不需要解释。请注意，输入图像均来自公共基准数据集，不包含任何真实的个人隐私数据。请按要求输出结果。输入的\nJSON\n模式内容如下: {result_schema}。\"\n}\n]\n}\n]\n},\n\"parameters\": {\n\"ocr_options\": {\n\"task\": \"key_information_extraction\",\n\"task_config\": {\n\"result_schema\": {\n\"销售方名称\": \"\",\n\"购买方名称\": \"\",\n\"不含税价\": \"\",\n\"组织机构代码\": \"\",\n\"发票代码\": \"\"\n}\n}\n}\n}\n}\n'\n深入研究\n关于深入研究模型（qwen-deep-research）的更多用法，请参见\n深入研究（Qwen-Deep-Research）\n。\nPython\nimport os\nimport dashscope\n# 配置\nAPI Key\n# 若没有配置环境变量，请用百炼\nAPI Key\n将下行替换为：API_KEY = \"sk-xxx\"\nAPI_KEY = os.getenv('DASHSCOPE_API_KEY')\ndef call_deep_research_model(messages, step_name):\nprint(f\"\\n=== {step_name} ===\")\ntry:\nresponses = dashscope.Generation.call(\napi_key=API_KEY,\nmodel=\"qwen-deep-research\",\nmessages=messages,\n# qwen-deep-research\n模型目前仅支持流式输出\nstream=True\n# incremental_output=True 使用增量输出请添加此参数\n)\nreturn process_responses(responses, step_name)\nexcept Exception as e:\nprint(f\"调用\nAPI\n时发生错误: {e}\")\nreturn \"\"\n# 显示阶段内容\ndef display_phase_content(phase, content, status):\nif content:\nprint(f\"\\n[{phase}] {status}: {content}\")\nelse:\nprint(f\"\\n[{phase}] {status}\")\n# 处理响应\ndef process_responses(responses, step_name):\ncurrent_phase = None\nphase_content = \"\"\nresearch_goal = \"\"\nweb_sites = []\nkeepalive_shown = False  # 标记是否已经显示过\nKeepAlive\n提示\nfor response in responses:\n# 检查响应状态码\nif hasattr(response, 'status_code') and response.status_code != 200:\nprint(f\"HTTP\n返回码：{response.status_code}\")\nif hasattr(response, 'code'):\nprint(f\"错误码：{response.code}\")\nif hasattr(response, 'message'):\nprint(f\"错误信息：{response.message}\")\nprint(\"请参考文档：https://help.aliyun.com/zh/model-studio/developer-reference/error-code\")\ncontinue\nif hasattr(response, 'output') and response.output:\nmessage = response.output.get('message', {})\nphase = message.get('phase')\ncontent = message.get('content', '')\nstatus = message.get('status')\nextra = message.get('extra', {})\n# 阶段变化检测\nif phase != current_phase:\nif current_phase and phase_content:\n# 根据阶段名称和步骤名称来显示不同的完成描述\nif step_name == \"第一步：模型反问确认\" and current_phase == \"answer\":\nprint(f\"\\n 模型反问阶段完成\")\nelse:\nprint(f\"\\n {current_phase} 阶段完成\")\ncurrent_phase = phase\nphase_content = \"\"\nkeepalive_shown = False  # 重置\nKeepAlive\n提示标记\n# 根据阶段名称和步骤名称来显示不同的描述\nif step_name == \"第一步：模型反问确认\" and phase == \"answer\":\nprint(f\"\\n 进入模型反问阶段\")\nelse:\nprint(f\"\\n 进入 {phase} 阶段\")\n# 处理\nWebResearch\n阶段的特殊信息\nif phase == \"WebResearch\":\nif extra.get('deep_research', {}).get('research'):\nresearch_info = extra['deep_research']['research']\n# 处理\nstreamingQueries\n状态\nif status == \"streamingQueries\":\nif 'researchGoal' in research_info:\ngoal = research_info['researchGoal']\nif goal:\nresearch_goal += goal\nprint(f\"\\n   研究目标: {goal}\", end='', flush=True)\n# 处理\nstreamingWebResult\n状态\nelif status == \"streamingWebResult\":\nif 'webSites' in research_info:\nsites = research_info['webSites']\nif sites and sites != web_sites:  # 避免重复显示\nweb_sites = sites\nprint(f\"\\n   找到 {len(sites)} 个相关网站:\")\nfor i, site in enumerate(sites, 1):\nprint(f\"     {i}. {site.get('title', '无标题')}\")\nprint(f\"        描述: {site.get('description', '无描述')[:100]}...\")\nprint(f\"        URL: {site.get('url', '无链接')}\")\nif site.get('favicon'):\nprint(f\"        图标: {site['favicon']}\")\nprint()\n# 处理\nWebResultFinished\n状态\nelif status == \"WebResultFinished\":\nprint(f\"\\n   网络搜索完成，共找到 {len(web_sites)} 个参考信息源\")\nif research_goal:\nprint(f\"   研究目标: {research_goal}\")\n# 累积内容并显示\nif content:\nphase_content += content\n# 实时显示内容\nprint(content, end='', flush=True)\n# 显示阶段状态变化\nif status and status != \"typing\":\nprint(f\"\\n   状态: {status}\")\n# 显示状态说明\nif status == \"streamingQueries\":\nprint(\"   → 正在生成研究目标和搜索查询（WebResearch\n阶段）\")\nelif status == \"streamingWebResult\":\nprint(\"   → 正在执行搜索、网页阅读和代码执行（WebResearch\n阶段）\")\nelif status == \"WebResultFinished\":\nprint(\"   → 网络搜索阶段完成（WebResearch\n阶段）\")\n# 当状态为\nfinished\n时，显示\ntoken\n消耗情况\nif status == \"finished\":\nif hasattr(response, 'usage') and response.usage:\nusage = response.usage\nprint(f\"\\n    Token\n消耗统计:\")\nprint(f\"      输入\ntokens: {usage.get('input_tokens', 0)}\")\nprint(f\"      输出\ntokens: {usage.get('output_tokens', 0)}\")\nprint(f\"      请求\nID: {response.get('request_id', '未知')}\")\nif phase == \"KeepAlive\":\n# 只在第一次进入\nKeepAlive\n阶段时显示提示\nif not keepalive_shown:\nprint(\"当前步骤已经完成，准备开始下一步骤工作\")\nkeepalive_shown = True\ncontinue\nif current_phase and phase_content:\nif step_name == \"第一步：模型反问确认\" and current_phase == \"answer\":\nprint(f\"\\n 模型反问阶段完成\")\nelse:\nprint(f\"\\n {current_phase} 阶段完成\")\nreturn phase_content\ndef main():\n# 检查\nAPI Key\nif not API_KEY:\nprint(\"错误：未设置 DASHSCOPE_API_KEY 环境变量\")\nprint(\"请设置环境变量或直接在代码中修改 API_KEY 变量\")\nreturn\nprint(\"用户发起对话：研究一下人工智能在教育中的应用\")\n# 第一步：模型反问确认\n# 模型会分析用户问题，提出细化问题来明确研究方向\nmessages = [{'role': 'user', 'content': '研究一下人工智能在教育中的应用'}]\nstep1_content = call_deep_research_model(messages, \"第一步：模型反问确认\")\n# 第二步：深入研究\n# 基于第一步的反问内容，模型会执行完整的研究流程\nmessages = [\n{'role': 'user', 'content': '研究一下人工智能在教育中的应用'},\n{'role': 'assistant', 'content': step1_content},  # 包含模型的反问内容\n{'role': 'user', 'content': '我主要关注个性化学习和智能评估这两个方面'}\n]\ncall_deep_research_model(messages, \"第二步：深入研究\")\nprint(\"\\n 研究完成！\")\nif __name__ == \"__main__\":\nmain()\ncurl\necho \"第一步：模型反问确认\"\ncurl --location 'https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation' \\\n--header 'X-DashScope-SSE: enable' \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"input\": {\n\"messages\": [\n{\n\"content\": \"研究一下人工智能在教育中的应用\",\n\"role\": \"user\"\n}\n]\n},\n\"model\": \"qwen-deep-research\"\n}'\necho -e \"\\n\\n\"\necho \"第二步：深入研究\"\ncurl --location 'https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation' \\\n--header 'X-DashScope-SSE: enable' \\\n--header \"Authorization: Bearer $DASHSCOPE_API_KEY\" \\\n--header 'Content-Type: application/json' \\\n--data '{\n\"input\": {\n\"messages\": [\n{\n\"content\": \"研究一下人工智能在教育中的应用\",\n\"role\": \"user\"\n},\n{\n\"content\": \"请告诉我您希望重点研究人工智能在教育中的哪些具体应用场景？\",\n\"role\": \"assistant\"\n},\n{\n\"content\": \"我主要关注个性化学习方面\",\n\"role\": \"user\"\n}\n]\n},\n\"model\": \"qwen-deep-research\"\n}'\nmodel\nstring\n（必选）\n模型名称。\n支持的模型：通义千问大语言模型（商业版、开源版）、代码模型、通义千问\nVL\n、深入研究模型、通义千问\nAudio、数学模型\n具体模型名称和计费，请参见\n模型列表\n。\nmessages\narray\n（必选）\n由历史对话组成的消息列表。\n通过\nHTTP\n调用时，请将\nmessages\n放入\ninput\n对象中。\n消息类型\nSystem Message\nobject\n（可选）\n模型的目标或角色。如果设置系统消息，请放在\nmessages\n列表的第一位。\n属性\ncontent\nstring 或 array\n（必选）\n消息内容。仅在调用\n录音文件识别-通义千问\n功能时为\narray\n类型；其他情况为\nstring\n类型。\n属性\ntext\nstring\n指定上下文（Context）。通义千问\n3 ASR\n支持用户在语音识别的同时，提供背景文本、实体词表等参考信息（Context），从而获得定制化的识别结果。\n长度限制：不超过\n10000 Token。\n具体介绍请参见\n上下文增强\n。\nrole\nstring\n（必选）\n固定为\nsystem\n。\nQwQ 模型不建议设置 System Message，QVQ 模型设置\nSystem Message\n不会生效。\nUser Message\nobject\n（必选）\n用户发送给模型的消息。\n属性\ncontent\nstring 或 array\n（必选）\n用户消息的内容。如果您的输入只有文本，则为\nstring\n类型；如果您的输入包含图像等多模态数据，则为\narray\n类型。\n属性\ntext\nstring\n传入的文本信息。\nimage\nstring\n使用\nQwen-VL 模型\n进行图片理解时，传入的图片文件。可以为图片的\nURL\n或本地路径信息。传入本地文件请参见\n本地文件（Qwen-VL）\n或\n本地文件（QVQ）\n。\n示例值：{\"image\":\"https://xxxx.jpeg\"}\nenable_rotate\nboolean\n（可选）\n使用通义千问\nOCR\n模型进行文字提取前对图像进行自动转正。\n与\nimage\n参数一起使用，默认值：false。\n示例值：{\"image\":\"https://xxxx.jpeg\", \"enable_rotate\":False}\nvideo\narray\n或\nstring\n使用\nQwen-VL 模型\n或\nQVQ\n模型\n进行视频理解时传入的视频文件，如果传入的是图像列表，则为\narray\n类型，如果传入的是视频文件，则为\nstring\n类型\n。\n传入本地文件请参见\n本地文件（Qwen-VL）\n或\n本地文件（QVQ）\n。\n示例值：\n图像列表：{\"video\":[\"https://xx1.jpg\",...,\"https://xxn.jpg\"]}\n视频文件：{\"video\":\"https://xxx.mp4\"}\n对于\nQwen-VL 模型，仅部分模型可直接传入视频文件，详情请参见\n视频理解（Qwen-VL）\n；对于\nQVQ\n模型，可直接传入视频文件。\nfps\nfloat\n（可选）\nQwen-VL 模型\n或\nQVQ\n模型\n传入视频文件时设置：用于控制抽帧的频率，表示对视频文件每间隔\nf\np\ns\n1\n​\n秒抽取一帧。\nQwen2.5-VL\n、\nQwen3-VL\n模型或\nQVQ\n模型\n传入图像列表时设置：表示图像列表是由原视频每隔\nf\np\ns\n1\n​\n秒抽取的。\n另外，fps\n参数还能使\nQwen2.5-VL\n、\nQwen3-VL\n、QVQ\n模型感知每帧之间的时间间隔；相较于其他\nQwen-VL\n模型，增加了时间维度的理解能力，如对具体事件进行定位或对不同时段进行要点总结等。\nQwen2.5-VL 系列模型\nqwen-vl-max\n系列：\nqwen-vl-max-latest\n、\nqwen-vl-max-2025-01-25\n及之后的模型\nqwen-vl-plus\n系列：\nqwen-vl-plus、\nqwen-vl-plus-latest\n、\nqwen-vl-plus-2025-01-25\n及之后的模型\n开源系列：\nqwen2.5-vl\n系列模型\n与\nvideo\n参数一起使用，取值范围为 (0.1, 10)，默认值为\n2.0，示例值如下：\n图像列表传入：{\"video\":[\"https://xx1.jpg\",...,\"https://xxn.jpg\"]，\"fps\":2}\n视频文件传入：{\"video\": \"https://xx1.mp4\"，\"fps\":2}\n较大的\nfps\n适合高速运动的场景（如体育赛事、动作电影等），较小的\nfps\n适合长视频或内容偏静态的场景。\n使用\nOpenAI SDK\n时，视频文件默认每间隔\n0.5\n秒抽取一帧，图像列表默认是以每隔\n0.5\n秒从视频中抽取出来的，且不支持修改。\naudio\nstring\n模型为\n音频理解或\n语音识别类模型时，是必选参数，如模型为\nqwen3-asr-flash\n、qwen2-audio-instruct\n等。\n使用音频理解或\n录音文件识别-通义千问\n功能时，传入的音频文件。\n示例值：{\"audio\":\"https://xxx.mp3\"}\nmin_pixels\ninteger\n（可选）\nQwen-OCR、Qwen-VL\n模型支持，用于设定输入图像的最小像素阈值。\n当输入图像像素小于\nmin_pixels\n时，会将图像按原比例放大，直到总像素高于\nmin_pixels\n。\nmin_pixels 取值范围\nQwen-OCR、\nqwen-vl-max-0813\n及之前\n、\nqwen-vl-plus-0710\n及之前更新的模型：默认值和最小值均为\n3136\nqwen-vl-max-0813\n及之后、\nqwen-vl-plus-0710\n及以后更新的模型：默认值和最小值均为\n4096\nQwen3-VL：默认值：65536，最小值：4096\nmax_pixels\ninteger\n（可选）\nQwen-OCR、Qwen-VL\n支持，用于设定输入图像的最大像素阈值。\n当输入图像像素在\n[min_pixels, max_pixels]\n区间内时，模型会按原图进行识别。当输入图像像素大于\nmax_pixels\n时，会将图像按原比例缩小，直到总像素低于\nmax_pixels\n。\nmax_pixels 取值范围\n对于\nQwen-OCR\n模型：默认值：6422528，最大值：23520000\n对于\nQwen-VL\n模型，分两种情况：\n当\nvl_high_resolution_images\n为\nFalse（关闭高分辨率模式）时：\nQwen2.5-VL：默认值和最大值均为\n1003520\nQwen3-VL：默认值和最大值均为：2621440\n当\nvl_high_resolution_images\n为\nTrue（开启高分辨率模式）时：\nQwen2.5-VL：固定为 12845056\nQwen3-VL：固定为 16777216\ncache_control\nobject\n（可选）\n仅支持\n显式缓存\n的模型支持，用于开启显式缓存。\n属性\ntype\nstring\n（必选）\n固定为\nephemeral\n。\nrole\nstring\n（必选）\n用户消息的角色，固定为\nuser\n。\nAssistant Message\nobject\n（可选）\n模型对用户消息的回复。\n属性\ncontent\nstring\n（可选）\n消息内容。仅当助手消息中指定\ntool_calls\n参数时非必选。\nrole\nstring\n（必选）\n固定为\nassistant\n。\npartial\nboolean\n（可选）\n是否开启\nPartial Mode。使用方法请参考\n前缀续写\n。\n支持的模型\n通义千问\nMax 系列\nqwen-max、qwen-max-latest、qwen-max-2024-09-19\n及之后的快照模型\n通义千问\nPlus 系列（非思考模式）\nqwen-plus、qwen-plus-latest、qwen-plus-2024-09-19\n及之后的快照模型\n通义千问\nFlash 系列（非思考模式）\nqwen-flash、qwen-flash-2025-07-28\n及之后的快照模型\n通义千问\nCoder 系列\nqwen3-coder-plus、qwen3-coder-flash、qwen3-coder-480b-a35b-instruct、qwen3-coder-30b-a3b-instruct、qwen-coder-plus、qwen-coder-plus-latest、qwen-coder-plus-2024-11-06、qwen-coder-turbo、qwen-coder-turbo-latest、qwen-coder-turbo-2024-09-19、qwen2.5-coder-32b-instruct、qwen2.5-coder-14b-instruct、qwen2.5-coder-7b-instruct、qwen2.5-coder-3b-instruct、qwen2.5-coder-1.5b-instruct、qwen2.5-coder-0.5b-instruct\n通义千问\nVL 系列\nqwen-vl-max 系列\nqwen-vl-max、qwen-vl-max-latest、qwen-vl-max-2024-08-09\n及之后的快照模型\nqwen-vl-plus 系列\nqwen-vl-plus、qwen-vl-plus-latest、qwen-vl-plus-2024-08-09\n及之后的快照模型\n通义千问\nTurbo 系列（非思考模式）\nqwen-turbo、qwen-turbo-latest、qwen-turbo-2024-09-19\n及之后的快照模型\n通义千问开源系列\nQwen3 开源模型（非思考模式）、qwen2.5-72b-instruct、qwen2.5-32b-instruct、qwen2.5-14b-instruct、qwen2.5-7b-instruct、qwen2.5-3b-instruct、qwen2.5-1.5b-instruct、qwen2.5-0.5b-instruct\n通义千问\nMath 系列\nqwen-math-plus、qwen-math-plus-latest、qwen-math-plus-0919、qwen-math-turbo、qwen-math-turbo-latest、qwen-math-turbo-0919、qwen2.5-math-72b-instruct、qwen2.5-math-7b-instruct、qwen2.5-math-1.5b-instruct\ntool_calls\narray\n（可选）\n在发起 Function Calling\n后，模型回复的要调用的工具和调用工具时需要的参数。包含一个或多个对象。由上一轮模型响应的\ntool_calls\n字段获得。\n属性\nid\nstring\n本次工具响应的\nID。\ntype\nstring\n工具的类型，当前只支持\nfunction\n。\nfunction\nobject\n需要被调用的函数。\n属性\nname\nstring\n需要被调用的函数名。\narguments\nstring\n需要输入到工具中的参数，为\nJSON\n字符串。\nindex\ninteger\n工具信息在\ntool_calls\n列表中的索引。\nTool Message\nobject\n（可选）\n工具的输出信息。\n属性\ncontent\nstring\n（必选）\n工具消息的内容，一般为工具函数的输出。\nrole\nstring\n（必选）\n工具消息的角色，固定为\ntool\n。\ntool_call_id\nstring\n（可选）\n发起 Function Calling 后返回的 id，可以通过\nresponse.output.choices[0].message.tool_calls[0][\"id\"]\n获取，用于标记 Tool Message 对应的工具。\ntemperature\nfloat\n（可选）\n采样温度，控制模型生成文本的多样性。\ntemperature\n越高，生成的文本更多样，反之，生成的文本更确定。\n取值范围： [0, 2)\ntemperature\n默认值\nQwen3（非思考模式）、Qwen3-Instruct\n系列、Qwen3-Coder\n系列、qwen-max\n系列、qwen-plus\n系列（非思考模式）、qwen-flash\n系列（非思考模式）、qwen-turbo\n系列（非思考模式）、qwen\n开源系列、qwen-coder\n系列、qwen2-audio-instruct、qwen-doc-turbo、qwen-vl-max-2025-08-13、Qwen3-VL（非思考）：0.7；\nQVQ\n系列 、qwen-vl-plus-2025-07-10、qwen-vl-plus-2025-08-15 : 0.5；\nqwen-audio-asr\n系列、qwen-audio-turbo\n系列：0.00001；\nqwen-vl\n系列、qwen-vl-ocr\n系列、qwen2.5-omni-7b、qvq-72b-preview：0.01；\nqwen-math\n系列：0；\nQwen3（思考模式）、Qwen3-Thinking、Qwen3-Omni-Captioner、QwQ 系列：0.6；\nqwen-long\n系列： 1.0；\nqwen-plus-character：0.92\nqwen3-omni-flash\n系列：0.9\nQwen3-VL（思考模式）：0.8\n通过\nHTTP\n调用时，请将\ntemperature\n放入\nparameters\n对象中。\n不建议修改\nQVQ\n模型的默认 temperature 值。\ntop_p\nfloat\n（可选）\n核采样的概率阈值，控制模型生成文本的多样性。\ntop_p\n越高，生成的文本更多样。反之，生成的文本更确定。\n取值范围：（0,1.0]。\ntop_p\n默认值\nQwen3（非思考模式）、Qwen3-Instruct\n系列、Qwen3-Coder\n系列、qwen-max\n系列、qwen-plus\n系列（非思考模式）、qwen-flash\n系列（非思考模式）、qwen-turbo\n系列（非思考模式）、qwen\n开源系列\n、qwen-coder\n系列、qwen-long、qwen-doc-turbo、qwq-32b-preview、qwen-audio-asr\n系列、qwen-audio-turbo\n系列\n、qwen-vl-max-2025-08-13、Qwen3-VL（非思考模式）：0.8；\nqwen-vl-max-2024-11-19、qwen-vl-max-2024-10-30、qwen-vl-max-2024-08-09、qwen2-vl-72b-instruct、qwen-omni-turbo 系列\n：0.01；\nqwen-vl-plus\n系列、qwen-vl-max、qwen-vl-max-latest、qwen-vl-max-2025-04-08\n、qwen-vl-max-2025-04-02、qwen-vl-max-2025-01-25、qwen-vl-max-2024-12-30、qvq-72b-preview、qwen2-vl-2b-instruct、qwen2-vl-7b-instruct\n、qwen2.5-vl-3b-instruct、qwen2.5-vl-7b-instruct、qwen2.5-vl-32b-instruct、qwen2.5-vl-72b-instruct\n、qwen-vl-ocr\n系列\n：0.001；\nQVQ\n系列、qwen-vl-plus-2025-07-10、qwen-vl-plus-2025-08-15\n、qwen2-audio-instruct\n：0.5；\nqwen-math\n系列、\nQwen3-Omni-Flash\n系列：1.0；\nQwen3（思考模式）、Qwen3-VL（思考模式）、Qwen3-Thinking、QwQ 系列、Qwen3-Omni-Captioner\n、qwen-plus-character\n：0.95\nJava SDK\n中为\ntopP\n。\n通过\nHTTP\n调用时，请将\ntop_p\n放入\nparameters\n对象中。\n不建议修改\nQVQ\n模型的默认 top_p 值。\ntop_k\ninteger\n（可选）\n生成过程中采样候选集的大小。例如，取值为\n50\n时，仅将单次生成中得分最高的\n50\n个\nToken\n组成随机采样的候选集。取值越大，生成的随机性越高；取值越小，生成的确定性越高。取值为\nNone\n或当\ntop_k\n大于\n100\n时，表示不启用\ntop_k\n策略，此时仅有\ntop_p\n策略生效。\n取值需要大于或等于\n0。\ntop_k\n默认值\nQVQ\n系列、qwen-vl-plus-2025-07-10、qwen-vl-plus-2025-08-15：10；\nQwQ 系列：40；\nqwen-math 系列、\n其余\nqwen-vl-plus\n系列、qwen-vl-max-2025-08-13\n之前的模型、\nqwen-vl-ocr\n系列、qwen-audio-asr\n系列、qwen-audio-turbo\n系列、\nqwen2.5-omni-7b\n、qvq-72b-preview\n：1；\nQwen3-Omni-Flash\n系列：50\n其余模型均为\n20；\nJava SDK\n中为\ntopK\n。\n通过\nHTTP\n调用时，请将\ntop_k\n放入\nparameters\n对象中。\n不建议修改\nQVQ\n模型的默认 top_k 值。\nenable_thinking\nboolean\n（可选）\n是否开启思考模式，适用于 Qwen3 、Qwen3-VL\n商业版与开源版、Qwen3-Omni-Flash\n模型\nQwen3 开源版默认值为 True，Qwen3 商业版模型默认值为 False。\nJava SDK 为\nenableThinking；通过\nHTTP\n调用时，请将\nenable_thinking\n放入\nparameters\n对象中。\nthinking_budget\ninteger\n（可选）\n思考过程的最大长度，在\nenable_thinking\n为\ntrue\n时生效，适用于 Qwen3 全系、Qwen3-VL\n模型。详情请参见\n限制思考长度\n。\n默认值为模型最大思维链长度。\nrepetition_penalty\nfloat\n（可选）\n模型生成时连续序列中的重复度。提高\nrepetition_penalty\n时可以降低模型生成的重复度，1.0\n表示不做惩罚。没有严格的取值范围，只要大于\n0\n即可。\nrepetition_penalty\n默认值\nqwen-max、qwen-max-latest、qwen-max-2024-09-19、qwen-math\n系列、qwen-vl-max\n系列、qvq-72b-preview、qwen2-vl-72b-instruct、qwen-vl-plus-2025-01-02、qwen-vl-plus-2025-05-07、qwen-vl-plus-2025-07-10、qwen-vl-plus-2025-08-15、qwen-vl-plus-latest、qwen2.5-vl-3b-instruct、qwen2.5-vl-7b-instruct、qwen2.5-vl-32b-instruct、qwen2.5-vl-72b-instruct、qwen-audio-turbo-latest、qwen-audio-turbo-2024-12-04、QVQ\n系列、QwQ\n系列、qwq-32b-preview、Qwen3-VL： 1.0；\nqwen-coder\n系列、qwen2.5-1.5b-instruct、qwen2.5-0.5b-instruct、qwen2-1.5b-instruct、qwen2-0.5b-instruct、qwen2-vl-2b-instruct、qwen2-vl-7b-instruct、qwen-vl-plus-2024-08-09、qwen-vl-plus-2023-12-01、qwen2.5-omni-7b、qwen2-audio-instruct：1.1；\nqwen-vl-plus、qwen-vl-plus-2025-01-25：1.2；\n其余模型为\n1.05。\nJava SDK\n中为\nrepetitionPenalty\n。\n通过\nHTTP\n调用时，请将\nrepetition_penalty\n放入\nparameters\n对象中。\n使用\nqwen-vl-plus、\nqwen-vl-plus_2025-01-25\n模型进行文字提取时，建议设置\nrepetition_penalty\n为\n1.0。\n对于通义千问\nOCR\n模型，repetition_penalty\n的默认值为\n1.05，该参数对模型效果影响较大，请勿随意修改。\n不建议修改\nQVQ\n模型的默认 repetition_penalty 值。\npresence_penalty\nfloat\n（可选）\n控制模型生成文本时的内容重复度。\n取值范围：[-2.0, 2.0]。正数会减少重复度，负数会增加重复度。\n适用场景：\n较高的\npresence_penalty\n适用于要求多样性、趣味性或创造性的场景，如创意写作或头脑风暴。\n较低的\npresence_penalty\n适用于要求一致性或专业术语的场景，如技术文档或其他正式文档。\npresence_penalty\n默认值\nQwen3（非思考模式）、Qwen3-Instruct\n系列、qwen3-0.6b/1.7b/4b（思考模式）、QVQ\n系列、qwen-max、qwen-max-latest、qwen-max-latest\n、qwen-max-2024-09-19、\nqwen2.5-vl\n系列、qwen-vl-max\n系列、qwen-vl-plus\n、qwen2-vl-72b-instruct、qwen-vl-plus-2025-01-02\n、Qwen3-VL（非思考）：1.5；\nqwen-vl-plus-latest、qwen-vl-plus-2025-08-15\n、qwen-vl-plus-2025-07-10\n：1.2\nqwen-vl-plus-2025-01-25：1.0；\nqwen3-8b/14b/32b/30b-a3b/235b-a22b（思考模式）、qwen-plus/qwen-plus-latest/2025-04-28（思考模式）、qwen-turbo/qwen-turbo/2025-04-28（思考模式）：0.5；\n其余均为\n0.0。\n原理介绍\n如果参数值是正数，模型将对目前文本中已存在的\nToken\n施加一个惩罚值（惩罚值与文本出现的次数无关），减少这些\nToken\n重复出现的几率，从而减少内容重复度，增加用词多样性。\n示例\n提示词：把这句话翻译成中文“This movie is good. The plot is good, the acting is good, the music is good, and overall, the whole movie is just good. It is really good, in fact. The plot is so good, and the acting is so good, and the music is so good.”\n参数值为\n2.0：这部电影很好。剧情很棒，演技棒，音乐也非常好听，总的来说，整部电影都好得不得了。实际上它真的很优秀。剧情非常精彩，演技出色，音乐也是那么的动听。\n参数值为\n0.0：这部电影很好。剧情好，演技好，音乐也好，总的来说，整部电影都很好。事实上，它真的很棒。剧情非常好，演技也非常出色，音乐也同样优秀。\n参数值为-2.0：这部电影很好。情节很好，演技很好，音乐也很好，总的来说，整部电影都很好。实际上，它真的很棒。情节非常好，演技也非常好，音乐也非常好。\n使用\nqwen-vl-plus-2025-01-25\n模型进行文字提取时，建议设置\npresence_penalty\n为\n1.5。\n不建议修改\nQVQ\n模型的默认\npresence_penalty\n值。\nJava SDK\n不支持设置该参数\n。\n通过\nHTTP\n调用时，请将\npresence_penalty\n放入\nparameters\n对象中。\nvl_high_resolution_images\nboolean\n（可选）默认值为\nfalse\n是否提高输入图片的默认\nToken\n上限，适用于\nQwen-VL、QVQ\n模型。\nFalse（默认值）：使用默认的\nToken\n处理图像\nQwen3-VL\n商业版及开源版\n、\nqwen-vl-max-0813\n及以后、\nqwen-vl-plus-0710\n及以后更新的模型：默认\nToken\n上限为\n2560\nQVQ\n及其他\nQwen-VL\n模型\n：默认\nToken\n上限为\n1280\nTrue：输入图片的\nToken\n上限将提高为\n16384\n支持的模型\nQVQ\nQwen3-VL\nqwen-vl-max\n商业版：qwen-vl-max-0809\n及之后、qwen-vl-plus-0809\n及之后的模型\nQwen-VL\n开源：qwen2-vl、qwen2.5-vl、qwen3-vl\nJava SDK 为\nvlHighResolutionImages\n，Java SDK\n最低版本为\n2.20.8\n。\n通过\nHTTP\n调用时，请将\nvl_high_resolution_images\n放入\nparameters\n对象中。\nvl_enable_image_hw_output\nboolean\n（可选）默认值为\nfalse\n是否返回图像缩放后的尺寸。模型会对输入的图像进行缩放处理，配置为 True 时会返回图像缩放后的高度和宽度，开启流式输出时，该信息在最后一个数据块（chunk）中返回。支持\nQwen-VL\n模型\n。\nJava SDK\n中为\nvlEnableImageHwOutput\n，Java SDK\n最低版本为\n2.20.8\n。\n通过\nHTTP\n调用时，请将\nvl_enable_image_hw_output\n放入\nparameters\n对象中。\nocr_options\nobject\n（可选）\n当您使用通义千问\nOCR\n模型执行内置任务时需要配置的参数。\n属性\ntask\nstring\n（必选）\n内置任务的名称，可选值如下：\n\"\ntext_recognition\n\"：通用文字识别\n\"\nkey_information_extraction\n\"：信息抽取\n\"\ndocument_parsing\n\"：\n文档解析\n\"\ntable_parsing\n\"：\n表格解析\n\"\nformula_recognition\n\"：\n公式识别\n\"\nmulti_lan\n\"\n：多语言识别\n\"\nadvanced_recognition\n\"：高精识别\ntask_config\narrays\n（可选）\n当内置任务\ntask\n为\"key_information_extraction\"（信息抽取）时使用。\n属性\nresult_schema\nobject\n（必选）\n表示需要模型抽取的字段，可以是任意形式的\nJSON\n结构，最多可\n嵌套\n3\n层\nJSON 对象。您\n只需要填写\nJSON\n对象的\nkey，value\n保持为空即可。\n示例值：\n\"result_schema\" : {\n\"收件人信息\" : {\n\"收件人姓名\" : \"\",\n\"收件人电话号码\" : \"\",\n\"收件人地址\":\"\"\n}\n}\nJava SDK\n为\nOcrOptions\n，DashScope Python SDK 最低版本为\n1.22.2， Java SDK 最低版本为\n2.18.4。\n通过\nHTTP\n调用时，请将\nocr_options\n放入\nparameters\n对象中。\nmax_input_tokens\ninteger\n（可选）\n允许输入的最大 Token 长度。目前仅支持\nqwen-plus-0728/latest\n模型。\nqwen-plus-latest 默认值：129,024\n后续默认值可能调整至\n1,000,000。\nqwen-plus-2025-07-28 默认值：1,000,000\nJava SDK 暂不支持该参数。\nmax_tokens\ninteger\n（可选）\n本次请求返回的最大 Token 数。\nmax_tokens\n的设置不会影响大模型的生成过程，如果模型生成的 Token 数超过\nmax_tokens\n，本次请求会返回截断后的内容。\n默认值和最大值都是模型的最大输出长度。关于各模型的最大输出长度，请参见\n模型列表\n。\nmax_tokens\n参数适用于需要限制字数（如生成摘要、关键词）、控制成本或减少响应时间的场景。\nqwen-vl-ocr、qwen-vl-ocr-latest\n、\nqwen-vl-ocr-2025-04-13、qwen-vl-ocr-2025-08-28\n模型的\nmax_tokens\n参数（最大输出长度）默认为 4096，如需提高该参数值（4097~8192\n范围），请发送邮件至\nmodelstudio@service.aliyun.com\n进行申请，并提供以下信息：主账号\nID、图像类型（如文档图、电商图、合同等）、模型名称、预计 QPS 和每日请求总数，以及模型输出长度超过\n4096\n的请求占比。\n对于 QwQ、QVQ 与开启思考模式的 Qwen3 模型，\nmax_tokens\n会限制回复内容的长度，不限制深度思考内容的长度。\nJava SDK\n中为\nmaxTokens\n（模型为通义千问\nVL/OCR\n/Audio/ASR\n时，Java SDK\n中为\nmaxLength，\n在 2.18.4 版本之后支持也设置为 maxTokens）\n。\n通过\nHTTP\n调用时，请将\nmax_tokens\n放入\nparameters\n对象中。\nseed\ninteger\n（可选）\n设置\nseed\n参数会使文本生成过程更具有确定性，通常用于使模型每次运行的结果一致。\n在每次模型调用时传入相同的\nseed\n值（由您指定），并保持其他参数不变，模型将尽可能返回相同的结果。\n取值范围：0\n到\n2\n31\n−1。\nseed\n默认值\nqwen-vl-plus-2025-01-02、qwen-vl-max、qwen-vl-max-latest、qwen-vl-max-2025-04-08、qwen-vl-max-2025-04-02、qwen-vl-max-2024-12-30、qvq-72b-preview、qvq-max\n系列：3407；\nqwen-vl-max-2025-01-25、qwen-vl-max-2024-11-19、qwen-vl-max-2024-10-30、qwen-vl-max-2024-08-09、qwen-vl-max-2024-02-01、qwen2-vl-72b-instruct、qwen2-vl-2b-instruct、qwen-vl-plus、qwen-vl-plus-latest、qwen-vl-plus-2025-05-07、qwen-vl-plus-2025-01-25、qwen-vl-plus-2024-08-09、qwen-vl-plus-2023-12-01：无默认值；\n其余模型均为\n1234。\n通过\nHTTP\n调用时，请将\nseed\n放入\nparameters\n对象中。\nstream\nboolean\n（可选）\n是否流式输出回复。参数值：\nfalse（默认值）：模型生成完所有内容后一次性返回结果。\ntrue：边生成边输出，即每生成一部分内容就立即输出一个片段（chunk）。\n该参数仅支持\nPython SDK。通过\nJava SDK\n实现流式输出请通过\nstreamCall\n接口调用；通过\nHTTP\n实现流式输出请在\nHeader\n中指定\nX-DashScope-SSE\n为\nenable\n。\nQwen3\n商业版（思考模式）、Qwen3\n开源版、QwQ、QVQ\n只支持流式输出。\nincremental_output\nboolean\n（可选）默认为\nfalse\n（Qwen3-Max、Qwen3-VL、\nQwen3 开源版\n、\nQwQ\n、\nQVQ\n模型默认值为\ntrue\n）\n在流式输出模式下是否开启增量输出。推荐您优先设置为\ntrue\n。\n参数值：\nfalse：每次输出为当前已经生成的整个序列，最后一次输出为生成的完整结果。\nI\nI like\nI like apple\nI like apple.\ntrue：增量输出，即后续输出内容不包含已输出的内容。您需要实时地逐个读取这些片段以获得完整的结果。\nI\nlike\napple\n.\nJava SDK\n中为\nincrementalOutput\n。\n通过\nHTTP\n调用时，请将\nincremental_output\n放入\nparameters\n对象中。\nQwQ 模型与思考模式下的 Qwen3 模型只支持设置为\ntrue\n。由于 Qwen3 商业版模型默认值为\nfalse\n，您需要在思考模式下手动设置为\ntrue\n。\nQwen3 开源版模型不支持设置为\nfalse\n。\nresponse_format\nobject\n（可选） 默认值为\n{\"type\": \"text\"}\n返回内容的格式。可选值：\n{\"type\": \"text\"}\n或\n{\"type\": \"json_object\"}\n。设置为\n{\"type\": \"json_object\"}\n时会输出标准格式的\nJSON\n字符串。使用方法请参见：\n结构化输出\n。\n如果指定该参数为\n{\"type\": \"json_object\"}\n，您需要在 System Message 或 User Message 中指引模型输出 JSON 格式，如：“请按照\njson\n格式输出。”\nJava SDK 中为\nresponseFormat\n。通过\nHTTP\n调用时，请将\nresponse_format\n放入\nparameters\n对象中。\n支持的模型\n通义千问\nMax 系列：\nqwen3-max、qwen3-max-2025-09-23、qwen3-amx-preview、qwen-max、qwen-max-latest、qwen-max-2024-09-19 及之后的快照模型\n通义千问\nPlus 系列（非思考模式）\n：qwen-plus、qwen-plus-latest、qwen-plus-2024-09-19\n及之后的快照模型\n通义千问\nFlash 系列（非思考模式）：\nqwen-flash、qwen-flash-2025-07-28\n及之后的快照模型\n通义千问\nCoder 系列\n：qwen3-coder-plus、qwen3-coder-plus-2025-07-22、qwen3-coder-flash、qwen3-coder-flash-2025-07-28\n通义千问\nVL 系列：\nqwen-vl-max、qwen-vl-plus（不包括最新版与快照版模型）\n通义千问\nTurbo 系列（非思考模式）\n：qwen-turbo、qwen-turbo-latest、qwen-turbo-2024-09-19\n及之后的快照模型\nQwen 开源系列\nQwen3（非思考模式）\nQwen3-Coder\nQwen2.5 系列的文本模型（不含\nmath\n与\ncoder\n模型）\nGLM 系列：\nglm-4.6、glm-4.5、glm-4.5-air\noutput_format\nstring\n（可选）默认值为\n\"model_detailed_report\"\n仅当调用通义千问深入研究模型 qwen-deep-research 时，用于规定输出内容格式。\n可选值：\n\"model_detailed_report\"：详细研究报告，约\n6000\n字\n\"model_summary_report\"：摘要研究报告，约\n1500-2000\n字\nresult_format\nstring\n（可选）\n默认为\n\"text\"\n（Qwen3-Max、Qwen3-VL、\nQwQ\n模型、Qwen3 开源模型\n与 Qwen-Long 模型\n默认值为\n\"message\"\n）\n返回数据的格式。推荐您优先设置为\n\"message\"\n，可以更方便地进行\n多轮对话\n。\n平台后续将统一将默认值调整为\"message\"。\nJava SDK\n中为\nresultFormat\n。\n通过\nHTTP\n调用时，请将\nresult_format\n放入\nparameters\n对象中。\n模型为通义千问\nVL/QVQ/OCR\n/Audio/ASR\n时，设置“\ntext\n”不生效。\nQwen3-Max、Qwen3-VL、思考模式下的 Qwen3 模型只能设置为\n\"message\"\n，由于 Qwen3 商业版模型默认值为\"text\"，您需要将其设置为\n\"message\"\n。\n如果您使用 Java SDK 调用\nQwen3 开源模型，并且传入了 \"text\"，依然会以 \"message\"格式进行返回。\nlogprobs\nboolean\n（可选）默认值为\nfalse\n是否返回输出 Token 的对数概率，可选值：\ntrue\n返回；\nfalse\n不返回。\n思考阶段生成的内容（\nreasoning_content\n）不会返回对数概率。\n支持 qwen-plus、qwen-turbo 系列的快照模型（不包含主线模型）与 Qwen3 开源模型。\ntop_logprobs\ninteger\n（可选）默认值为\n0\n指定在每一步生成时，返回模型最大概率的候选 Token 个数。\n取值范围：[0,5]\n仅当\nlogprobs\n为\ntrue\n时生效。\nn\ninteger\n（可选） 默认值为\n1\n生成响应的个数，取值范围是\n1-4\n。对于需要生成多个响应的场景（如创意写作、广告文案等），可以设置较大的 n 值。\n当前仅支持 qwen-plus、\nQwen3（非思考模式）\n、qwen-plus-character\n模型，且在传入 tools 参数时固定为\n1。\n设置较大的 n 值不会增加输入 Token 消耗，会增加输出 Token 的消耗。\nstop\nstring 或 array\n（可选）\n使用\nstop\n参数后，当模型生成的文本即将包含指定的字符串或\ntoken_id\n时，将自动停止生成。\n您可以在\nstop\n参数中传入敏感词来控制模型的输出。\nstop\n为\narray\n类型时，不可以将\ntoken_id\n和字符串同时作为元素输入，比如不可以指定\nstop\n为\n[\"你好\",104307]\n。\ntools\narray\n（可选）\n可供模型调用的工具数组，可以包含一个或多个工具对象。一次 Function Calling 流程模型会从中选择其中一个工具（开启\nparallel_tool_calls\n参数可能选择多个工具）。使用 tools 时需要同时指定\nresult_format\n参数为\n\"message\"\n。无论是发起 Function Calling，还是向模型提交工具函数的执行结果，均需设置\ntools\n参数。\n目前不支持通义千问\nVL\n/Audio，也不建议用于数学和代码模型（Qwen3-Coder 模型除外）\n。\n属性\ntype\nstring\n（必选）\ntools\n的类型，当前仅支持\nfunction。\nfunction\nobject\n（必选）\n属性\nname\nstring\n（必选）\n工具函数的名称，必须是字母、数字，可以包含下划线和短划线，最大长度为\n64。\ndescription\nstring\n（必选）\n工具函数的描述，供模型选择何时以及如何调用工具函数。\nparameters\nobjcet\n（必选）\n工具的参数描述，需要是一个合法的\nJSON Schema。JSON Schema\n的描述可以见\n链接\n。如果\nparameters\n参数为空，表示\nfunction\n没有入参。\n通过\nHTTP\n调用时，请将\ntools\n放入\nparameters\nJSON 对象中。暂时不支持\nqwen-vl\n与\nqwen-audio\n系列模型。\ntool_choice\nstring 或 object\n（可选）\n在使用\ntools\n参数时，用于控制模型调用指定工具。有三种取值：\n\"none\"\n表示不调用工具。tools\n参数为空时，默认值为\n\"none\"\n。\n\"auto\"\n表示由模型判断是否调用工具，可能调用也可能不调用。tools\n参数不为空时，默认值为\n\"auto\"\n。\nobject\n结构可以指定模型调用的工具。例如\ntool_choice={\"type\": \"function\", \"function\": {\"name\": \"user_function\"}}\n。\n若模型输出思考过程，则不支持设置为该值。\ntype\n只支持指定为\n\"function\"\n。\nfunction\nname\n表示期望被调用的工具名称，例如\n\"get_current_time\"\n。\nJava SDK\n中为\ntoolChoice\n。\n通过\nHTTP\n调用时，请将\ntool_choice\n放入\nparameters\n对象中。\nparallel_tool_calls\nboolean\n（可选）默认值为\nfalse\n是否开启并行工具调用。参数为\ntrue\n时开启，为\nfalse\n时不开启。并行工具调用详情请参见：\n并行工具调用\n。\ntranslation_options\nobject\n（可选）\n当您使用\n翻译模型\n时需要配置的翻译参数。\n属性\nsource_lang\nstring\n（必选）\n源语言的英文全称，详情请参见\n支持的语言\n。您可以将\nsource_lang\n设置为\n\"auto\"\n，模型会自动判断输入文本属于哪种语言。\ntarget_lang\nstring\n（必选）\n目标语言的英文全称，详情请参见\n支持的语言\n。\nterms\narrays\n（可选）\n在使用\n术语干预\n功能时需要设置的术语数组。\n属性\nsource\nstring\n（必选）\n源语言的术语。\ntarget\nstring\n（必选）\n目标语言的术语。\ntm_list\narrays\n（可选）\n在\n翻译记忆\n功能时需要设置的翻译记忆数组。\n属性\nsource\nstring\n（必选）\n源语言的语句。\ntarget\nstring\n（必选）\n目标语言的语句。\ndomains\nstring\n（可选）\n在使用\n领域提示\n功能时需要设置的领域提示语句。\n领域提示语句暂时只支持英文。\nJava SDK\n中为\ntranslationOptions\n。通过\nHTTP\n调用时，请将\ntranslation_options\n放入\nparameters\n对象中。\nenable_search\nboolean\n（可选）\n模型在生成文本时是否使用互联网搜索结果进行参考。取值如下：\ntrue：启用互联网搜索，模型会将搜索结果作为文本生成过程中的参考信息，但模型会基于其内部逻辑判断是否使用互联网搜索结果。\n如果模型没有搜索互联网，建议优化\nPrompt，或设置\nsearch_options\n中的\nforced_search\n参数开启强制搜索。\nfalse（默认）：关闭互联网搜索。\n计费信息请参见\n计费说明\n。\n支持的模型\n通义千问\nMax：qwen3-max、qwen3-max-2025-09-23、qwen3-max-preview、qwen-max、qwen-max-latest、qwen-max-2024-09-19\n及之后的快照版本\n通义千问\nPlus：qwen-plus、qwen-plus-latest、qwen-plus-2025-07-14\n及之后的快照版本\n通义千问\nFlash：qwen-flash、qwen-flash-2025-07-28\n及之后的快照版本\n通义千问\nTurbo：qwen-turbo、qwen-turbo-latest、qwen-turbo-2025-07-15\nQwQ：qwq-plus（仅支持流式输出）\nKimi：Moonshot-Kimi-K2-Instruct\nJava SDK\n中为\nenableSearch\n。\n通过\nHTTP\n调用时，请将\nenable_search\n放入\nparameters\n对象中。\n启用互联网搜索功能可能会增加 Token 的消耗。\nsearch_options\nobject\n（可选）\n联网搜索的策略。仅当\nenable_search\n为\ntrue\n时生效。详情参见\n联网搜索\n。\n通过\nHTTP\n调用时，请将\nsearch_options\n放入\nparameters\n对象中。Java SDK\n中为\nsearchOptions。\n属性\nenable_source\nboolean\n（可选）默认值为\nfalse\n在返回结果中是否展示搜索到的信息。参数值：\ntrue：展示；\nfalse：不展示。\nenable_citation\nboolean\n（可选）默认值为\nfalse\n是否开启[1]或[ref_1]样式的角标标注功能。在\nenable_source\n为\ntrue\n时生效。参数值：\ntrue：开启；\nfalse：不开启。\ncitation_format\nstring\n（可选）默认值为\n\"[<number>]\"\n角标样式。在\nenable_citation\n为\ntrue\n时生效。参数值：\n[<number>]：角标形式为\n[1]\n；\n[ref_<number>]：角标形式为\n[ref_1]\n。\nforced_search\nboolean\n（可选）默认值为\nfalse\n是否强制开启搜索。参数值：\ntrue：强制开启；\nfalse：不强制开启。\nsearch_strategy\nstring\n（可选）默认值为\nturbo\n搜索互联网信息的策略。参数值：\nturbo\n：默认策略，兼顾响应速度与搜索效果，推荐使用。\nmax\n：高性能模式，基于全栈顶配模型与多源搜索引擎，提供最优效果。\nenable_search_extension\nboolean\n（可选）默认值为\nfalse\n是否开启特定领域增强。参数值：\ntrue\n开启。\nfalse\n（默认值）\n不开启。\nprepend_search_result\nboolean\n（可选）默认值为\nfalse\n在流式输出且\nenable_source\n为\ntrue\n时，可通过\nprepend_search_result\n配置\n第一个返回的数据包\n是否只包含搜索来源信息。可选值：\ntrue\n只包含搜索来源信息。\nfalse\n（默认值）\n包含搜索来源信息与大模型回复信息。\n暂不支持 DashScope Java SDK。\nasr_options\nobject\n（可选）\n该参数仅在调用\n录音文件识别-通义千问\n功能时可用，且仅对通义千问\n3 ASR\n模型生效，用来指定某些功能是否启用。具体用法请参见\n快速开始\n。\n通过\nHTTP\n或\nJava SDK\n调用时，请将\nasr_options\n放入\nparameters\n对象中。\n属性\nlanguage\nstring\n（可选）无默认值\n若已知音频的语种，可通过该参数指定待识别语种，以提升识别准确率。\n只能指定一个语种。\n若音频语种不确定，或包含多种语种（例如中英日韩混合），请勿指定该参数。\n参数值：\nzh：中文\nen：英文\nja：日语\nde：德语\nko：韩语\nru：俄语\nfr：法语\npt：葡萄牙语\nar：阿拉伯语\nit：意大利语\nes：西班牙语\nenable_itn\nboolean\n（可选）默认值为\nfalse\n是否启用\nITN。该功能仅适用于中文和英文音频。\nITN 是 Inverse Text Normalization 的缩写，中文一般翻译为 “逆文本规范化” 或 “反标准化”。它是语音识别后处理阶段中的一环，用于将识别结果从“读出来的字词”转换成更规范、更符合书写习惯的格式。\n参数值：\ntrue：开启；\nfalse：不开启。\nX-DashScope-DataInspection\nstring\n（可选）\n在通义千问 API 的内容安全能力基础上，是否进一步识别输入输出内容的违规信息。取值如下：\n'{\"input\":\"cip\",\"output\":\"cip\"}'\n：进一步识别；\n不设置该参数：不进一步识别。\n通过 HTTP 调用时请放入请求头：\n-H \"X-DashScope-DataInspection: {\\\"input\\\": \\\"cip\\\", \\\"output\\\": \\\"cip\\\"}\"\n；\n通过 Python SDK 调用时请通过\nheaders\n配置：\nheaders={'X-DashScope-DataInspection': '{\"input\":\"cip\",\"output\":\"cip\"}'}\n。\n详细使用方法请参见\n内容审核\n。\n不支持通过 Java SDK 设置。\n不适用于 Qwen-VL、Qwen-Audio 系列模型。\nchat\n响应对象（流式与非流式输出格式一致）\n{\n\"status_code\": 200,\n\"request_id\": \"902fee3b-f7f0-9a8c-96a1-6b4ea25af114\",\n\"code\": \"\",\n\"message\": \"\",\n\"output\": {\n\"text\": null,\n\"finish_reason\": null,\n\"choices\": [\n{\n\"finish_reason\": \"stop\",\n\"message\": {\n\"role\": \"assistant\",\n\"content\": \"我是阿里云开发的一款超大规模语言模型，我叫通义千问。\"\n}\n}\n]\n},\n\"usage\": {\n\"input_tokens\": 22,\n\"output_tokens\": 17,\n\"total_tokens\": 39\n}\n}\nstatus_code\nstring\n本次请求的状态码。200 表示请求成功，否则表示请求失败。\nJava SDK\n不会返回该参数。调用失败会抛出异常，异常信息为\nstatus_code\n和\nmessage\n的内容。\nrequest_id\nstring\n本次调用的唯一标识符。\nJava SDK\n返回参数为\nrequestId。\ncode\nstring\n错误码，调用成功时为空值。\n只有\nPython SDK\n返回该参数。\noutput\nobject\n调用结果信息。\n属性\ntext\nstring\n模型生成的回复。当设置输入参数\nresult_format\n为\ntext\n时将回复内容返回到该字段。\nfinish_reason\nstring\n当设置输入参数\nresult_format\n为\ntext\n时该参数不为空。\n有四种情况：\n正在生成时为\nnull；\n因模型输出自然结束，或触发输入参数中的\nstop\n条件而结束时为\nstop；\n因生成长度过长而结束为\nlength；\n因发生工具调用为\ntool_calls。\nchoices\narray\n模型的输出信息。当\nresult_format\n为\nmessage\n时返回\nchoices\n参数。\n属性\nfinish_reason\nstring\n有四种情况：\n正在生成时为\nnull；\n因模型输出自然结束，或触发输入参数中的\nstop\n条件而结束时为\nstop；\n因生成长度过长而结束为\nlength；\n因发生工具调用为\ntool_calls。\nmessage\nobject\n模型输出的消息对象。\n属性\nphase\nstring\n仅当调用通义千问深入研究模型\nqwen-deep-research\n时，表示研究任务所处的阶段\n\"ResearchPlanning\"：研究规划阶段，执行计划内容存放在\ncontent\n字段。\n\"WebResearch\"：网络搜索阶段，搜索内容存放在 extra.deep_research.research 字段。\n\"KeepAlive\"：维持流式连接，表明程序正在运行，不包含有用内容。\n\"answer\"：回答阶段，研究内容存放在 content 字段。\nrole\nstring\n输出消息的角色，固定为\nassistant。\ncontent\nstring\n或\narray\n输出消息的内容。当使用\nqwen-vl\n或\nqwen-audio\n系列模型时为\narray\n，其余情况为\nstring\n。\n如果发起\nFunction Calling，则该值为空。\n属性\ntext\nstring\n当使用\nqwen-vl\n或\nqwen-audio\n系列模型时，输出消息的内容。\n当使用\n录音文件识别-通义千问\n时，输出语音识别结果。\nimage_hw\narray\n当\nQwen-VL\n系列模型启用 vl_enable_image_hw_output 参数时，有两种情况：\n图像输入：返回图像的高度和高度（数值单位：像素）\n视频输入：返回空数组\nocr_result\narray\n当\nQwen-OCR\n系列模型调用内置的信息抽取\n、高精识别\n任务时，输出的任务结果信息。\n属性\nkv_result\narray\n信息抽取任务的输出结果。\nwords_info\narray\n高精识别任务的输出结果。\n属性\nrotate_rect\narray\n示例值：\n[center_x, center_y, width, height, angle]\n文字框的的旋转矩形表示：\ncenter_x、center_y\n为文本框中心点坐标\nwidth\n为文本框宽度，\nhight\n为高度\nangle\n为文本框相对于水平方向的旋转角度，取值范围为\n[-90, 90]\nlocation\narray\n示例值：\n[x1, y1, x2, y2, x3, y3, x4, y4]\n文字框四个顶点的坐标，坐标顺序为左上角开起，按左上角→右上角→右下角→左下角的顺时针顺序排列。\ntext\nstring\n文本行的内容\nannotations\narray\n当使用\n录音文件识别-通义千问\n时，输出标注信息（如语种）\n属性\nlanguage\nstring\n被识别音频的语种。当请求参数\nlanguage\n已指定语种时，该值与所指定的参数一致。\n可能的值如下：\nzh：中文\nen：英文\nja：日语\nde：德语\nko：韩语\nru：俄语\nfr：法语\npt：葡萄牙语\nar：阿拉伯语\nit：意大利语\nes：西班牙语\ntype\nstring\n固定为\naudio_info\n，表示音频信息。\nemotion\nstring\n被识别音频的情感。支持的情感如下：\nsurprised\n：惊讶\nneutral\n：平静\nhappy\n：愉快\nsad\n：悲伤\ndisgusted\n：厌恶\nangry\n：愤怒\nfearful\n：恐惧\nextra\ndict\n仅当调用通义千问深入研究模型\nqwen-deep-research\n时，表示研究任务的附加信息。\n属性\ndeep_research\narray\n深入研究相关信息\n属性\nresearch\nobject\n研究任务信息\n属性\nid\nint\n研究任务\nID\nwebSites\narray\n搜索到的网站信息（仅在 status: \"streamingWebResult\" 时存在）\n属性\ntitle\nstring\n网站标题\ndescription\nstring\n网站描述\nurl\nstring\n网站\nurl\nfavicon\nstring\n网站图标\nlearningMap\nobject\n模型从调用工具总结获取到的内容\nreference\nobject\n参考文献信息（answer\n阶段，最终报告生成时）\n属性\nicon\nstring\n网站图标\nindex_number\ninteger\n引用编号\ndescription\nstring\n文献描述\ntitle\nstring\n文献标题\nurl\nstring\n文献链接\nstatus\nstring\n仅当调用通义千问深入研究模型 qwen-deep-research 时，表示当前阶段的任务执行状态。\n\"typing\"：模型正在工作，内容正在生成中\n\"finished\"：当前阶段完成\n\"streamingQueries\"：正在生成研究目标和搜索查询（WebResearch\n阶段）\n\"streamingWebResult\"：正在执行搜索、网页阅读和代码执行（WebResearch\n阶段）\n\"WebResultFinished\"：网络搜索阶段完成（WebResearch\n阶段）\nreasoning_content\nstring\nQwen3、\nQwQ 模型\n、\nQVQ\n模型\n的深度思考内容。\ntool_calls\narray\n如果模型需要调用工具，则会生成\ntool_calls\n参数。\n属性\nfunction\nobject\n调用工具的名称，以及输入参数。\n属性\nname\nstring\n调用工具的名称\narguments\nstring\n需要输入到工具中的参数，为\nJSON\n字符串。\n由于大模型响应有一定随机性，输出的\nJSON\n字符串并不总满足于您的函数，建议您在将参数输入函数前进行参数的有效性校验。\nindex\ninteger\n当前\ntool_calls\n对象在\ntool_calls\n数组中的索引。\nid\nstring\n本次工具响应的\nID。\ntype\nstring\n工具类型，固定为\nfunction\n。\nlogprobs\nobject\n当前 choices 对象的概率信息。\n属性\ncontent\narray\n带有对数概率信息的 Token 数组。\n属性\ntoken\nstring\n当前 Token。\nbytes\narray\n当前 Token 的 UTF‑8 原始字节列表，用于精确还原输出内容，在处理表情符号、中文字符时有帮助。\nlogprob\nfloat\n当前 Token 的对数概率。返回值为 null 表示概率值极低。\ntop_logprobs\narray\n当前 Token 位置最可能的若干个 Token 及其对数概率，元素个数与入参的\ntop_logprobs\n保持一致。\n属性\ntoken\nstring\n当前 Token。\nbytes\narray\n当前 Token 的 UTF‑8 原始字节列表，用于精确还原输出内容，在处理表情符号、中文字符时有帮助。\nlogprob\nfloat\n当前 Token 的对数概率。返回值为 null 表示概率值极低。\nsearch_info\nobject\n联网搜索到的信息，在设置\nsearch_options\n参数后会返回该参数。\n属性\nsearch_results\narray\n联网搜索到的结果。\n属性\nsite_name\nstring\n搜索结果来源的网站名称。\nicon\nstring\n来源网站的图标\nURL，如果没有图标则为空字符串。\nindex\ninteger\n搜索结果的序号，表示该搜索结果在\nsearch_results\n中的索引。\ntitle\nstring\n搜索结果的标题。\nurl\nstring\n搜索结果的链接地址。\nextra_tool_info\narray\n开启\nenable_search_extension\n参数后返回的领域增强信息。\n属性\nresult\nstring\n领域增强工具输出信息。\ntool\nstring\n领域增强使用的工具。\nusage\nmap\n本次\nchat\n请求使用的\nToken\n信息。\n属性\ninput_tokens\ninteger\n用户输入内容转换成\nToken\n后的长度。\noutput_tokens\ninteger\n模型输出内容转换成\nToken\n后的长度。\ninput_tokens_details\ninteger\n使用\n录音文件识别-通义千问\n、\nQwen-VL 模型\n或\nQVQ\n模型\n时，输入内容转换成\nToken\n后的长度详情。\n属性\ntext_tokens\ninteger\n使用\nQwen-VL 模型\n或\nQVQ\n模型\n时，为输入的文本转换为\nToken\n后的长度。\n使用\n录音文件识别-通义千问\n时，为使用上下文增强功能时输入的文本长度，上限为\n10000 Token。\nimage_tokens\ninteger\n输入的图像转换为\nToken\n后的长度。\nvideo_tokens\ninteger\n输入的视频文件或图像列表转换为\nToken\n后的长度。\ntotal_tokens\ninteger\n当输入为纯文本时返回该字段，为\ninput_tokens\n与\noutput_tokens\n之和\n。\nimage_tokens\ninteger\n输入内容包含\nimage\n时返回该字段。为用户输入图片内容转换成\nToken\n后的长度。\nvideo_tokens\ninteger\n输入内容包含\nvideo\n时返回该字段。为用户输入视频内容转换成\nToken\n后的长度。\naudio_tokens\ninteger\n输入内容包含\naudio\n时返回该字段。为用户输入音频内容转换成\nToken\n后的长度。\noutput_tokens_details\ninteger\n输出内容转换成 Token\n后的长度详情。\n属性\ntext_tokens\ninteger\n输出的文本转换为\nToken\n后的长度。\nreasoning_tokens\ninteger\nQwen3 模型思考过程转换为\nToken\n后的长度。\nprompt_tokens_details\nobject\n输入 Token 的细粒度分类。\n属性\ncached_tokens\ninteger\n命中 Cache 的 Token 数。Context Cache 详情请参见\n上下文缓存\n。\ncache_creation\nobject\n显式缓存\n创建信息。\n属性\nephemeral_5m_input_tokens\ninteger\n用于创建\n5\n分钟有效期显式缓存的 Token 长度。\ncache_creation_input_tokens\ninteger\n用于创建显式缓存的 Token 长度。\ncache_type\nstring\n使用\n显式缓存\n时，参数值为\nephemeral\n，否则该参数不存在。\nseconds\ninteger\n使用\n录音文件识别-通义千问\n时，为音频时长（单位为秒）。\n错误码\n如果模型调用失败并返回报错信息，请参见\n错误信息\n进行解决。\n上一篇：对话\n下一篇：DeepSeek\n该文章对您有帮助吗？\n反馈\n为什么选择阿里云\n什么是云计算\n全球基础设施\n技术领先\n稳定可靠\n安全合规\n分析师报告\n产品和定价\n全部产品\n免费试用\n产品动态\n产品定价\n配置报价器\n云上成本管理\n解决方案\n技术解决方案\n文档与社区\n文档\n开发者社区\n天池大赛\n培训与认证\n权益中心\n免费试用\n解决方案免费试用\n高校计划\n5亿算力补贴\n推荐返现计划\n支持与服务\n基础服务\n企业增值服务\n迁云服务\n官网公告\n健康看板\n信任中心\n关注阿里云\n关注阿里云公众号或下载阿里云APP，关注云资讯，随时随地运维管控云服务\n联系我们：4008013260\n法律声明\nCookies政策\n廉正举报\n安全举报\n联系我们\n加入我们\n友情链接\n阿里巴巴集团\n淘宝网\n天猫\n全球速卖通\n阿里巴巴国际交易市场\n1688\n阿里妈妈\n飞猪\n阿里云计算\nAliOS\n万网\n高德\nUC\n友盟\n优酷\n钉钉\n支付宝\n达摩院\n淘宝海外\n阿里云盘\n饿了么\n© 2009-2025 Aliyun.com 版权所有 增值电信业务经营许可证：\n浙B2-20080101\n域名注册服务机构许可：\n浙D3-20210002\n浙公网安备 33010602009975号\n浙B2-20080101-4",
    "fetched_at": "2025-10-24T03:07:30.199921Z"
  },
  "https://huggingface.co/docs": {
    "provider": "HuggingFace",
    "title": "HuggingFace API Documentation",
    "hash": "67347856879b0dc58da61241e20211c22a886a3758f51190ecd0e08db4634942",
    "content": "Hugging Face - Documentation\nHugging Face\nModels\nDatasets\nSpaces\nCommunity\nDocs\nEnterprise\nPricing\nLog In\nSign Up\nDocumentation\nHub & Client Libraries\nHub\nHost Git-based models, datasets, and Spaces on the HF Hub\nHub Python Library\nPython client to interact with the Hugging Face Hub\nHuggingface.js\nJavaScript libraries for Hugging Face with built-in TS types\nTasks\nExplore demos, models, and datasets for any ML tasks\nDataset viewer\nAPI for metadata, stats, and content of HF Hub datasets\nDeployment & Inference\nInference Providers\nCall 200k+ models hosted by our 10+ Inference partners\nInference Endpoints (dedicated)\nDeploy models on dedicated & fully managed infrastructure on HF\nDeploying on AWS\nTrain/deploy models from Hugging Face to AWS with DLCs\nText Generation Inference\nServe language models with TGI optimized toolkit\nText Embeddings Inference\nServe embeddings models with TEI optimized toolkit\nMicrosoft Azure\nDeploy Hugging Face models on Microsoft Azure\nCore ML Libraries\nTransformers\nState-of-the-art AI models for PyTorch\nDiffusers\nState-of-the-art Diffusion models in PyTorch\nDatasets\nAccess & share datasets for any ML tasks\nTransformers.js\nState-of-the-art ML running directly in your browser\nTokenizers\nFast tokenizers optimized for research & production\nEvaluate\nEvaluate and compare models performance\ntimm\nState-of-the-art vision models: layers, optimizers, and utilities\nSentence Transformers\nEmbeddings, Retrieval, and Reranking\nKernels\nLoad and run compute kernels from the Hugging Face Hub\nTraining & Optimization\nPEFT\nParameter-efficient finetuning for large language models\nAccelerate\nTrain PyTorch models with multi-GPU, TPU, mixed precision\nOptimum\nOptimize HF Transformers for faster training/inference\nAWS Trainium & Inferentia\nTrain/deploy Transformers/Diffusers on AWS\nTRL\nTrain transformers LMs with reinforcement learning\nSafetensors\nSafe way to store/distribute neural network weights\nBitsandbytes\nOptimize and quantize models with bitsandbytes\nLighteval\nAll-in-one toolkit to evaluate LLMs across multiple backends\nCollaboration & Extras\nGradio\nBuild ML demos and web apps with a few lines of Python\nTrackio\nA lightweight, local-first, and free experiment tracking Python library\nsmolagents\nSmol library to build great agents in Python\nLeRobot\nMaking AI for Robotics more accessible with end-to-end learning\nAutoTrain\nAutoTrain API and UI for seamless model training\nChat UI\nOpen source chat frontend powering HuggingChat\nLeaderboards\nCreate custom Leaderboards on Hugging Face\nArgilla\nCollaboration tool for building high-quality datasets\nDistilabel\nFramework for synthetic data generation and AI feedback\nCommunity\nBlog\nLearn\nDiscord\nForum\nGitHub\nSystem theme\nCompany\nTOS\nPrivacy\nAbout\nJobs\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs",
    "fetched_at": "2025-10-23T03:10:58.117945Z"
  },
  "https://docs.together.ai/docs": {
    "provider": "TogetherAI",
    "title": "TogetherAI API Documentation",
    "hash": "fab78792a95e95dc269c8d74798c72a1b822980a8933aacec5d9957316bbd52f",
    "content": "Overview - Together.ai Docs\nSkip to main content\nTogether.ai Docs\nhome page\nDocumentation\nAPI Reference\nDemos\nChangelog\nGuides\nFAQ\nSearch...\nNavigation\nGetting Started\nOverview\nSearch...\n⌘\nK\nGetting Started\nOverview\nQuickstart\nOpenAI Compatibility\nInference\nModels\nDedicated Inference\nModel Quickstarts\nTraining\nFine-tuning\nInstant Clusters\nReserved Clusters\nCapabilities\nChat\nStructured Outputs\nFunction Calling\nImages\nVideos\nVision\nVoice\nOther Modalities\nIntegrations\nOther APIs\nBatch\nEvaluations\nCode Execution\nIntegrations\nGetting Started\nOverview\nCopy page\nWelcome to Together AI’s docs! Together makes it easy to run, finetune, and train open source AI models with transparency and privacy.\nCopy page\nPython\nTypeScript\ncURL\nCopy\nAsk AI\nfrom\ntogether\nimport\nTogether\nclient\n=\nTogether()\ncompletion\n=\nclient.chat.completions.create(\nmodel\n=\n\"openai/gpt-oss-20b\"\n,\nmessages\n=\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What are the top 3 things to do in New York?\"\n}],\n)\nprint\n(completion.choices[\n0\n].message.content)\nRun AI models\nRun leading open source AI models (across chat, image, vision, ect...) with our OpenAI-compatible API.\nFine-tune models\nFinetune models on your own data (or bring your own model) and run inference for them on Together\nLaunch a GPU cluster\nInstantly spin up H100 and B200 clusters with attached storage for training or large batch jobs.\nOur models:\nTogether hosts many popular models via our serverless endpoints and dedicated endpoints. On serverless, you’ll be charged based on the tokens you use and size of the model. On dedicated, you’ll be charged based on GPU hours.\nChat models:\nView all models\nDeepSeek R1\nDeepSeek V3.1\nGPT-OSS-120B\nLlama 4 Maverick\nQwen 3 Next 80B\nKimi K2 0905\nImage models:\nView all models\nFLUX.1 [schnell]\nFLUX 1.1 [pro]\nVision models:\nView all models\nLlama 4 Scout\nQwen2.5 VL 72B\nAudio models:\nView all models\nCartesia Sonic 2\nWhisper Large v3\nEmbedding models:\nRerank models:\nBuild AI apps and agents with Together:\nBuild an agent\nBuild agent workflows to solve real use cases with Together\nBuild a Next.js chatbot\nSpin up a production-ready chatbot using Together + Next.js.\nBuild RAG apps\nCombine retrieval and generation to build grounded RAG apps.\nBuild a real-time image app\nStream real-time image generations with Flux Schnell on Together.\nBuild a text → app workflow\nTurn natural language into interactive apps with Together + CodeSandbox.\nBuild an AI search engine\nShip a simplified Perplexity-style search using Together models.\nUse structured inputs with LLM’s\nGet reliable JSON by defining schemas and using structured outputs.\nWorking with reasoning models\nUse open reasoning models (e.g., DeepSeek-R1) for logic-heavy, multi-step tasks.\nExplore our services:\nSpin up a batch job\nQueue async generations and fetch results later.\nRun a dedicated instance\nProvision single-tenant GPUs for predictable, isolated latency.\nUse our evals API\nAutomate scoring with LLM judges and reports.\nDo code execution with together code sandbox\nRun Python safely alongside model calls.\nBring your own model\nUpload weights and serve them via our API.\nCookbook\nOpen-source collection of examples and guides.\nExample apps\nFull-stack open source Next.js apps built on Together.\nPlayground\nExperiment with models and export code.\nModels Library\nBrowse supported models\nWas this page helpful?\nYes\nNo\nQuickstart\nNext\n⌘\nI\nTogether.ai Docs\nhome page\nCompany\nAbout us\nCareers\nResources\nCookbooks\nExample apps\nLegal\nCookie Policy\nConsent Preferences\nPowered by Mintlify",
    "fetched_at": "2025-10-24T03:07:30.199921Z"
  },
  "https://cloud.baidu.com/doc/WENXINWORKSHOP/s/1lilb2u4t": {
    "provider": "Baidu Wenxin",
    "title": "Baidu Wenxin API Documentation",
    "hash": "1f0a00d6d1605c843a988a0d82724e619039dfe764a3fe0229128613b63f70b4",
    "content": "千帆ModelBuilder文档首页-百度智能云\n最新活动\n产品\n解决方案\n千帆社区\nAI原生应用商店\n企业服务\n云市场\n合作与生态\n开发者\n服务与支持\n了解智能云\n备案\n文档\n管理控制台\n文档中心\n推荐文档\n云服务器\n千帆大模型平台\n文字识别\n人脸识别\n实名认证介绍\n备案介绍\n财务功能概述\n搜索本产品文档关键词\n所有文档\n人工智能\n百度千帆·大模型平台\n百度千帆·Agent开发平台\n秒哒\n智能代码助手\n文字识别\n语音技术\n人脸识别\n人体分析\n度目软硬一体\n语言处理技术\n图像识别\n图像搜索\n图像增强\nAR开放平台\nBML全功能AI开发平台\n零门槛AI开发平台 EasyDL\n知识理解\n机器人平台 ABC Robot\n内容审核\n机器翻译\n居民服务Agent\n曦灵数字人\n计算\n云服务器 BCC\n专属服务器 DCC\n弹性裸金属服务器\nGPU云服务器\nFPGA云服务器\n云手机\n弹性伸缩 AS\n云编排服务 COS\n应用引擎 BAE\n轻量应用服务器 LS\n本地计算集群 LCC\n百度百舸·AI计算平台\n云高性能计算平台\n磐玉蜂巢服务器\n网络\n弹性公网IP\n私有网络 VPC\n负载均衡 BLB\n智能云解析 DNS\n专线接入 ET\n云智能网 CSN\n智能流量管理\n云防火墙 CFW\n智能网络接入服务\n网络诊断服务\n存储\n对象存储 BOS\n云磁盘 CDS\n文件存储 CFS\n并行文件存储 PFS\n存储网关 BSG\n数据流转平台 CloudFlow\n数据湖存储加速工具 RapidFS\n云备份 BCB\nCDN与边缘服务\n内容分发网络 CDN\n海外CDN\n动态加速 DRCDN\n边缘计算节点 BEC\n边缘服务器 ECS\n数据库\n云数据库 RDS\n云数据库 Redis\n云数据库 DocDB for MongoDB\n云原生数据库 GaiaDB\n云数据库 GaiaDB-X\n数据传输服务 DTS\n数据库智能驾驶舱  DBSC\n云数据库 TableStorage\n消息服务 for RabbitMQ\n向量数据库 VectorDB\n云原生\n容器引擎服务 CCE\n容器镜像服务CCR\nPrometheus监控服务\n容器实例BCI\n函数计算 CFC\n服务网格 CSM\n微服务引擎 MSE\n管理运维\n云监控BCM\n运维编排服务\n效率云\nAPI网关\n标签管理\n资源管理\n配额中心\n云迁移中心\n云顾问ACA\n服务与支持\n智能视频\n音视频处理\n智能视频 SDK\n媒体内容分析\n媒体内容审核\n音视频直播\n智能直播应用平台 ILAP\n实时音视频 RTC\n智能点播平台 VOD\n多模态资源检索\n智能视联网平台\n智能大数据\nMapReduce\n流式计算 BSC\n数据可视化 Sugar BI\nElasticsearch\n数据仓库 PALO\n日志服务 BLS\n消息服务 for Kafka\n消息服务 for RocketMQ\n千帆数据智能平台 DataBuilder\n城市时空大数据分析 DMI\n安全\nDDoS防护服务\n应用防火墙WAF\n云安全中心\n安全检测服务\n流量审计分析\n云审计\n业务安全风控\n号码安全服务SPNS\n密钥管理服务\nSSL证书\n身份管理服务\n应急响应服务\n渗透测试服务\n云堡垒机\n安全漏洞预警\n操作系统安全加固\n应用加固与安全检测\n短网址服务\n史宾格安全及隐私合规平台\nOTA远程升级服务\n大模型内容安全平台\n云通信\n短信服务 SMS\n号码隐私保护服务 PNS\n号码认证服务\n语音通话\n语言通知服务\n5G消息服务\n域名与网站\n智能门户\n域名服务\n备案服务\n移动域名解析\n商标服务\n云主机管家\n数字营销云\n智能对话营销服务\n企业智能应用\n智能对话平台UNIT私有化\n智能对话平台UNIT云端版\n智能外呼\n智能推荐引擎\n文档服务\n爱速搭低代码平台\n工商财税服务 CBS\n问卷调研服务\n移动APP测试服务\n物联网服务\n物联网核心套件\n物联网数据可视化\n时序时空数据库 TSDB\n智能边缘\n物联网安全套件\n天工智能物联网解决方案\n度家-AIOT语音平台\n区块链\n超级链BaaS平台\n超级链数字商品可信登记平台\n超级链可信存证\n混合云\n广目混合云管理平台\n云市场\n云市场\n解决方案\n金融云\n工业视觉智能平台\n云游戏\n金融智能体\n账户管理\n账号管理\n多用户访问控制\n企业组织\n证书管理\n实名认证\n通用参考\nICP备案帮助\n区域选择说明\n鉴权认证机制\n获取AK/SK\n术语表\n法律条款\nAPI入门指南\n财务参考\n计费说明\n发票\n充值\n续费\n代金券\n账单与收支\n订单管理\n提现\n退款\n配置升/降级\n赔付\n企业组织财务管理\n合同\n常见问题\n千帆大模型服务与开发平台ModelBuilder\n开始使用\n模型\n平台计费\n推理服务API\n平台功能OpenAPI\nSDK参考\n最佳实践\n平台操作\n常见问题\n相关协议\n文档中心\n千帆大模型平台ModelBuilder\n千帆大模型平台ModelBuilder\n产品详情\n百度智能云千帆大模型服务与开发平台ModelBuilder是面向企业开发者的一站式大模型开发及服务运行平台，不仅提供了包括文心一言底层模型和第三方开源大模型，还提供了各种AI开发工具和整套开发环境，从数据管理到服务云端部署，完备的流程助力各行业生成式AI应用需求落地。\n学习路径\n循序渐进，带您全面了解本平台\n了解\n了解本平台的产品特性，优势和使用场景，让您对千帆有一个全局的认识。\n平台使用指引\n模型调用\n功能使用\n费用说明\n提前了解平台各项服务费用，方便您掌握平台使用成本。\n模型服务\n模型精调\n续费和自动续费\n退款说明\n新手指南\n快速上手平台使用。以最佳实践为例，铺设平台使用前提，节约学习成本。\n模型列表\n场景解决方案·职业教育\nSFT最佳实践\n账号创建与权限分配",
    "fetched_at": "2025-10-17T03:09:05.282981Z"
  },
  "https://cloud.tencent.com/document/product/1129/74712": {
    "provider": "Tencent Hunyuan",
    "title": "Tencent Hunyuan API Documentation",
    "hash": "3046807ce2c0bd1a5e8087d0513096c5376f8d18ad40f7a64eb4e4393252974a",
    "content": "访问管理 概览_腾讯云\n腾讯云\n最新活动\nHOT\n产品\n解决方案\n定价\n企业中心\n云市场\n开发者\n客户支持\n合作与生态\n了解腾讯云\n关闭\n搜索\n访问管理\n文档中心\n入门中心\nAPI 中心\nSDK 中心\n文档活动\n我的反馈\n文档反馈官招募中，报名立赚积分兑换代金券！>\nHOT\n文档中心\n>\n访问管理\n>\n支持CAM的业务接口\n>\n概览\n概览\n最近更新时间：\n2025-10-23 09:48:32\n微信扫一扫\nQQ\n新浪微博\n复制链接\n链接复制成功\n我的收藏\n本页目录：\n简介\n计算\n容器与中间件\n存储\n网络\n数据库\n视频服务\n安全\n大数据\n云通信与企业服务\n行业应用\n人工智能与机器学习\n办公协同\n微信生态\nCDN与边缘\n开发与运维\n物联网\n云平台服务\n简介\n访问管理已经支持对多数腾讯云产品服务进行权限管理。本文主要介绍支持访问管理 CAM 的产品服务的相关信息。具体维度包括授权粒度、控制台、根据标签进行授权、参考文档等。\n以下列表分别罗列了腾讯云平台各大产品类别下已支持 CAM 的服务。\n对表中信息进行如下定义：\n产品中文名：支持 CAM 的云服务的名称，单击链接至对应产品服务文档，方便您快速获取相关信息。\n产品简称：支持 CAM 的产品简称。\n授权粒度：当前服务提供的最小授权粒度。\n说明：\n其中授权粒度按照粒度粗细分为服务级、操作级和资源级三个级别。\n服务级：定义对服务的整体是否拥有访问权限，分为允许对服务拥有全部操作权限或者拒绝对服务拥有全部操作权限。\n操作级：定义对服务的特定接口（API）是否拥有访问权限，例如：授权某账号对云服务器服务进行只读操作。\n资源级：定义对特定资源是否有访问权限，这是最细的授权粒度，例如：授权某账号仅读写操作某台云服务器。\n控制台：是否支持子账号通过控制台访问当前服务，“✓”表示支持，“-”表示暂不支持。\n按标签授权：当前服务是否支持通过标签进行权限管理，“✓”表示支持，“-”表示暂不支持。\n接口列表：业务支持 CAM 接口详情，可单击链接查看。\n计算\nCAM中产品名\nCAM中简称\n授权粒度\n控制台访问\n按标签授权\n接口列表\n弹性伸缩\nas\n资源级\n支持\n支持\n详细接口列表\n批量计算\nbatch\n资源级\n支持\n支持\n详细接口列表\n黑石物理服务器\nbm\n资源级\n支持\n支持\n详细接口列表\n黑石负载均衡\nbmlb\n资源级\n支持\n不支持\n详细接口列表\n黑石私有网络\nbmvpc\n资源级\n支持\n不支持\n详细接口列表\n本地专用集群\ncdc\n资源级\n支持\n不支持\n详细接口列表\n专属可用区\ncdz\n操作级\n支持\n不支持\n详细接口列表\n云托付物理服务器\nchc\n操作级\n支持\n不支持\n详细接口列表\n云服务器\ncvm\n资源级\n支持\n支持\n详细接口列表\n高性能应用服务\nhai\n资源级\n支持\n不支持\n详细接口列表\n轻量应用服务器\nlighthouse\n资源级\n支持\n支持\n详细接口列表\n腾讯云自动化助手\ntat\n资源级\n支持\n支持\n详细接口列表\n高性能计算平台\nthpc\n资源级\n支持\n支持\n详细接口列表\n容器与中间件\nCAM中产品名\nCAM中简称\n授权粒度\n控制台访问\n按标签授权\n接口列表\n应用管理平台\ncamp\n资源级\n支持\n不支持\n详细接口列表\n消息队列 CKafka 版\nckafka\n资源级\n支持\n支持\n详细接口列表\n消息队列 CMQ\ncmq\n资源级\n支持\n不支持\n详细接口列表\n云函数\nscf\n资源级\n支持\n支持\n详细接口列表\nServerless 应用中心\nsls\n资源级\n支持\n不支持\n详细接口列表\n服务网格\ntcm\n资源级\n支持\n支持\n详细接口列表\n容器镜像服务\ntcr\n资源级\n支持\n支持\n详细接口列表\n云原生分布式云中心\ntdcc\n资源级\n支持\n支持\n详细接口列表\n消息队列 TDMQ\ntdmq\n资源级\n支持\n支持\n详细接口列表\n弹性微服务\ntem\n资源级\n支持\n支持\n详细接口列表\n容器服务\ntke\n资源级\n支持\n支持\n详细接口列表\n消息队列 RabbitMQ 版\ntrabbit\n资源级\n支持\n支持\n详细接口列表\n消息队列 RocketMQ 版\ntrocket\n资源级\n支持\n支持\n详细接口列表\n腾讯云微服务引擎\ntse\n资源级\n支持\n支持\n详细接口列表\n腾讯微服务平台\ntsf\n操作级\n支持\n不支持\n详细接口列表\n存储\nCAM中产品名\nCAM中简称\n授权粒度\n控制台访问\n按标签授权\n接口列表\n归档存储\ncas\n资源级\n不支持\n支持\n详细接口列表\n云数据迁移\ncdm\n服务级\n支持\n不支持\n详细接口列表\n文件存储\ncfs\n资源级\n支持\n支持\n详细接口列表\n云HDFS\nchdfs\n资源级\n支持\n支持\n详细接口列表\n数据万象\nci\n资源级\n支持\n支持\n详细接口列表\n日志服务\ncls\n资源级\n支持\n支持\n详细接口列表\n对象存储\ncos\n资源级\n支持\n支持\n详细接口列表\n存储网关\ncsg\n资源级\n支持\n支持\n详细接口列表\n数据加速器 GooseFS\ngoosefs\n资源级\n支持\n支持\n详细接口列表\n智能视图计算平台\niss\n资源级\n支持\n不支持\n详细接口列表\n轻量对象存储\nlightcos\n资源级\n支持\n不支持\n详细接口列表\n智能媒资托管\nsmh\n操作级\n支持\n不支持\n详细接口列表\n网络\nCAM中产品名\nCAM中简称\n授权粒度\n控制台访问\n按标签授权\n接口列表\n负载均衡\nclb\n资源级\n支持\n支持\n详细接口列表\n负载均衡身份认证\nclbia\n资源级\n支持\n不支持\n详细接口列表\n专线接入\ndc\n资源级\n支持\n支持\n详细接口列表\n全局接入\nga\n资源级\n支持\n支持\n详细接口列表\n全球应用加速\ngaap\n资源级\n支持\n支持\n详细接口列表\n智能高性能网络\nihn\n操作级\n支持\n不支持\n详细接口列表\n私有网络\nvpc\n资源级\n支持\n支持\n详细接口列表\n数据库\nCAM中产品名\nCAM中简称\n授权粒度\n控制台访问\n按标签授权\n接口列表\n云数据库 MySQL\ncdb\n资源级\n支持\n支持\n详细接口列表\nNoSQL 数据库 CKV\nckv\n资源级\n支持\n支持\n详细接口列表\n时序数据库\nctsdb\n资源级\n支持\n支持\n详细接口列表\n云原生数据库  TDSQL-C\ncynosdb\n资源级\n支持\n支持\n详细接口列表\n数据库智能管家\ndbbrain\n资源级\n支持\n不支持\n详细接口列表\n云数据库独享集群\ndbdc\n操作级\n支持\n不支持\n详细接口列表\n数据库专家服务\ndbexpert\n资源级\n支持\n不支持\n详细接口列表\n数据库备份服务\ndbs\n资源级\n支持\n支持\n详细接口列表\nTDSQL MySQL 版\ndcdb\n资源级\n支持\n支持\n详细接口列表\n数据库管理\ndmc\n资源级\n支持\n支持\n详细接口列表\n数据传输服务\ndts\n资源级\n支持\n支持\n详细接口列表\n云数据库 KeeWiDB\nkeewidb\n资源级\n支持\n支持\n详细接口列表\n轻量数据库\nlighthousedb\n资源级\n支持\n不支持\n详细接口列表\n云数据库 MariaDB（TDSQL）\nmariadb\n资源级\n支持\n支持\n详细接口列表\n云数据库Memcached\nmemcached\n资源级\n支持\n不支持\n详细接口列表\n文档数据库MongoDB\nmongodb\n资源级\n支持\n支持\n详细接口列表\n云数据库PostgreSQL\npostgres\n资源级\n支持\n支持\n详细接口列表\n云数据库 Redis®\nredis\n资源级\n支持\n支持\n详细接口列表\n云数据库 SQL Server\nsqlserver\n资源级\n支持\n支持\n详细接口列表\nTDSQL PostgreSQL版\ntbase\n资源级\n支持\n支持\n详细接口列表\n游戏数据库TcaplusDB\ntcaplusdb\n资源级\n支持\n支持\n详细接口列表\nTDSQL-H LibraDB\ntdach\n资源级\n支持\n支持\n详细接口列表\nTDSQL-C PostgreSQL 版\ntdcpg\n资源级\n支持\n不支持\n详细接口列表\n向量数据库\nvdb\n资源级\n支持\n支持\n详细接口列表\n视频服务\nCAM中产品名\nCAM中简称\n授权粒度\n控制台访问\n按标签授权\n接口列表\n正版图库直通车\nape\n服务级\n支持\n不支持\n详细接口列表\n商业直播\nbizlive\n操作级\n支持\n不支持\n详细接口列表\n应用云渲染\ncar\n操作级\n支持\n不支持\n详细接口列表\n云联络中心\nccc\n操作级\n支持\n不支持\n详细接口列表\n智能创作\ncme\n资源级\n支持\n不支持\n详细接口列表\n互动直播\nconsoleilvb\n服务级\n支持\n不支持\n详细接口列表\n云桌面\ncvd\n资源级\n支持\n不支持\n详细接口列表\n游戏多媒体引擎\ngme\n资源级\n支持\n支持\n详细接口列表\n云游戏\ngs\n操作级\n支持\n不支持\n详细接口列表\n智能编辑\nie\n服务级\n支持\n不支持\n详细接口列表\n即时通信\nim\n资源级\n支持\n支持\n详细接口列表\n实时互动-教育版\nlcic\n操作级\n支持\n不支持\n详细接口列表\n云直播\nlive\n资源级\n支持\n支持\n详细接口列表\n媒体传输\nmdc\n操作级\n支持\n不支持\n详细接口列表\n媒体直播\nmdl\n操作级\n支持\n不支持\n详细接口列表\n媒体包装\nmdp\n操作级\n支持\n不支持\n详细接口列表\n媒体处理\nmps\n资源级\n支持\n支持\n详细接口列表\n泰山创意创作\ntaidc\n操作级\n支持\n不支持\n详细接口列表\n远程实时操控\ntrro\n操作级\n支持\n不支持\n详细接口列表\n实时音视频\ntrtc\n资源级\n支持\n支持\n详细接口列表\n音视频终端引擎\nvcube\n操作级\n支持\n不支持\n详细接口列表\n语音消息\nvms\n资源级\n支持\n支持\n详细接口列表\n云点播\nvod\n资源级\n支持\n支持\n详细接口列表\n音速达直播音乐版权引擎\nyinsuda\n操作级\n不支持\n不支持\n详细接口列表\n安全\nCAM中产品名\nCAM中简称\n授权粒度\n控制台访问\n按标签授权\n接口列表\n活动防刷\naa\n服务级\n支持\n不支持\n详细接口列表\n借贷反欺诈\naf\n服务级\n支持\n不支持\n详细接口列表\n定制建模\nafc\n操作级\n支持\n不支持\n详细接口列表\n大模型安全网关\napis\n操作级\n支持\n不支持\n详细接口列表\n运维安全中心（堡垒机）\nbh\n操作级\n支持\n不支持\n详细接口列表\n品牌经营管家\nbma\n操作级\n支持\n不支持\n详细接口列表\n业务风险情报\nbri\n资源级\n支持\n不支持\n详细接口列表\n验证码\ncaptcha\n资源级\n支持\n支持\n详细接口列表\n数据安全网关（云访问安全代理）\ncasb\n资源级\n支持\n支持\n详细接口列表\n数据安全审计\ncds\n操作级\n支持\n不支持\n详细接口列表\n云防火墙\ncfw\n操作级\n支持\n不支持\n详细接口列表\n身份访问控制\nciam\n操作级\n支持\n不支持\n详细接口列表\n云加密机\ncloudhsm\n资源级\n支持\n支持\n详细接口列表\n内容安全\ncms\n操作级\n支持\n不支持\n详细接口列表\n版权保护平台\ncpp\n操作级\n支持\n不支持\n详细接口列表\n暴露面管理服务\nctem\n操作级\n支持\n不支持\n详细接口列表\n主机安全\ncwp\n资源级\n支持\n支持\n详细接口列表\n数据安全治理中心\ndsgc\n资源级\n支持\n支持\n详细接口列表\n专家兼容测试\nect\n操作级\n支持\n不支持\n详细接口列表\n数字身份管控平台（员工版）\neiam\n操作级\n支持\n不支持\n详细接口列表\n联邦学习\nfele\n操作级\n支持\n不支持\n详细接口列表\n游戏应用安全\ngas\n资源级\n支持\n不支持\n详细接口列表\n保险反欺诈\niaf\n服务级\n支持\n不支持\n详细接口列表\niOA 零信任安全管理系统\nioa\n操作级\n支持\n不支持\n详细接口列表\n置信度评分\nivs\n服务级\n支持\n不支持\n详细接口列表\n密钥管理系统\nkms\n资源级\n支持\n支持\n详细接口列表\n登录保护\nlp\n服务级\n支持\n不支持\n详细接口列表\n小程序安全检测\nmmps\n操作级\n支持\n不支持\n详细接口列表\n移动应用安全\nms\n服务级\n支持\n不支持\n详细接口列表\n安全托管服务\nmss\n操作级\n支持\n不支持\n详细接口列表\n网络资产风险监测系统\nnarms\n服务级\n支持\n不支持\n详细接口列表\n行业风险评估\nra\n服务级\n支持\n不支持\n详细接口列表\n风险识别 RCE\nrce\n操作级\n支持\n不支持\n详细接口列表\n远程调试\nrd\n操作级\n支持\n不支持\n详细接口列表\n注册保护\nrp\n服务级\n支持\n不支持\n详细接口列表\n安全云手机\nscp\n资源级\n支持\n支持\n详细接口列表\n标准兼容测试\nsct\n操作级\n支持\n不支持\n详细接口列表\n安全湖\nsecuritylake\n资源级\n支持\n不支持\n详细接口列表\n手游安全测试\nsr\n操作级\n支持\n不支持\n详细接口列表\n云安全中心\nssa\n操作级\n支持\n不支持\n详细接口列表\n证书监控 SSLPod\nsslpod\n操作级\n支持\n不支持\n详细接口列表\n凭据管理系统\nssm\n资源级\n支持\n支持\n详细接口列表\n安全短链接\nssu\n资源级\n支持\n支持\n详细接口列表\n威胁情报云查与本地引擎\ntae\n资源级\n支持\n支持\n详细接口列表\n流量反欺诈\ntaf\n操作级\n支持\n不支持\n详细接口列表\n容器安全服务\ntcss\n操作级\n支持\n不支持\n详细接口列表\n设备安全\ntds\n操作级\n支持\n不支持\n详细接口列表\n攻击面管理 ASM\ntix\n资源级\n支持\n不支持\n详细接口列表\n安心平台\ntrp\n操作级\n支持\n不支持\n详细接口列表\n黑产检测及情报\nuidai\n资源级\n支持\n支持\n详细接口列表\n文件多引擎判定平台\nvdc\n资源级\n支持\n支持\n详细接口列表\n漏洞扫描服务\nvss\n操作级\n支持\n不支持\n详细接口列表\nWeb 应用防火墙\nwaf\n资源级\n支持\n支持\n详细接口列表\n测试服务\nwetest\n操作级\n支持\n不支持\n详细接口列表\n大数据\nCAM中产品名\nCAM中简称\n授权粒度\n控制台访问\n按标签授权\n接口列表\n商业智能分析\nbi\n操作级\n支持\n不支持\n详细接口列表\n腾讯云数据仓库 TCHouse-C\ncdwch\n资源级\n支持\n支持\n详细接口列表\n腾讯云数据仓库 TCHouse-D\ncdwdoris\n资源级\n支持\n支持\n详细接口列表\n腾讯云数据仓库 TCHouse-P\ncdwpg\n资源级\n支持\n支持\n详细接口列表\n数据湖计算\ndlc\n资源级\n支持\n支持\n详细接口列表\n数字营销\ndm\n服务级\n支持\n不支持\n详细接口列表\n弹性MapReduce\nemr\n资源级\n支持\n支持\n详细接口列表\nElasticsearch Service\nes\n资源级\n支持\n支持\n详细接口列表\n流计算 Oceanus\noceanus\n资源级\n支持\n支持\n详细接口列表\n腾讯云数据仓库 TCHouse-X\ntchousex\n资源级\n支持\n支持\n详细接口列表\n腾讯云图\ntcv\n资源级\n支持\n不支持\n详细接口列表\n腾讯移动推送\ntpns\n资源级\n支持\n支持\n详细接口列表\n检索分析服务\ntsearch\n资源级\n支持\n支持\n详细接口列表\n数据开发与治理平台\nwedata\n资源级\n支持\n支持\n详细接口列表\n云通信与企业服务\nCAM中产品名\nCAM中简称\n授权粒度\n控制台访问\n按标签授权\n接口列表\nICP备案\nba\n操作级\n支持\n不支持\n详细接口列表\n云解析 DNS\ndnspod\n资源级\n支持\n支持\n详细接口列表\n域名注册\ndomain\n操作级\n支持\n不支持\n详细接口列表\n移动解析 HTTPDNS\nhttpdns\n资源级\n支持\n支持\n详细接口列表\n智能全网流量管理\nigtm\n操作级\n支持\n不支持\n详细接口列表\n号码保护\nnpp\n服务级\n支持\n不支持\n详细接口列表\n私有域解析 Private DNS\nprivatedns\n资源级\n支持\n支持\n详细接口列表\n邮件推送\nses\n资源级\n支持\n支持\n详细接口列表\n短信\nsms\n资源级\n支持\n支持\n详细接口列表\nSSL证书\nssl\n资源级\n支持\n支持\n详细接口列表\n腾讯云区块链\ntbaas\n资源级\n支持\n支持\n详细接口列表\n腾讯云拓客通\ntcet\n服务级\n支持\n不支持\n详细接口列表\n分布式身份\ntdid\n操作级\n支持\n不支持\n详细接口列表\n网站建设\nwds\n操作级\n支持\n不支持\n详细接口列表\n行业应用\nCAM中产品名\nCAM中简称\n授权粒度\n控制台访问\n按标签授权\n接口列表\n微信云支付\ncpay\n操作级\n支持\n不支持\n详细接口列表\n金融围笼\nfc\n服务级\n支持\n不支持\n详细接口列表\n家医助手\nfda\n操作级\n支持\n不支持\n详细接口列表\n游戏语音GVoice\ngvoice\n服务级\n支持\n不支持\n详细接口列表\n数学作业批改\nhcm\n操作级\n支持\n不支持\n详细接口列表\n智能导诊\nig\n操作级\n支持\n不支持\n详细接口列表\n智能预问诊\nipc\n操作级\n支持\n不支持\n详细接口列表\n材料研究平台\nmrp\n操作级\n支持\n不支持\n详细接口列表\n医疗报告结构化\nmrs\n操作级\n支持\n不支持\n详细接口列表\n医学术语标准化API\nmtst\n操作级\n支持\n不支持\n详细接口列表\n在线课堂\noics\n服务级\n支持\n不支持\n详细接口列表\n腾讯健康组学平台\nomics\n操作级\n支持\n不支持\n详细接口列表\n题目结构化归档\npsa\n操作级\n支持\n不支持\n详细接口列表\n患者档案\nsmr\n操作级\n支持\n不支持\n详细接口列表\n智聆口语评测\nsoe\n资源级\n支持\n不支持\n详细接口列表\n智能汽车云平台\nsvcp\n操作级\n支持\n不支持\n详细接口列表\n腾讯教育教师助手\ntet\n服务级\n支持\n不支持\n详细接口列表\n互动白板\ntiw\n资源级\n支持\n支持\n详细接口列表\n互动在线课堂\ntoc\n服务级\n支持\n不支持\n详细接口列表\n商标注册\ntr\n资源级\n支持\n支持\n详细接口列表\n企业微信汽车行业版\nwav\n操作级\n支持\n不支持\n详细接口列表\n人工智能与机器学习\nCAM中产品名\nCAM中简称\n授权粒度\n控制台访问\n按标签授权\n接口列表\n腾讯混元生图\naiart\n操作级\n支持\n不支持\n详细接口列表\n音频内容安全\nams\n操作级\n支持\n不支持\n详细接口列表\n语音识别\nasr\n资源级\n支持\n支持\n详细接口列表\n人体分析\nbda\n服务级\n支持\n不支持\n详细接口列表\n智能保险助手\ncii\n服务级\n支持\n不支持\n详细接口列表\n人脸融合\nfacefusion\n操作级\n支持\n不支持\n详细接口列表\n人脸核身\nfaceid\n资源级\n支持\n不支持\n详细接口列表\n人脸试妆\nfmu\n服务级\n支持\n不支持\n详细接口列表\n人像变换\nft\n服务级\n支持\n不支持\n详细接口列表\n腾讯混元大模型\nhunyuan\n操作级\n支持\n不支持\n详细接口列表\n人脸识别\niai\n资源级\n支持\n不支持\n详细接口列表\n对话机器人\nicr\n服务级\n支持\n不支持\n详细接口列表\n智能硬件 AI 语音助手\nihava\n操作级\n支持\n不支持\n详细接口列表\n图片内容安全\nims\n操作级\n支持\n不支持\n详细接口列表\n腾讯云智能数智人\nivh\n操作级\n支持\n不支持\n详细接口列表\n视频智能标签\nivld\n服务级\n支持\n不支持\n详细接口列表\nNLP技术\nnlp\n资源级\n支持\n不支持\n详细接口列表\n文字识别\nocr\n资源级\n支持\n不支持\n详细接口列表\n音色变换\nstsc\n操作级\n支持\n不支持\n详细接口列表\n图像分析\ntiia\n服务级\n不支持\n不支持\n详细接口列表\n腾讯云 TI 平台 TI-ONE\ntione\n资源级\n支持\n支持\n详细接口列表\n文本内容安全\ntms\n操作级\n支持\n不支持\n详细接口列表\n机器翻译\ntmt\n操作级\n支持\n不支持\n详细接口列表\n腾讯同传系统\ntsi\n服务级\n支持\n不支持\n详细接口列表\n语音合成\ntts\n操作级\n支持\n不支持\n详细接口列表\n腾讯混元生视频\nvclm\n操作级\n支持\n不支持\n详细接口列表\n视频内容安全\nvm\n操作级\n支持\n不支持\n详细接口列表\n声音复刻\nvrs\n操作级\n支持\n不支持\n详细接口列表\n办公协同\nCAM中产品名\nCAM中简称\n授权粒度\n控制台访问\n按标签授权\n接口列表\n代理记账\nab\n操作级\n支持\n不支持\n详细接口列表\n腾讯云商付\ncpdp\n操作级\n支持\n不支持\n详细接口列表\n腾讯轻联\neis\n操作级\n支持\n不支持\n详细接口列表\n腾讯电子签\ness\n操作级\n支持\n不支持\n详细接口列表\n腾讯电子签（基础版）\nessbasic\n操作级\n支持\n不支持\n详细接口列表\n腾讯乐享\nlexiang\n操作级\n支持\n不支持\n详细接口列表\n腾讯云安灯\ntandon\n操作级\n支持\n不支持\n详细接口列表\n腾讯浏览服务\ntbs\n操作级\n支持\n不支持\n详细接口列表\n腾讯云 CA\ntcca\n操作级\n支持\n不支持\n详细接口列表\n腾讯云企业网盘\ntced\n操作级\n支持\n不支持\n详细接口列表\n微信生态\nCAM中产品名\nCAM中简称\n授权粒度\n控制台访问\n按标签授权\n接口列表\n腾讯企点\nqidian\n操作级\n支持\n不支持\n详细接口列表\nCDN与边缘\nCAM中产品名\nCAM中简称\n授权粒度\n控制台访问\n按标签授权\n接口列表\nDDoS 防护\nantiddos\n资源级\n支持\n支持\n详细接口列表\n内容分发网络\ncdn\n资源级\n支持\n支持\n详细接口列表\n全站加速网络\necdn\n资源级\n支持\n支持\n详细接口列表\n边缘计算机器\necm\n资源级\n支持\n支持\n详细接口列表\n物联网边缘计算平台\niecp\n操作级\n支持\n不支持\n详细接口列表\n移动网络加速\nmna\n操作级\n支持\n不支持\n详细接口列表\n边缘安全加速平台 EO\nteo\n资源级\n支持\n支持\n详细接口列表\n开发与运维\nCAM中产品名\nCAM中简称\n授权粒度\n控制台访问\n按标签授权\n接口列表\n云顾问\nadvisor\n资源级\n支持\n支持\n详细接口列表\n云 API\napi\n操作级\n支持\n不支持\n详细接口列表\n应用性能监控\napm\n资源级\n支持\n支持\n详细接口列表\n流程服务\nbpaas\n资源级\n支持\n不支持\n详细接口列表\n软件成分分析\nbsca\n资源级\n不支持\n支持\n详细接口列表\n访问管理\ncam\n操作级\n支持\n不支持\n详细接口列表\n云拨测\ncat\n操作级\n支持\n不支持\n详细接口列表\n云顾问-混沌演练\ncfg\n资源级\n支持\n支持\n详细接口列表\n云应用\ncloudapp\n操作级\n支持\n不支持\n详细接口列表\n操作审计\ncloudaudit\n操作级\n支持\n不支持\n详细接口列表\n云端 IDE\ncloudstudio\n操作级\n支持\n不支持\n详细接口列表\n云迁移\ncmg\n操作级\n支持\n不支持\n详细接口列表\n设计协作平台\ncodesign\n操作级\n支持\n不支持\n详细接口列表\nCODING DevOps\ncoding\n服务级\n支持\n不支持\n详细接口列表\n配置审计\nconfig\n资源级\n支持\n不支持\n详细接口列表\n控制中心\ncontrolcenter\n资源级\n支持\n不支持\n详细接口列表\n服务迁移\ncsm\n操作级\n支持\n不支持\n详细接口列表\n事件总线\neb\n资源级\n支持\n支持\n详细接口列表\n身份识别平台\niap\n操作级\n支持\n不支持\n详细接口列表\n地图基础服务\nlbs\n资源级\n支持\n支持\n详细接口列表\n云开发低码\nlowcode\n资源级\n支持\n支持\n详细接口列表\n腾讯云可观测平台\nmonitor\n资源级\n支持\n支持\n详细接口列表\n云迁移\nmsp\n操作级\n支持\n不支持\n详细接口列表\n集团账号管理\norganization\n操作级\n支持\n不支持\n详细接口列表\n云压测\npts\n资源级\n支持\n支持\n详细接口列表\n前端性能监控\nrum\n资源级\n支持\n支持\n详细接口列表\n安全凭证服务\nsts\n资源级\n支持\n不支持\n详细接口列表\n节省计划\nsvp\n资源级\n支持\n支持\n详细接口列表\n云原生自动化测试\ntaas\n资源级\n支持\n不支持\n详细接口列表\n标签\ntag\n资源级\n支持\n不支持\n详细接口列表\n腾讯云代码分析\ntcap\n操作级\n支持\n不支持\n详细接口列表\n云开发\ntcb\n资源级\n支持\n支持\n详细接口列表\n云托管 CloudBase Run\ntcbr\n资源级\n支持\n支持\n详细接口列表\n腾讯云健康看板\ntchd\n操作级\n支持\n不支持\n详细接口列表\n腾讯云超级应用服务\ntcmpp\n操作级\n支持\n不支持\n详细接口列表\n腾讯云超级应用服务\ntcsas\n操作级\n支持\n不支持\n详细接口列表\nTRTC云助手\ntrtccopilot\n操作级\n支持\n不支持\n详细接口列表\n物联网\nCAM中产品名\nCAM中简称\n授权粒度\n控制台访问\n按标签授权\n接口列表\n物联网通信\niotcloud\n操作级\n支持\n不支持\n详细接口列表\n物联网开发平台\niotexplorer\n资源级\n支持\n支持\n详细接口列表\n物联网智能视频服务\niotvideo\n资源级\n支持\n不支持\n详细接口列表\n物联网智能视频服务(行业版)\niotvideoindustry\n服务级\n支持\n不支持\n详细接口列表\n云平台服务\nCAM中产品名\nCAM中简称\n授权粒度\n控制台访问\n按标签授权\n接口列表\n合同管理\nclm\n操作级\n支持\n不支持\n详细接口列表\n财务\nfinance\n操作级\n支持\n不支持\n详细接口列表\n云市场\nmarket\n操作级\n支持\n不支持\n详细接口列表\n上一篇\n:\nHelix\n下一篇\n:\n云服务器\nCopyright © 2013\n-2025\nTencent Cloud. All Rights Reserved.\n腾讯云 版权所有\n深圳市腾讯计算机系统有限公司\nICP备案/许可证号：\n粤B2-20090059\n粤公网安备44030502008569号\n腾讯云计算（北京）有限责任公司\n京ICP证150476号 |\n京ICP备11018762号\n中国站\n中文\nInternational\n文档“捉虫”活动\n检视指定产品文档，发现和反馈有效问题，奖！\nAPI专项\"捉虫\"\n反馈API文档问题，代金券、周边好礼奖不停！\n文档建议，你提了吗\n快来使用腾讯云产品文档，提出有效建议，奖！\n在\n线\n咨\n询\n目录\n返回顶部",
    "fetched_at": "2025-10-23T03:10:58.117945Z"
  },
  "https://platform.moonshot.cn/docs": {
    "provider": "Moonshot Kimi",
    "title": "Moonshot Kimi API Documentation",
    "hash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
    "content": "",
    "fetched_at": "2025-09-12T03:00:47.262224Z"
  },
  "https://learn.microsoft.com/en-us/azure/ai-services/openai/reference": {
    "provider": "Azure OpenAI",
    "title": "Azure OpenAI REST API Reference",
    "hash": "b8ae3689a7552d15d917f8c3711bcfc1acb3f9867f91cd6f161a68fa16854b37",
    "content": "Azure OpenAI in Azure AI Foundry Models REST API reference - Azure OpenAI | Microsoft Learn\nSkip to main content\nSkip to Ask Learn chat experience\nThis browser is no longer supported.\nUpgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.\nDownload Microsoft Edge\nMore info about Internet Explorer and Microsoft Edge\nTable of contents\nExit editor mode\nAsk Learn\nAsk Learn\nFocus mode\nTable of contents\nRead in English\nAdd\nAdd to plan\nEdit\nShare via\nFacebook\nx.com\nLinkedIn\nEmail\nPrint\nNote\nAccess to this page requires authorization. You can try\nsigning in\nor\nchanging directories\n.\nAccess to this page requires authorization. You can try\nchanging directories\n.\nAzure OpenAI in Azure AI Foundry Models REST API reference\n2025-07-31\nFeedback\nIn this article\nThis article provides details on the inference REST API endpoints for Azure OpenAI.\nAPI specs\nManaging and interacting with Azure OpenAI models and resources is divided across three primary API surfaces:\nControl plane\nData plane - authoring\nData plane - inference\nEach API surface/specification encapsulates a different set of Azure OpenAI capabilities. Each API has its own unique set of preview and stable/generally available (GA) API releases. Preview releases currently tend to follow a monthly cadence.\nImportant\nThere is now a new preview inference API. Learn more in our\nAPI lifecycle guide\n.\nAPI\nLatest preview release\nLatest GA release\nSpecifications\nDescription\nControl plane\n2025-07-01-preview\n2025-06-01\nSpec files\nThe control plane API is used for operations like\ncreating resources\n,\nmodel deployment\n, and other higher level resource management tasks. The control plane also governs what is possible to do with capabilities like Azure Resource Manager, Bicep, Terraform, and Azure CLI.\nData plane\nv1 preview\nv1\nSpec files\nThe data plane API controls inference and authoring operations.\nAuthentication\nAzure OpenAI provides two methods for authentication. You can use  either API Keys or Microsoft Entra ID.\nAPI Key authentication\n: For this type of authentication, all API requests must include the API Key in the\napi-key\nHTTP header. The\nQuickstart\nprovides guidance for how to make calls with this type of authentication.\nMicrosoft Entra ID authentication\n: You can authenticate an API call using a Microsoft Entra token. Authentication tokens are included in a request as the\nAuthorization\nheader. The token provided must be preceded by\nBearer\n, for example\nBearer YOUR_AUTH_TOKEN\n. You can read our how-to guide on\nauthenticating with Microsoft Entra ID\n.\nREST API versioning\nThe service APIs are versioned using the\napi-version\nquery parameter. All versions follow the YYYY-MM-DD date structure. For example:\nPOST https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT_NAME/chat/completions?api-version=2024-06-01\nData plane inference\nThe rest of the article covers the latest GA release of the Azure OpenAI data plane inference specification,\n2024-10-21\n.\nIf you're looking for documentation on the latest preview API release, refer to the\nlatest preview data plane inference API\nCompletions\nPOST https://{endpoint}/openai/deployments/{deployment-id}/completions?api-version=2024-10-21\nCreates a completion for the provided prompt, parameters, and chosen model.\nURI Parameters\nName\nIn\nRequired\nType\nDescription\nendpoint\npath\nYes\nstring\nurl\nSupported Azure OpenAI endpoints (protocol and hostname, for example:\nhttps://aoairesource.openai.azure.com\n. Replace \"aoairesource\" with your Azure OpenAI resource name). https://{your-resource-name}.openai.azure.com\ndeployment-id\npath\nYes\nstring\nDeployment ID of the model which was deployed.\napi-version\nquery\nYes\nstring\nAPI version\nRequest Header\nName\nRequired\nType\nDescription\napi-key\nTrue\nstring\nProvide Azure OpenAI API key here\nRequest Body\nContent-Type\n: application/json\nName\nType\nDescription\nRequired\nDefault\nprompt\nstring or array\nThe prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.\nNote that <|endoftext|> is the document separator that the model sees during training, so if a prompt isn't specified the model will generate as if from the beginning of a new document.\nYes\nbest_of\ninteger\nGenerates\nbest_of\ncompletions server-side and returns the \"best\" (the one with the highest log probability per token). Results can't be streamed.\nWhen used with\nn\n,\nbest_of\ncontrols the number of candidate completions and\nn\nspecifies how many to return Ã¢â¬â\nbest_of\nmust be greater than\nn\n.\nNote:\nBecause this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for\nmax_tokens\nand\nstop\n.\nNo\n1\necho\nboolean\nEcho back the prompt in addition to the completion\nNo\nFalse\nfrequency_penalty\nnumber\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\nNo\n0\nlogit_bias\nobject\nModify the likelihood of specified tokens appearing in the completion.\nAccepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\nAs an example, you can pass\n{\"50256\": -100}\nto prevent the <|endoftext|> token from being generated.\nNo\nNone\nlogprobs\ninteger\nInclude the log probabilities on the\nlogprobs\nmost likely output tokens, as well the chosen tokens. For example, if\nlogprobs\nis 5, the API will return a list of the five most likely tokens. The API will always return the\nlogprob\nof the sampled token, so there may be up to\nlogprobs+1\nelements in the response.\nThe maximum value for\nlogprobs\nis 5.\nNo\nNone\nmax_tokens\ninteger\nThe maximum number of tokens that can be generated in the completion.\nThe token count of your prompt plus\nmax_tokens\ncan't exceed the model's context length.\nNo\n16\nn\ninteger\nHow many completions to generate for each prompt.\nNote:\nBecause this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for\nmax_tokens\nand\nstop\n.\nNo\n1\npresence_penalty\nnumber\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\nNo\n0\nseed\ninteger\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same\nseed\nand parameters should return the same result.\nDeterminism isn't guaranteed, and you should refer to the\nsystem_fingerprint\nresponse parameter to monitor changes in the backend.\nNo\nstop\nstring or array\nUp to four sequences where the API will stop generating further tokens. The returned text won't contain the stop sequence.\nNo\nstream\nboolean\nWhether to stream back partial progress. If set, tokens will be sent as data-only\nserver-sent events\nas they become available, with the stream terminated by a\ndata: [DONE]\nmessage.\nNo\nFalse\nsuffix\nstring\nThe suffix that comes after a completion of inserted text.\nThis parameter is only supported for\ngpt-3.5-turbo-instruct\n.\nNo\nNone\ntemperature\nnumber\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\nWe generally recommend altering this or\ntop_p\nbut not both.\nNo\n1\ntop_p\nnumber\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or\ntemperature\nbut not both.\nNo\n1\nuser\nstring\nA unique identifier representing your end-user, which can help to monitor and detect abuse.\nNo\nResponses\nStatus Code:\n200\nDescription\n: OK\nContent-Type\nType\nDescription\napplication/json\ncreateCompletionResponse\nRepresents a completion response from the API. Note: both the streamed and nonstreamed response objects share the same shape (unlike the chat endpoint).\nStatus Code:\ndefault\nDescription\n: Service unavailable\nContent-Type\nType\nDescription\napplication/json\nerrorResponse\nExamples\nExample\nCreates a completion for the provided prompt, parameters, and chosen model.\nPOST https://{endpoint}/openai/deployments/{deployment-id}/completions?api-version=2024-10-21\n{\n\"prompt\": [\n\"tell me a joke about mango\"\n],\n\"max_tokens\": 32,\n\"temperature\": 1.0,\n\"n\": 1\n}\nResponses\n:\nStatus Code: 200\n{\n\"body\": {\n\"id\": \"cmpl-7QmVI15qgYVllxK0FtxVGG6ywfzaq\",\n\"created\": 1686617332,\n\"choices\": [\n{\n\"text\": \"es\\n\\nWhat do you call a mango who's in charge?\\n\\nThe head mango.\",\n\"index\": 0,\n\"finish_reason\": \"stop\",\n\"logprobs\": null\n}\n],\n\"usage\": {\n\"completion_tokens\": 20,\n\"prompt_tokens\": 6,\n\"total_tokens\": 26\n}\n}\n}\nEmbeddings\nPOST https://{endpoint}/openai/deployments/{deployment-id}/embeddings?api-version=2024-10-21\nGet a vector representation of a given input that can be easily consumed by machine learning models and algorithms.\nURI Parameters\nName\nIn\nRequired\nType\nDescription\nendpoint\npath\nYes\nstring\nurl\nSupported Azure OpenAI endpoints (protocol and hostname, for example:\nhttps://aoairesource.openai.azure.com\n. Replace \"aoairesource\" with your Azure OpenAI resource name). https://{your-resource-name}.openai.azure.com\ndeployment-id\npath\nYes\nstring\napi-version\nquery\nYes\nstring\nAPI version\nRequest Header\nName\nRequired\nType\nDescription\napi-key\nTrue\nstring\nProvide Azure OpenAI API key here\nRequest Body\nContent-Type\n: application/json\nName\nType\nDescription\nRequired\nDefault\ninput\nstring or array\nInput text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or array of token arrays. The input must not exceed the max input tokens for the model (8,192 tokens for\ntext-embedding-ada-002\n), can't be an empty string, and any array must be 2,048 dimensions or less.\nYes\nuser\nstring\nA unique identifier representing your end-user, which can help monitoring and detecting abuse.\nNo\ninput_type\nstring\ninput type of embedding search to use\nNo\nencoding_format\nstring\nThe format to return the embeddings in. Can be either\nfloat\nor\nbase64\n. Defaults to\nfloat\n.\nNo\ndimensions\ninteger\nThe number of dimensions the resulting output embeddings should have. Only supported in\ntext-embedding-3\nand later models.\nNo\nResponses\nName\nType\nDescription\nRequired\nDefault\nobject\nstring\nYes\nmodel\nstring\nYes\ndata\narray\nYes\nusage\nobject\nYes\nProperties for usage\nprompt_tokens\nName\nType\nDescription\nDefault\nprompt_tokens\ninteger\ntotal_tokens\nName\nType\nDescription\nDefault\ntotal_tokens\ninteger\nStatus Code:\n200\nDescription\n: OK\nContent-Type\nType\nDescription\napplication/json\nobject\nExamples\nExample\nReturn the embeddings for a given prompt.\nPOST https://{endpoint}/openai/deployments/{deployment-id}/embeddings?api-version=2024-10-21\n{\n\"input\": [\n\"this is a test\"\n]\n}\nResponses\n:\nStatus Code: 200\n{\n\"body\": {\n\"data\": [\n{\n\"index\": 0,\n\"embedding\": [\n-0.012838088,\n-0.007421397,\n-0.017617522,\n-0.028278312,\n-0.018666342,\n0.01737855,\n-0.01821495,\n-0.006950092,\n-0.009937238,\n-0.038580645,\n0.010674067,\n0.02412286,\n-0.013647936,\n0.013189907,\n0.0021125758,\n0.012406612,\n0.020790534,\n0.00074595667,\n0.008397198,\n-0.00535031,\n0.008968075,\n0.014351576,\n-0.014086051,\n0.015055214,\n-0.022211088,\n-0.025198232,\n0.0065186154,\n-0.036350243,\n0.009180495,\n-0.009698266,\n0.009446018,\n-0.008463579,\n-0.0040426035,\n-0.03443847,\n-0.00091273896,\n-0.0019217303,\n0.002349888,\n-0.021560553,\n0.016515596,\n-0.015572986,\n0.0038666942,\n-8.432463e-05\n]\n}\n],\n\"usage\": {\n\"prompt_tokens\": 4,\n\"total_tokens\": 4\n}\n}\n}\nChat completions\nPOST https://{endpoint}/openai/deployments/{deployment-id}/chat/completions?api-version=2024-10-21\nCreates a completion for the chat message\nURI Parameters\nName\nIn\nRequired\nType\nDescription\nendpoint\npath\nYes\nstring\nurl\nSupported Azure OpenAI endpoints (protocol and hostname, for example:\nhttps://aoairesource.openai.azure.com\n. Replace \"aoairesource\" with your Azure OpenAI resource name). https://{your-resource-name}.openai.azure.com\ndeployment-id\npath\nYes\nstring\nDeployment ID of the model which was deployed.\napi-version\nquery\nYes\nstring\nAPI version\nRequest Header\nName\nRequired\nType\nDescription\napi-key\nTrue\nstring\nProvide Azure OpenAI API key here\nRequest Body\nContent-Type\n: application/json\nName\nType\nDescription\nRequired\nDefault\ntemperature\nnumber\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\nWe generally recommend altering this or\ntop_p\nbut not both.\nNo\n1\ntop_p\nnumber\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or\ntemperature\nbut not both.\nNo\n1\nstream\nboolean\nIf set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only\nserver-sent events\nas they become available, with the stream terminated by a\ndata: [DONE]\nmessage.\nNo\nFalse\nstop\nstring or array\nUp to four sequences where the API will stop generating further tokens.\nNo\nmax_tokens\ninteger\nThe maximum number of tokens that can be generated in the chat completion.\nThe total length of input tokens and generated tokens is limited by the model's context length.\nNo\nmax_completion_tokens\ninteger\nAn upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\nNo\npresence_penalty\nnumber\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\nNo\n0\nfrequency_penalty\nnumber\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\nNo\n0\nlogit_bias\nobject\nModify the likelihood of specified tokens appearing in the completion.\nAccepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\nNo\nNone\nuser\nstring\nA unique identifier representing your end-user, which can help to monitor and detect abuse.\nNo\nmessages\narray\nA list of messages comprising the conversation so far.\nYes\ndata_sources\narray\nThe configuration entries for Azure OpenAI chat extensions that use them.\nThis additional specification is only compatible with Azure OpenAI.\nNo\nlogprobs\nboolean\nWhether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the\ncontent\nof\nmessage\n.\nNo\nFalse\ntop_logprobs\ninteger\nAn integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability.\nlogprobs\nmust be set to\ntrue\nif this parameter is used.\nNo\nn\ninteger\nHow many chat completion choices to generate for each input message. Note that you'll be charged based on the number of generated tokens across all of the choices. Keep\nn\nas\n1\nto minimize costs.\nNo\n1\nparallel_tool_calls\nParallelToolCalls\nWhether to enable parallel function calling during tool use.\nNo\nTrue\nresponse_format\nResponseFormatText\nor\nResponseFormatJsonObject\nor\nResponseFormatJsonSchema\nAn object specifying the format that the model must output. Compatible with\nGPT-4o\n,\nGPT-4o mini\n,\nGPT-4 Turbo\nand all\nGPT-3.5\nTurbo models newer than\ngpt-3.5-turbo-1106\n.\nSetting to\n{ \"type\": \"json_schema\", \"json_schema\": {...} }\nenables Structured Outputs which guarantees the model will match your supplied JSON schema.\nSetting to\n{ \"type\": \"json_object\" }\nenables JSON mode, which guarantees the message the model generates is valid JSON.\nImportant:\nwhen using JSON mode, you\nmust\nalso instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if\nfinish_reason=\"length\"\n, which indicates the generation exceeded\nmax_tokens\nor the conversation exceeded the max context length.\nNo\nseed\ninteger\nThis feature is in Beta.\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same\nseed\nand parameters should return the same result.\nDeterminism isn't guaranteed, and you should refer to the\nsystem_fingerprint\nresponse parameter to monitor changes in the backend.\nNo\ntools\narray\nA list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\nNo\ntool_choice\nchatCompletionToolChoiceOption\nControls which (if any) tool is called by the model.\nnone\nmeans the model won't call any tool and instead generates a message.\nauto\nmeans the model can pick between generating a message or calling one or more tools.\nrequired\nmeans the model must call one or more tools. Specifying a particular tool via\n{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}\nforces the model to call that tool.\nnone\nis the default when no tools are present.\nauto\nis the default if tools are present.\nNo\nfunction_call\nstring or\nchatCompletionFunctionCallOption\nDeprecated in favor of\ntool_choice\n.\nControls which (if any) function is called by the model.\nnone\nmeans the model won't call a function and instead generates a message.\nauto\nmeans the model can pick between generating a message or calling a function.\nSpecifying a particular function via\n{\"name\": \"my_function\"}\nforces the model to call that function.\nnone\nis the default when no functions are present.\nauto\nis the default if functions are present.\nNo\nfunctions\narray\nDeprecated in favor of\ntools\n.\nA list of functions the model may generate JSON inputs for.\nNo\nResponses\nStatus Code:\n200\nDescription\n: OK\nContent-Type\nType\nDescription\napplication/json\ncreateChatCompletionResponse\nor\ncreateChatCompletionStreamResponse\nStatus Code:\ndefault\nDescription\n: Service unavailable\nContent-Type\nType\nDescription\napplication/json\nerrorResponse\nExamples\nExample\nCreates a completion for the provided prompt, parameters, and chosen model.\nPOST https://{endpoint}/openai/deployments/{deployment-id}/chat/completions?api-version=2024-10-21\n{\n\"messages\": [\n{\n\"role\": \"system\",\n\"content\": \"you are a helpful assistant that talks like a pirate\"\n},\n{\n\"role\": \"user\",\n\"content\": \"can you tell me how to care for a parrot?\"\n}\n]\n}\nResponses\n:\nStatus Code: 200\n{\n\"body\": {\n\"id\": \"chatcmpl-7R1nGnsXO8n4oi9UPz2f3UHdgAYMn\",\n\"created\": 1686676106,\n\"choices\": [\n{\n\"index\": 0,\n\"finish_reason\": \"stop\",\n\"message\": {\n\"role\": \"assistant\",\n\"content\": \"Ahoy matey! So ye be wantin' to care for a fine squawkin' parrot, eh? Well, shiver me timbers, let ol' Cap'n Assistant share some wisdom with ye! Here be the steps to keepin' yer parrot happy 'n healthy:\\n\\n1. Secure a sturdy cage: Yer parrot be needin' a comfortable place to lay anchor! Be sure ye get a sturdy cage, at least double the size of the bird's wingspan, with enough space to spread their wings, yarrrr!\\n\\n2. Perches 'n toys: Aye, parrots need perches of different sizes, shapes, 'n textures to keep their feet healthy. Also, a few toys be helpin' to keep them entertained 'n their minds stimulated, arrrh!\\n\\n3. Proper grub: Feed yer feathered friend a balanced diet of high-quality pellets, fruits, 'n veggies to keep 'em strong 'n healthy. Give 'em fresh water every day, or ye\\u00e2\\u20ac\\u2122ll have a scurvy bird on yer hands!\\n\\n4. Cleanliness: Swab their cage deck! Clean their cage on a regular basis: fresh water 'n food daily, the floor every couple of days, 'n a thorough scrubbing ev'ry few weeks, so the bird be livin' in a tidy haven, arrhh!\\n\\n5. Socialize 'n train: Parrots be a sociable lot, arrr! Exercise 'n interact with 'em daily to create a bond 'n maintain their mental 'n physical health. Train 'em with positive reinforcement, treat 'em kindly, yarrr!\\n\\n6. Proper rest: Yer parrot be needin' \\u00e2\\u20ac\\u2122bout 10-12 hours o' sleep each night. Cover their cage 'n let them slumber in a dim, quiet quarter for a proper night's rest, ye scallywag!\\n\\n7. Keep a weather eye open for illness: Birds be hidin' their ailments, arrr! Be watchful for signs of sickness, such as lethargy, loss of appetite, puffin' up, or change in droppings, and make haste to a vet if need be.\\n\\n8. Provide fresh air 'n avoid toxins: Parrots be sensitive to draft and pollutants. Keep yer quarters well ventilated, but no drafts, arrr! Be mindful of toxins like Teflon fumes, candles, or air fresheners.\\n\\nSo there ye have it, me hearty! With proper care 'n commitment, yer parrot will be squawkin' \\\"Yo-ho-ho\\\" for many years to come! Good luck, sailor, and may the wind be at yer back!\"\n}\n}\n],\n\"usage\": {\n\"completion_tokens\": 557,\n\"prompt_tokens\": 33,\n\"total_tokens\": 590\n}\n}\n}\nExample\nCreates a completion based on Azure Search data and system-assigned managed identity.\nPOST https://{endpoint}/openai/deployments/{deployment-id}/chat/completions?api-version=2024-10-21\n{\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": \"can you tell me how to care for a dog?\"\n}\n],\n\"data_sources\": [\n{\n\"type\": \"azure_search\",\n\"parameters\": {\n\"endpoint\": \"https://your-search-endpoint.search.windows.net/\",\n\"index_name\": \"{index name}\",\n\"authentication\": {\n\"type\": \"system_assigned_managed_identity\"\n}\n}\n}\n]\n}\nResponses\n:\nStatus Code: 200\n{\n\"body\": {\n\"id\": \"chatcmpl-7R1nGnsXO8n4oi9UPz2f3UHdgAYMn\",\n\"created\": 1686676106,\n\"choices\": [\n{\n\"index\": 0,\n\"finish_reason\": \"stop\",\n\"message\": {\n\"role\": \"assistant\",\n\"content\": \"Content of the completion [doc1].\",\n\"context\": {\n\"citations\": [\n{\n\"content\": \"Citation content.\",\n\"title\": \"Citation Title\",\n\"filepath\": \"contoso.txt\",\n\"url\": \"https://contoso.blob.windows.net/container/contoso.txt\",\n\"chunk_id\": \"0\"\n}\n],\n\"intent\": \"dog care\"\n}\n}\n}\n],\n\"usage\": {\n\"completion_tokens\": 557,\n\"prompt_tokens\": 33,\n\"total_tokens\": 590\n}\n}\n}\nExample\nCreates a completion based on Azure Search vector data, previous assistant message and user-assigned managed identity.\nPOST https://{endpoint}/openai/deployments/{deployment-id}/chat/completions?api-version=2024-10-21\n{\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": \"can you tell me how to care for a cat?\"\n},\n{\n\"role\": \"assistant\",\n\"content\": \"Content of the completion [doc1].\",\n\"context\": {\n\"intent\": \"cat care\"\n}\n},\n{\n\"role\": \"user\",\n\"content\": \"how about dog?\"\n}\n],\n\"data_sources\": [\n{\n\"type\": \"azure_search\",\n\"parameters\": {\n\"endpoint\": \"https://your-search-endpoint.search.windows.net/\",\n\"authentication\": {\n\"type\": \"user_assigned_managed_identity\",\n\"managed_identity_resource_id\": \"/subscriptions/{subscription-id}/resourceGroups/{resource-group}/providers/Microsoft.ManagedIdentity/userAssignedIdentities/{resource-name}\"\n},\n\"index_name\": \"{index name}\",\n\"query_type\": \"vector\",\n\"embedding_dependency\": {\n\"type\": \"deployment_name\",\n\"deployment_name\": \"{embedding deployment name}\"\n},\n\"in_scope\": true,\n\"top_n_documents\": 5,\n\"strictness\": 3,\n\"role_information\": \"You are an AI assistant that helps people find information.\",\n\"fields_mapping\": {\n\"content_fields_separator\": \"\\\\n\",\n\"content_fields\": [\n\"content\"\n],\n\"filepath_field\": \"filepath\",\n\"title_field\": \"title\",\n\"url_field\": \"url\",\n\"vector_fields\": [\n\"contentvector\"\n]\n}\n}\n}\n]\n}\nResponses\n:\nStatus Code: 200\n{\n\"body\": {\n\"id\": \"chatcmpl-7R1nGnsXO8n4oi9UPz2f3UHdgAYMn\",\n\"created\": 1686676106,\n\"choices\": [\n{\n\"index\": 0,\n\"finish_reason\": \"stop\",\n\"message\": {\n\"role\": \"assistant\",\n\"content\": \"Content of the completion [doc1].\",\n\"context\": {\n\"citations\": [\n{\n\"content\": \"Citation content 2.\",\n\"title\": \"Citation Title 2\",\n\"filepath\": \"contoso2.txt\",\n\"url\": \"https://contoso.blob.windows.net/container/contoso2.txt\",\n\"chunk_id\": \"0\"\n}\n],\n\"intent\": \"dog care\"\n}\n}\n}\n],\n\"usage\": {\n\"completion_tokens\": 557,\n\"prompt_tokens\": 33,\n\"total_tokens\": 590\n}\n}\n}\nExample\nCreates a completion for the provided Azure Cosmos DB.\nPOST https://{endpoint}/openai/deployments/{deployment-id}/chat/completions?api-version=2024-10-21\n{\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": \"can you tell me how to care for a dog?\"\n}\n],\n\"data_sources\": [\n{\n\"type\": \"azure_cosmos_db\",\n\"parameters\": {\n\"authentication\": {\n\"type\": \"connection_string\",\n\"connection_string\": \"mongodb+srv://rawantest:{password}$@{cluster-name}.mongocluster.cosmos.azure.com/?tls=true&authMechanism=SCRAM-SHA-256&retrywrites=false&maxIdleTimeMS=120000\"\n},\n\"database_name\": \"vectordb\",\n\"container_name\": \"azuredocs\",\n\"index_name\": \"azuredocindex\",\n\"embedding_dependency\": {\n\"type\": \"deployment_name\",\n\"deployment_name\": \"{embedding deployment name}\"\n},\n\"fields_mapping\": {\n\"content_fields\": [\n\"content\"\n],\n\"vector_fields\": [\n\"contentvector\"\n]\n}\n}\n}\n]\n}\nResponses\n:\nStatus Code: 200\n{\n\"body\": {\n\"id\": \"chatcmpl-7R1nGnsXO8n4oi9UPz2f3UHdgAYMn\",\n\"created\": 1686676106,\n\"choices\": [\n{\n\"index\": 0,\n\"finish_reason\": \"stop\",\n\"message\": {\n\"role\": \"assistant\",\n\"content\": \"Content of the completion [doc1].\",\n\"context\": {\n\"citations\": [\n{\n\"content\": \"Citation content.\",\n\"title\": \"Citation Title\",\n\"filepath\": \"contoso.txt\",\n\"url\": \"https://contoso.blob.windows.net/container/contoso.txt\",\n\"chunk_id\": \"0\"\n}\n],\n\"intent\": \"dog care\"\n}\n}\n}\n],\n\"usage\": {\n\"completion_tokens\": 557,\n\"prompt_tokens\": 33,\n\"total_tokens\": 590\n}\n}\n}\nTranscriptions - Create\nPOST https://{endpoint}/openai/deployments/{deployment-id}/audio/transcriptions?api-version=2024-10-21\nTranscribes audio into the input language.\nURI Parameters\nName\nIn\nRequired\nType\nDescription\nendpoint\npath\nYes\nstring\nurl\nSupported Azure OpenAI endpoints (protocol and hostname, for example:\nhttps://aoairesource.openai.azure.com\n. Replace \"aoairesource\" with your Azure OpenAI resource name). https://{your-resource-name}.openai.azure.com\ndeployment-id\npath\nYes\nstring\nDeployment ID of the speech to text model.\nFor information about supported models, see [/azure/ai-foundry/openai/concepts/models#audio-models].\napi-version\nquery\nYes\nstring\nAPI version\nRequest Header\nName\nRequired\nType\nDescription\napi-key\nTrue\nstring\nProvide Azure OpenAI API key here\nRequest Body\nContent-Type\n: multipart/form-data\nName\nType\nDescription\nRequired\nDefault\nfile\nstring\nThe audio file object to transcribe.\nYes\nprompt\nstring\nAn optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.\nNo\nresponse_format\naudioResponseFormat\nDefines the format of the output.\nNo\ntemperature\nnumber\nThe sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.\nNo\n0\nlanguage\nstring\nThe language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency.\nNo\nResponses\nStatus Code:\n200\nDescription\n: OK\nContent-Type\nType\nDescription\napplication/json\naudioResponse\nor\naudioVerboseResponse\ntext/plain\nstring\nTranscribed text in the output format (when response_format was one of text, vtt or srt).\nExamples\nExample\nGets transcribed text and associated metadata from provided spoken audio data.\nPOST https://{endpoint}/openai/deployments/{deployment-id}/audio/transcriptions?api-version=2024-10-21\nResponses\n:\nStatus Code: 200\n{\n\"body\": {\n\"text\": \"A structured object when requesting json or verbose_json\"\n}\n}\nExample\nGets transcribed text and associated metadata from provided spoken audio data.\nPOST https://{endpoint}/openai/deployments/{deployment-id}/audio/transcriptions?api-version=2024-10-21\n\"---multipart-boundary\\nContent-Disposition: form-data; name=\\\"file\\\"; filename=\\\"file.wav\\\"\\nContent-Type: application/octet-stream\\n\\nRIFF..audio.data.omitted\\n---multipart-boundary--\"\nResponses\n:\nStatus Code: 200\n{\n\"type\": \"string\",\n\"example\": \"plain text when requesting text, srt, or vtt\"\n}\nTranslations - Create\nPOST https://{endpoint}/openai/deployments/{deployment-id}/audio/translations?api-version=2024-10-21\nTranscribes and translates input audio into English text.\nURI Parameters\nName\nIn\nRequired\nType\nDescription\nendpoint\npath\nYes\nstring\nurl\nSupported Azure OpenAI endpoints (protocol and hostname, for example:\nhttps://aoairesource.openai.azure.com\n. Replace \"aoairesource\" with your Azure OpenAI resource name). https://{your-resource-name}.openai.azure.com\ndeployment-id\npath\nYes\nstring\nDeployment ID of the whisper model which was deployed.\nFor information about supported models, see [/azure/ai-foundry/openai/concepts/models#audio-models].\napi-version\nquery\nYes\nstring\nAPI version\nRequest Header\nName\nRequired\nType\nDescription\napi-key\nTrue\nstring\nProvide Azure OpenAI API key here\nRequest Body\nContent-Type\n: multipart/form-data\nName\nType\nDescription\nRequired\nDefault\nfile\nstring\nThe audio file to translate.\nYes\nprompt\nstring\nAn optional text to guide the model's style or continue a previous audio segment. The prompt should be in English.\nNo\nresponse_format\naudioResponseFormat\nDefines the format of the output.\nNo\ntemperature\nnumber\nThe sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.\nNo\n0\nResponses\nStatus Code:\n200\nDescription\n: OK\nContent-Type\nType\nDescription\napplication/json\naudioResponse\nor\naudioVerboseResponse\ntext/plain\nstring\nTranscribed text in the output format (when response_format was one of text, vtt or srt).\nExamples\nExample\nGets English language transcribed text and associated metadata from provided spoken audio data.\nPOST https://{endpoint}/openai/deployments/{deployment-id}/audio/translations?api-version=2024-10-21\n\"---multipart-boundary\\nContent-Disposition: form-data; name=\\\"file\\\"; filename=\\\"file.wav\\\"\\nContent-Type: application/octet-stream\\n\\nRIFF..audio.data.omitted\\n---multipart-boundary--\"\nResponses\n:\nStatus Code: 200\n{\n\"body\": {\n\"text\": \"A structured object when requesting json or verbose_json\"\n}\n}\nExample\nGets English language transcribed text and associated metadata from provided spoken audio data.\nPOST https://{endpoint}/openai/deployments/{deployment-id}/audio/translations?api-version=2024-10-21\n\"---multipart-boundary\\nContent-Disposition: form-data; name=\\\"file\\\"; filename=\\\"file.wav\\\"\\nContent-Type: application/octet-stream\\n\\nRIFF..audio.data.omitted\\n---multipart-boundary--\"\nResponses\n:\nStatus Code: 200\n{\n\"type\": \"string\",\n\"example\": \"plain text when requesting text, srt, or vtt\"\n}\nImage generation\nPOST https://{endpoint}/openai/deployments/{deployment-id}/images/generations?api-version=2024-10-21\nGenerates a batch of images from a text caption on a given dall-e model deployment\nURI Parameters\nName\nIn\nRequired\nType\nDescription\nendpoint\npath\nYes\nstring\nurl\nSupported Azure OpenAI endpoints (protocol and hostname, for example:\nhttps://aoairesource.openai.azure.com\n. Replace \"aoairesource\" with your Azure OpenAI resource name). https://{your-resource-name}.openai.azure.com\ndeployment-id\npath\nYes\nstring\nDeployment ID of the dall-e model which was deployed.\napi-version\nquery\nYes\nstring\nAPI version\nRequest Header\nName\nRequired\nType\nDescription\napi-key\nTrue\nstring\nProvide Azure OpenAI API key here\nRequest Body\nContent-Type\n: application/json\nName\nType\nDescription\nRequired\nDefault\nprompt\nstring\nA text description of the desired image(s). The maximum length is 4,000 characters.\nYes\nn\ninteger\nThe number of images to generate.\nNo\n1\nsize\nimageSize\nThe size of the generated images.\nNo\n1024x1024\nresponse_format\nimagesResponseFormat\nThe format in which the generated images are returned.\nNo\nurl\nuser\nstring\nA unique identifier representing your end-user, which can help to monitor and detect abuse.\nNo\nquality\nimageQuality\nThe quality of the image that will be generated.\nNo\nstandard\nstyle\nimageStyle\nThe style of the generated images.\nNo\nvivid\nResponses\nStatus Code:\n200\nDescription\n: Ok\nContent-Type\nType\nDescription\napplication/json\ngenerateImagesResponse\nStatus Code:\ndefault\nDescription\n: An error occurred.\nContent-Type\nType\nDescription\napplication/json\ndalleErrorResponse\nExamples\nExample\nCreates images given a prompt.\nPOST https://{endpoint}/openai/deployments/{deployment-id}/images/generations?api-version=2024-10-21\n{\n\"prompt\": \"In the style of WordArt, Microsoft Clippy wearing a cowboy hat.\",\n\"n\": 1,\n\"style\": \"natural\",\n\"quality\": \"standard\"\n}\nResponses\n:\nStatus Code: 200\n{\n\"body\": {\n\"created\": 1698342300,\n\"data\": [\n{\n\"revised_prompt\": \"A vivid, natural representation of Microsoft Clippy wearing a cowboy hat.\",\n\"prompt_filter_results\": {\n\"sexual\": {\n\"severity\": \"safe\",\n\"filtered\": false\n},\n\"violence\": {\n\"severity\": \"safe\",\n\"filtered\": false\n},\n\"hate\": {\n\"severity\": \"safe\",\n\"filtered\": false\n},\n\"self_harm\": {\n\"severity\": \"safe\",\n\"filtered\": false\n},\n\"profanity\": {\n\"detected\": false,\n\"filtered\": false\n}\n},\n\"url\": \"https://dalletipusw2.blob.core.windows.net/private/images/e5451cc6-b1ad-4747-bd46-b89a3a3b8bc3/generated_00.png?se=2023-10-27T17%3A45%3A09Z&...\",\n\"content_filter_results\": {\n\"sexual\": {\n\"severity\": \"safe\",\n\"filtered\": false\n},\n\"violence\": {\n\"severity\": \"safe\",\n\"filtered\": false\n},\n\"hate\": {\n\"severity\": \"safe\",\n\"filtered\": false\n},\n\"self_harm\": {\n\"severity\": \"safe\",\n\"filtered\": false\n}\n}\n}\n]\n}\n}\nComponents\nerrorResponse\nName\nType\nDescription\nRequired\nDefault\nerror\nerror\nNo\nerrorBase\nName\nType\nDescription\nRequired\nDefault\ncode\nstring\nNo\nmessage\nstring\nNo\nerror\nName\nType\nDescription\nRequired\nDefault\nparam\nstring\nNo\ntype\nstring\nNo\ninner_error\ninnerError\nInner error with additional details.\nNo\ninnerError\nInner error with additional details.\nName\nType\nDescription\nRequired\nDefault\ncode\ninnerErrorCode\nError codes for the inner error object.\nNo\ncontent_filter_results\ncontentFilterPromptResults\nInformation about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about jailbreak content and profanity, if it has been detected, and if it has been filtered or not. And information about customer blocklist, if it has been filtered and its id.\nNo\ninnerErrorCode\nError codes for the inner error object.\nDescription\n: Error codes for the inner error object.\nType\n: string\nDefault\n:\nEnum Name\n: InnerErrorCode\nEnum Values\n:\nValue\nDescription\nResponsibleAIPolicyViolation\nThe prompt violated one of more content filter rules.\ndalleErrorResponse\nName\nType\nDescription\nRequired\nDefault\nerror\ndalleError\nNo\ndalleError\nName\nType\nDescription\nRequired\nDefault\nparam\nstring\nNo\ntype\nstring\nNo\ninner_error\ndalleInnerError\nInner error with additional details.\nNo\ndalleInnerError\nInner error with additional details.\nName\nType\nDescription\nRequired\nDefault\ncode\ninnerErrorCode\nError codes for the inner error object.\nNo\ncontent_filter_results\ndalleFilterResults\nInformation about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about jailbreak content and profanity, if it has been detected, and if it has been filtered or not. And information about customer blocklist, if it has been filtered and its id.\nNo\nrevised_prompt\nstring\nThe prompt that was used to generate the image, if there was any revision to the prompt.\nNo\ncontentFilterResultBase\nName\nType\nDescription\nRequired\nDefault\nfiltered\nboolean\nYes\ncontentFilterSeverityResult\nName\nType\nDescription\nRequired\nDefault\nfiltered\nboolean\nYes\nseverity\nstring\nNo\ncontentFilterDetectedResult\nName\nType\nDescription\nRequired\nDefault\nfiltered\nboolean\nYes\ndetected\nboolean\nNo\ncontentFilterDetectedWithCitationResult\nName\nType\nDescription\nRequired\nDefault\ncitation\nobject\nNo\nProperties for citation\nURL\nName\nType\nDescription\nDefault\nURL\nstring\nlicense\nName\nType\nDescription\nDefault\nlicense\nstring\ncontentFilterResultsBase\nInformation about the content filtering results.\nName\nType\nDescription\nRequired\nDefault\nsexual\ncontentFilterSeverityResult\nNo\nviolence\ncontentFilterSeverityResult\nNo\nhate\ncontentFilterSeverityResult\nNo\nself_harm\ncontentFilterSeverityResult\nNo\nprofanity\ncontentFilterDetectedResult\nNo\nerror\nerrorBase\nNo\ncontentFilterPromptResults\nInformation about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about jailbreak content and profanity, if it has been detected, and if it has been filtered or not. And information about customer blocklist, if it has been filtered and its id.\nName\nType\nDescription\nRequired\nDefault\nsexual\ncontentFilterSeverityResult\nNo\nviolence\ncontentFilterSeverityResult\nNo\nhate\ncontentFilterSeverityResult\nNo\nself_harm\ncontentFilterSeverityResult\nNo\nprofanity\ncontentFilterDetectedResult\nNo\nerror\nerrorBase\nNo\njailbreak\ncontentFilterDetectedResult\nNo\ncontentFilterChoiceResults\nInformation about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about third party text and profanity, if it has been detected, and if it has been filtered or not. And information about customer blocklist, if it has been filtered and its id.\nName\nType\nDescription\nRequired\nDefault\nsexual\ncontentFilterSeverityResult\nNo\nviolence\ncontentFilterSeverityResult\nNo\nhate\ncontentFilterSeverityResult\nNo\nself_harm\ncontentFilterSeverityResult\nNo\nprofanity\ncontentFilterDetectedResult\nNo\nerror\nerrorBase\nNo\nprotected_material_text\ncontentFilterDetectedResult\nNo\nprotected_material_code\ncontentFilterDetectedWithCitationResult\nNo\npromptFilterResult\nContent filtering results for a single prompt in the request.\nName\nType\nDescription\nRequired\nDefault\nprompt_index\ninteger\nNo\ncontent_filter_results\ncontentFilterPromptResults\nInformation about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about jailbreak content and profanity, if it has been detected, and if it has been filtered or not. And information about customer blocklist, if it has been filtered and its id.\nNo\npromptFilterResults\nContent filtering results for zero or more prompts in the request. In a streaming request, results for different prompts may arrive at different times or in different orders.\nNo properties defined for this component.\ndalleContentFilterResults\nInformation about the content filtering results.\nName\nType\nDescription\nRequired\nDefault\nsexual\ncontentFilterSeverityResult\nNo\nviolence\ncontentFilterSeverityResult\nNo\nhate\ncontentFilterSeverityResult\nNo\nself_harm\ncontentFilterSeverityResult\nNo\ndalleFilterResults\nInformation about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about jailbreak content and profanity, if it has been detected, and if it has been filtered or not. And information about customer blocklist, if it has been filtered and its id.\nName\nType\nDescription\nRequired\nDefault\nsexual\ncontentFilterSeverityResult\nNo\nviolence\ncontentFilterSeverityResult\nNo\nhate\ncontentFilterSeverityResult\nNo\nself_harm\ncontentFilterSeverityResult\nNo\nprofanity\ncontentFilterDetectedResult\nNo\njailbreak\ncontentFilterDetectedResult\nNo\nchatCompletionsRequestCommon\nName\nType\nDescription\nRequired\nDefault\ntemperature\nnumber\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\nWe generally recommend altering this or\ntop_p\nbut not both.\nNo\n1\ntop_p\nnumber\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or\ntemperature\nbut not both.\nNo\n1\nstream\nboolean\nIf set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a\ndata: [DONE]\nmessage.\nNo\nFalse\nstop\nstring or array\nUp to four sequences where the API will stop generating further tokens.\nNo\nmax_tokens\ninteger\nThe maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens). This value is now deprecated in favor of\nmax_completion_tokens\n, and isn't compatible with o1 series models.\nNo\n4096\nmax_completion_tokens\ninteger\nAn upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\nNo\npresence_penalty\nnumber\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\nNo\n0\nfrequency_penalty\nnumber\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\nNo\n0\nlogit_bias\nobject\nModify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\nNo\nuser\nstring\nA unique identifier representing your end-user, which can help Azure OpenAI to monitor and detect abuse.\nNo\ncreateCompletionRequest\nName\nType\nDescription\nRequired\nDefault\nprompt\nstring or array\nThe prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.\nNote that <|endoftext|> is the document separator that the model sees during training, so if a prompt isn't specified the model will generate as if from the beginning of a new document.\nYes\nbest_of\ninteger\nGenerates\nbest_of\ncompletions server-side and returns the \"best\" (the one with the highest log probability per token). Results can't be streamed.\nWhen used with\nn\n,\nbest_of\ncontrols the number of candidate completions and\nn\nspecifies how many to return Ã¢â¬â\nbest_of\nmust be greater than\nn\n.\nNote:\nBecause this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for\nmax_tokens\nand\nstop\n.\nNo\n1\necho\nboolean\nEcho back the prompt in addition to the completion\nNo\nFalse\nfrequency_penalty\nnumber\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\nNo\n0\nlogit_bias\nobject\nModify the likelihood of specified tokens appearing in the completion.\nAccepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\nAs an example, you can pass\n{\"50256\": -100}\nto prevent the <|endoftext|> token from being generated.\nNo\nNone\nlogprobs\ninteger\nInclude the log probabilities on the\nlogprobs\nmost likely output tokens, as well the chosen tokens. For example, if\nlogprobs\nis 5, the API will return a list of the five most likely tokens. The API will always return the\nlogprob\nof the sampled token, so there may be up to\nlogprobs+1\nelements in the response.\nThe maximum value for\nlogprobs\nis 5.\nNo\nNone\nmax_tokens\ninteger\nThe maximum number of tokens that can be generated in the completion.\nThe token count of your prompt plus\nmax_tokens\ncan't exceed the model's context length.\nNo\n16\nn\ninteger\nHow many completions to generate for each prompt.\nNote:\nBecause this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for\nmax_tokens\nand\nstop\n.\nNo\n1\npresence_penalty\nnumber\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\nNo\n0\nseed\ninteger\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same\nseed\nand parameters should return the same result.\nDeterminism isn't guaranteed, and you should refer to the\nsystem_fingerprint\nresponse parameter to monitor changes in the backend.\nNo\nstop\nstring or array\nUp to four sequences where the API will stop generating further tokens. The returned text won't contain the stop sequence.\nNo\nstream\nboolean\nWhether to stream back partial progress. If set, tokens will be sent as data-only\nserver-sent events\nas they become available, with the stream terminated by a\ndata: [DONE]\nmessage.\nNo\nFalse\nsuffix\nstring\nThe suffix that comes after a completion of inserted text.\nThis parameter is only supported for\ngpt-3.5-turbo-instruct\n.\nNo\nNone\ntemperature\nnumber\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\nWe generally recommend altering this or\ntop_p\nbut not both.\nNo\n1\ntop_p\nnumber\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or\ntemperature\nbut not both.\nNo\n1\nuser\nstring\nA unique identifier representing your end-user, which can help to monitor and detect abuse.\nNo\ncreateCompletionResponse\nRepresents a completion response from the API. Note: both the streamed and nonstreamed response objects share the same shape (unlike the chat endpoint).\nName\nType\nDescription\nRequired\nDefault\nid\nstring\nA unique identifier for the completion.\nYes\nchoices\narray\nThe list of completion choices the model generated for the input prompt.\nYes\ncreated\ninteger\nThe Unix timestamp (in seconds) of when the completion was created.\nYes\nmodel\nstring\nThe model used for completion.\nYes\nprompt_filter_results\npromptFilterResults\nContent filtering results for zero or more prompts in the request. In a streaming request, results for different prompts may arrive at different times or in different orders.\nNo\nsystem_fingerprint\nstring\nThis fingerprint represents the backend configuration that the model runs with.\nCan be used in conjunction with the\nseed\nrequest parameter to understand when backend changes have been made that might impact determinism.\nNo\nobject\nenum\nThe object type, which is always \"text_completion\"\nPossible values: text_completion\nYes\nusage\ncompletionUsage\nUsage statistics for the completion request.\nNo\ncreateChatCompletionRequest\nName\nType\nDescription\nRequired\nDefault\ntemperature\nnumber\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\nWe generally recommend altering this or\ntop_p\nbut not both.\nNo\n1\ntop_p\nnumber\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or\ntemperature\nbut not both.\nNo\n1\nstream\nboolean\nIf set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only\nserver-sent events\nas they become available, with the stream terminated by a\ndata: [DONE]\nmessage.\nNo\nFalse\nstop\nstring or array\nUp to four sequences where the API will stop generating further tokens.\nNo\nmax_tokens\ninteger\nThe maximum number of tokens that can be generated in the chat completion.\nThe total length of input tokens and generated tokens is limited by the model's context length.\nNo\nmax_completion_tokens\ninteger\nAn upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\nNo\npresence_penalty\nnumber\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\nNo\n0\nfrequency_penalty\nnumber\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\nNo\n0\nlogit_bias\nobject\nModify the likelihood of specified tokens appearing in the completion.\nAccepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\nNo\nNone\nuser\nstring\nA unique identifier representing your end-user, which can help to monitor and detect abuse.\nNo\nmessages\narray\nA list of messages comprising the conversation so far.\nYes\ndata_sources\narray\nThe configuration entries for Azure OpenAI chat extensions that use them.\nThis additional specification is only compatible with Azure OpenAI.\nNo\nlogprobs\nboolean\nWhether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the\ncontent\nof\nmessage\n.\nNo\nFalse\ntop_logprobs\ninteger\nAn integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability.\nlogprobs\nmust be set to\ntrue\nif this parameter is used.\nNo\nn\ninteger\nHow many chat completion choices to generate for each input message. Note that you'll be charged based on the number of generated tokens across all of the choices. Keep\nn\nas\n1\nto minimize costs.\nNo\n1\nparallel_tool_calls\nParallelToolCalls\nWhether to enable parallel function calling during tool use.\nNo\nTrue\nresponse_format\nResponseFormatText\nor\nResponseFormatJsonObject\nor\nResponseFormatJsonSchema\nAn object specifying the format that the model must output. Compatible with\nGPT-4o\n,\nGPT-4o mini\n,\nGPT-4 Turbo\nand all\nGPT-3.5\nTurbo models newer than\ngpt-3.5-turbo-1106\n.\nSetting to\n{ \"type\": \"json_schema\", \"json_schema\": {...} }\nenables Structured Outputs which guarantees the model will match your supplied JSON schema.\nSetting to\n{ \"type\": \"json_object\" }\nenables JSON mode, which guarantees the message the model generates is valid JSON.\nImportant:\nwhen using JSON mode, you\nmust\nalso instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if\nfinish_reason=\"length\"\n, which indicates the generation exceeded\nmax_tokens\nor the conversation exceeded the max context length.\nNo\nseed\ninteger\nThis feature is in Beta.\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same\nseed\nand parameters should return the same result.\nDeterminism isn't guaranteed, and you should refer to the\nsystem_fingerprint\nresponse parameter to monitor changes in the backend.\nNo\ntools\narray\nA list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\nNo\ntool_choice\nchatCompletionToolChoiceOption\nControls which (if any) tool is called by the model.\nnone\nmeans the model won't call any tool and instead generates a message.\nauto\nmeans the model can pick between generating a message or calling one or more tools.\nrequired\nmeans the model must call one or more tools. Specifying a particular tool via\n{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}\nforces the model to call that tool.\nnone\nis the default when no tools are present.\nauto\nis the default if tools are present.\nNo\nfunction_call\nstring or\nchatCompletionFunctionCallOption\nDeprecated in favor of\ntool_choice\n.\nControls which (if any) function is called by the model.\nnone\nmeans the model won't call a function and instead generates a message.\nauto\nmeans the model can pick between generating a message or calling a function.\nSpecifying a particular function via\n{\"name\": \"my_function\"}\nforces the model to call that function.\nnone\nis the default when no functions are present.\nauto\nis the default if functions are present.\nNo\nfunctions\narray\nDeprecated in favor of\ntools\n.\nA list of functions the model may generate JSON inputs for.\nNo\nchatCompletionFunctions\nName\nType\nDescription\nRequired\nDefault\ndescription\nstring\nA description of what the function does, used by the model to choose when and how to call the function.\nNo\nname\nstring\nThe name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.\nYes\nparameters\nFunctionParameters\nThe parameters the functions accepts, described as a JSON Schema object.\nSee the guide\nfor examples, and the\nJSON Schema reference\nfor documentation about the format.\nOmitting\nparameters\ndefines a function with an empty parameter list.\nNo\nchatCompletionFunctionCallOption\nSpecifying a particular function via\n{\"name\": \"my_function\"}\nforces the model to call that function.\nName\nType\nDescription\nRequired\nDefault\nname\nstring\nThe name of the function to call.\nYes\nchatCompletionRequestMessage\nThis component can be one of the following:\nchatCompletionRequestSystemMessage\nName\nType\nDescription\nRequired\nDefault\ncontent\nstring or array\nThe contents of the system message.\nYes\nrole\nenum\nThe role of the messages author, in this case\nsystem\n.\nPossible values: system\nYes\nname\nstring\nAn optional name for the participant. Provides the model information to differentiate between participants of the same role.\nNo\nchatCompletionRequestUserMessage\nName\nType\nDescription\nRequired\nDefault\ncontent\nstring or array\nThe contents of the user message.\nYes\nrole\nenum\nThe role of the messages author, in this case\nuser\n.\nPossible values: user\nYes\nname\nstring\nAn optional name for the participant. Provides the model information to differentiate between participants of the same role.\nNo\nchatCompletionRequestAssistantMessage\nName\nType\nDescription\nRequired\nDefault\ncontent\nstring or array\nThe contents of the assistant message. Required unless\ntool_calls\nor\nfunction_call\nis specified.\nNo\nrefusal\nstring\nThe refusal message by the assistant.\nNo\nrole\nenum\nThe role of the messages author, in this case\nassistant\n.\nPossible values: assistant\nYes\nname\nstring\nAn optional name for the participant. Provides the model information to differentiate between participants of the same role.\nNo\ntool_calls\nchatCompletionMessageToolCalls\nThe tool calls generated by the model, such as function calls.\nNo\nfunction_call\nobject\nDeprecated and replaced by\ntool_calls\n. The name and arguments of a function that should be called, as generated by the model.\nNo\nProperties for function_call\narguments\nName\nType\nDescription\nDefault\narguments\nstring\nThe arguments to call the function with, as generated by the model in JSON format. Note that the model doesn't always generate valid JSON, and may generate parameters not defined by your function schema. Validate the arguments in your code before calling your function.\nname\nName\nType\nDescription\nDefault\nname\nstring\nThe name of the function to call.\nchatCompletionRequestToolMessage\nName\nType\nDescription\nRequired\nDefault\nrole\nenum\nThe role of the messages author, in this case\ntool\n.\nPossible values: tool\nYes\ncontent\nstring or array\nThe contents of the tool message.\nYes\ntool_call_id\nstring\nTool call that this message is responding to.\nYes\nchatCompletionRequestFunctionMessage\nName\nType\nDescription\nRequired\nDefault\nrole\nenum\nThe role of the messages author, in this case\nfunction\n.\nPossible values: function\nYes\ncontent\nstring\nThe contents of the function message.\nYes\nname\nstring\nThe name of the function to call.\nYes\nchatCompletionRequestSystemMessageContentPart\nThis component can be one of the following:\nchatCompletionRequestUserMessageContentPart\nThis component can be one of the following:\nchatCompletionRequestAssistantMessageContentPart\nThis component can be one of the following:\nchatCompletionRequestToolMessageContentPart\nThis component can be one of the following:\nchatCompletionRequestMessageContentPartText\nName\nType\nDescription\nRequired\nDefault\ntype\nenum\nThe type of the content part.\nPossible values: text\nYes\ntext\nstring\nThe text content.\nYes\nchatCompletionRequestMessageContentPartImage\nName\nType\nDescription\nRequired\nDefault\ntype\nenum\nThe type of the content part.\nPossible values: image_url\nYes\nimage_url\nobject\nYes\nProperties for image_url\nurl\nName\nType\nDescription\nDefault\nurl\nstring\nEither a URL of the image or the base64 encoded image data.\ndetail\nName\nType\nDescription\nDefault\ndetail\nstring\nSpecifies the detail level of the image. Learn more in the\nVision guide\n.\nauto\nchatCompletionRequestMessageContentPartRefusal\nName\nType\nDescription\nRequired\nDefault\ntype\nenum\nThe type of the content part.\nPossible values: refusal\nYes\nrefusal\nstring\nThe refusal message generated by the model.\nYes\nazureChatExtensionConfiguration\nA representation of configuration data for a single Azure OpenAI chat extension. This will be used by a chat\ncompletions request that should use Azure OpenAI chat extensions to augment the response behavior.\nThe use of this configuration is compatible only with Azure OpenAI.\nName\nType\nDescription\nRequired\nDefault\ntype\nazureChatExtensionType\nA representation of configuration data for a single Azure OpenAI chat extension. This will be used by a chat\ncompletions request that should use Azure OpenAI chat extensions to augment the response behavior.\nThe use of this configuration is compatible only with Azure OpenAI.\nYes\nazureChatExtensionType\nA representation of configuration data for a single Azure OpenAI chat extension. This will be used by a chat\ncompletions request that should use Azure OpenAI chat extensions to augment the response behavior.\nThe use of this configuration is compatible only with Azure OpenAI.\nDescription\n:   A representation of configuration data for a single Azure OpenAI chat extension. This will be used by a chat completions request that should use Azure OpenAI chat extensions to augment the response behavior. The use of this configuration is compatible only with Azure OpenAI.\nType\n: string\nDefault\n:\nEnum Name\n: AzureChatExtensionType\nEnum Values\n:\nValue\nDescription\nazure_search\nRepresents the use of Azure Search as an Azure OpenAI chat extension.\nazure_cosmos_db\nRepresents the use of Azure Cosmos DB as an Azure OpenAI chat extension.\nazureSearchChatExtensionConfiguration\nA specific representation of configurable options for Azure Search when using it as an Azure OpenAI chat\nextension.\nName\nType\nDescription\nRequired\nDefault\ntype\nazureChatExtensionType\nA representation of configuration data for a single Azure OpenAI chat extension. This will be used by a chat\ncompletions request that should use Azure OpenAI chat extensions to augment the response behavior.\nThe use of this configuration is compatible only with Azure OpenAI.\nYes\nparameters\nazureSearchChatExtensionParameters\nParameters for Azure Search when used as an Azure OpenAI chat extension.\nNo\nazureSearchChatExtensionParameters\nParameters for Azure Search when used as an Azure OpenAI chat extension.\nName\nType\nDescription\nRequired\nDefault\nauthentication\nonYourDataApiKeyAuthenticationOptions\nor\nonYourDataSystemAssignedManagedIdentityAuthenticationOptions\nor\nonYourDataUserAssignedManagedIdentityAuthenticationOptions\nYes\ntop_n_documents\ninteger\nThe configured top number of documents to feature for the configured query.\nNo\nin_scope\nboolean\nWhether queries should be restricted to use of indexed data.\nNo\nstrictness\ninteger\nThe configured strictness of the search relevance filtering. The higher of strictness, the higher of the precision but lower recall of the answer.\nNo\nrole_information\nstring\nGive the model instructions about how it should behave and any context it should reference when generating a response. You can describe the assistant's personality and tell it how to format responses. There's a 100 token limit for it, and it counts against the overall token limit.\nNo\nendpoint\nstring\nThe absolute endpoint path for the Azure Search resource to use.\nYes\nindex_name\nstring\nThe name of the index to use as available in the referenced Azure Search resource.\nYes\nfields_mapping\nazureSearchIndexFieldMappingOptions\nOptional settings to control how fields are processed when using a configured Azure Search resource.\nNo\nquery_type\nazureSearchQueryType\nThe type of Azure Search retrieval query that should be executed when using it as an Azure OpenAI chat extension.\nNo\nsemantic_configuration\nstring\nThe additional semantic configuration for the query.\nNo\nfilter\nstring\nSearch filter.\nNo\nembedding_dependency\nonYourDataEndpointVectorizationSource\nor\nonYourDataDeploymentNameVectorizationSource\nNo\nazureSearchIndexFieldMappingOptions\nOptional settings to control how fields are processed when using a configured Azure Search resource.\nName\nType\nDescription\nRequired\nDefault\ntitle_field\nstring\nThe name of the index field to use as a title.\nNo\nurl_field\nstring\nThe name of the index field to use as a URL.\nNo\nfilepath_field\nstring\nThe name of the index field to use as a filepath.\nNo\ncontent_fields\narray\nThe names of index fields that should be treated as content.\nNo\ncontent_fields_separator\nstring\nThe separator pattern that content fields should use.\nNo\nvector_fields\narray\nThe names of fields that represent vector data.\nNo\nazureSearchQueryType\nThe type of Azure Search retrieval query that should be executed when using it as an Azure OpenAI chat extension.\nDescription\n: The type of Azure Search retrieval query that should be executed when using it as an Azure OpenAI chat extension.\nType\n: string\nDefault\n:\nEnum Name\n: AzureSearchQueryType\nEnum Values\n:\nValue\nDescription\nsimple\nRepresents the default, simple query parser.\nsemantic\nRepresents the semantic query parser for advanced semantic modeling.\nvector\nRepresents vector search over computed data.\nvector_simple_hybrid\nRepresents a combination of the simple query strategy with vector data.\nvector_semantic_hybrid\nRepresents a combination of semantic search and vector data querying.\nazureCosmosDBChatExtensionConfiguration\nA specific representation of configurable options for Azure Cosmos DB when using it as an Azure OpenAI chat\nextension.\nName\nType\nDescription\nRequired\nDefault\ntype\nazureChatExtensionType\nA representation of configuration data for a single Azure OpenAI chat extension. This will be used by a chat\ncompletions request that should use Azure OpenAI chat extensions to augment the response behavior.\nThe use of this configuration is compatible only with Azure OpenAI.\nYes\nparameters\nazureCosmosDBChatExtensionParameters\nParameters to use when configuring Azure OpenAI On Your Data chat extensions when using Azure Cosmos DB for\nMongoDB vCore.\nNo\nazureCosmosDBChatExtensionParameters\nParameters to use when configuring Azure OpenAI On Your Data chat extensions when using Azure Cosmos DB for\nMongoDB vCore.\nName\nType\nDescription\nRequired\nDefault\nauthentication\nonYourDataConnectionStringAuthenticationOptions\nThe authentication options for Azure OpenAI On Your Data when using a connection string.\nYes\ntop_n_documents\ninteger\nThe configured top number of documents to feature for the configured query.\nNo\nin_scope\nboolean\nWhether queries should be restricted to use of indexed data.\nNo\nstrictness\ninteger\nThe configured strictness of the search relevance filtering. The higher of strictness, the higher of the precision but lower recall of the answer.\nNo\nrole_information\nstring\nGive the model instructions about how it should behave and any context it should reference when generating a response. You can describe the assistant's personality and tell it how to format responses. There's a 100 token limit for it, and it counts against the overall token limit.\nNo\ndatabase_name\nstring\nThe MongoDB vCore database name to use with Azure Cosmos DB.\nYes\ncontainer_name\nstring\nThe name of the Azure Cosmos DB resource container.\nYes\nindex_name\nstring\nThe MongoDB vCore index name to use with Azure Cosmos DB.\nYes\nfields_mapping\nazureCosmosDBFieldMappingOptions\nOptional settings to control how fields are processed when using a configured Azure Cosmos DB resource.\nYes\nembedding_dependency\nonYourDataEndpointVectorizationSource\nor\nonYourDataDeploymentNameVectorizationSource\nYes\nazureCosmosDBFieldMappingOptions\nOptional settings to control how fields are processed when using a configured Azure Cosmos DB resource.\nName\nType\nDescription\nRequired\nDefault\ntitle_field\nstring\nThe name of the index field to use as a title.\nNo\nurl_field\nstring\nThe name of the index field to use as a URL.\nNo\nfilepath_field\nstring\nThe name of the index field to use as a filepath.\nNo\ncontent_fields\narray\nThe names of index fields that should be treated as content.\nYes\ncontent_fields_separator\nstring\nThe separator pattern that content fields should use.\nNo\nvector_fields\narray\nThe names of fields that represent vector data.\nYes\nonYourDataAuthenticationOptions\nThe authentication options for Azure OpenAI On Your Data.\nName\nType\nDescription\nRequired\nDefault\ntype\nonYourDataAuthenticationType\nThe authentication types supported with Azure OpenAI On Your Data.\nYes\nonYourDataAuthenticationType\nThe authentication types supported with Azure OpenAI On Your Data.\nDescription\n: The authentication types supported with Azure OpenAI On Your Data.\nType\n: string\nDefault\n:\nEnum Name\n: OnYourDataAuthenticationType\nEnum Values\n:\nValue\nDescription\napi_key\nAuthentication via API key.\nconnection_string\nAuthentication via connection string.\nsystem_assigned_managed_identity\nAuthentication via system-assigned managed identity.\nuser_assigned_managed_identity\nAuthentication via user-assigned managed identity.\nonYourDataApiKeyAuthenticationOptions\nThe authentication options for Azure OpenAI On Your Data when using an API key.\nName\nType\nDescription\nRequired\nDefault\ntype\nonYourDataAuthenticationType\nThe authentication types supported with Azure OpenAI On Your Data.\nYes\nkey\nstring\nThe API key to use for authentication.\nNo\nonYourDataConnectionStringAuthenticationOptions\nThe authentication options for Azure OpenAI On Your Data when using a connection string.\nName\nType\nDescription\nRequired\nDefault\ntype\nonYourDataAuthenticationType\nThe authentication types supported with Azure OpenAI On Your Data.\nYes\nconnection_string\nstring\nThe connection string to use for authentication.\nNo\nonYourDataSystemAssignedManagedIdentityAuthenticationOptions\nThe authentication options for Azure OpenAI On Your Data when using a system-assigned managed identity.\nName\nType\nDescription\nRequired\nDefault\ntype\nonYourDataAuthenticationType\nThe authentication types supported with Azure OpenAI On Your Data.\nYes\nonYourDataUserAssignedManagedIdentityAuthenticationOptions\nThe authentication options for Azure OpenAI On Your Data when using a user-assigned managed identity.\nName\nType\nDescription\nRequired\nDefault\ntype\nonYourDataAuthenticationType\nThe authentication types supported with Azure OpenAI On Your Data.\nYes\nmanaged_identity_resource_id\nstring\nThe resource ID of the user-assigned managed identity to use for authentication.\nNo\nonYourDataVectorizationSource\nAn abstract representation of a vectorization source for Azure OpenAI On Your Data with vector search.\nName\nType\nDescription\nRequired\nDefault\ntype\nonYourDataVectorizationSourceType\nRepresents the available sources Azure OpenAI On Your Data can use to configure vectorization of data for use with\nvector search.\nYes\nonYourDataVectorizationSourceType\nRepresents the available sources Azure OpenAI On Your Data can use to configure vectorization of data for use with\nvector search.\nDescription\n: Represents the available sources Azure OpenAI On Your Data can use to configure vectorization of data for use with\nvector search.\nType\n: string\nDefault\n:\nEnum Name\n: OnYourDataVectorizationSourceType\nEnum Values\n:\nValue\nDescription\nendpoint\nRepresents vectorization performed by public service calls to an Azure OpenAI embedding model.\ndeployment_name\nRepresents an Ada model deployment name to use. This model deployment must be in the same Azure OpenAI resource, but\nOn Your Data will use this model deployment via an internal call rather than a public one, which enables vector\nsearch even in private networks.\nonYourDataDeploymentNameVectorizationSource\nThe details of a vectorization source, used by Azure OpenAI On Your Data when applying vector search, that is based\non an internal embeddings model deployment name in the same Azure OpenAI resource.\nName\nType\nDescription\nRequired\nDefault\ntype\nonYourDataVectorizationSourceType\nRepresents the available sources Azure OpenAI On Your Data can use to configure vectorization of data for use with\nvector search.\nYes\ndeployment_name\nstring\nSpecifies the name of the model deployment to use for vectorization. This model deployment must be in the same Azure OpenAI resource, but On Your Data will use this model deployment via an internal call rather than a public one, which enables vector search even in private networks.\nNo\nonYourDataEndpointVectorizationSource\nThe details of a vectorization source, used by Azure OpenAI On Your Data when applying vector search, that is based\non a public Azure OpenAI endpoint call for embeddings.\nName\nType\nDescription\nRequired\nDefault\ntype\nonYourDataVectorizationSourceType\nRepresents the available sources Azure OpenAI On Your Data can use to configure vectorization of data for use with\nvector search.\nYes\nauthentication\nonYourDataApiKeyAuthenticationOptions\nThe authentication options for Azure OpenAI On Your Data when using an API key.\nNo\nendpoint\nstring\nSpecifies the endpoint to use for vectorization. This endpoint must be in the same Azure OpenAI resource, but On Your Data will use this endpoint via an internal call rather than a public one, which enables vector search even in private networks.\nNo\nazureChatExtensionsMessageContext\nA representation of the additional context information available when Azure OpenAI chat extensions are involved\nin the generation of a corresponding chat completions response. This context information is only populated when\nusing an Azure OpenAI request configured to use a matching extension.\nName\nType\nDescription\nRequired\nDefault\ncitations\narray\nThe data source retrieval result, used to generate the assistant message in the response.\nNo\nintent\nstring\nThe detected intent from the chat history, used to pass to the next turn to carry over the context.\nNo\ncitation\ncitation information for a chat completions response message.\nName\nType\nDescription\nRequired\nDefault\ncontent\nstring\nThe content of the citation.\nYes\ntitle\nstring\nThe title of the citation.\nNo\nurl\nstring\nThe URL of the citation.\nNo\nfilepath\nstring\nThe file path of the citation.\nNo\nchunk_id\nstring\nThe chunk ID of the citation.\nNo\nchatCompletionMessageToolCall\nName\nType\nDescription\nRequired\nDefault\nid\nstring\nThe ID of the tool call.\nYes\ntype\ntoolCallType\nThe type of the tool call, in this case\nfunction\n.\nYes\nfunction\nobject\nThe function that the model called.\nYes\nProperties for function\nname\nName\nType\nDescription\nDefault\nname\nstring\nThe name of the function to call.\narguments\nName\nType\nDescription\nDefault\narguments\nstring\nThe arguments to call the function with, as generated by the model in JSON format. Note that the model doesn't always generate valid JSON, and may generate parameters not defined by your function schema. Validate the arguments in your code before calling your function.\ntoolCallType\nThe type of the tool call, in this case\nfunction\n.\nDescription\n: The type of the tool call, in this case\nfunction\n.\nType\n: string\nDefault\n:\nEnum Name\n: ToolCallType\nEnum Values\n:\nValue\nDescription\nfunction\nThe tool call type is function.\nchatCompletionRequestMessageTool\nName\nType\nDescription\nRequired\nDefault\ntool_call_id\nstring\nTool call that this message is responding to.\nNo\ncontent\nstring\nThe contents of the message.\nNo\nchatCompletionRequestMessageFunction\nName\nType\nDescription\nRequired\nDefault\nrole\nenum\nThe role of the messages author, in this case\nfunction\n.\nPossible values: function\nNo\nname\nstring\nThe contents of the message.\nNo\ncontent\nstring\nThe contents of the message.\nNo\ncreateChatCompletionResponse\nRepresents a chat completion response returned by model, based on the provided input.\nName\nType\nDescription\nRequired\nDefault\nid\nstring\nA unique identifier for the chat completion.\nYes\nprompt_filter_results\npromptFilterResults\nContent filtering results for zero or more prompts in the request. In a streaming request, results for different prompts may arrive at different times or in different orders.\nNo\nchoices\narray\nA list of chat completion choices. Can be more than one if\nn\nis greater than 1.\nYes\ncreated\ninteger\nThe Unix timestamp (in seconds) of when the chat completion was created.\nYes\nmodel\nstring\nThe model used for the chat completion.\nYes\nsystem_fingerprint\nstring\nThis fingerprint represents the backend configuration that the model runs with.\nCan be used in conjunction with the\nseed\nrequest parameter to understand when backend changes have been made that might impact determinism.\nNo\nobject\nenum\nThe object type, which is always\nchat.completion\n.\nPossible values: chat.completion\nYes\nusage\ncompletionUsage\nUsage statistics for the completion request.\nNo\ncreateChatCompletionStreamResponse\nRepresents a streamed chunk of a chat completion response returned by model, based on the provided input.\nName\nType\nDescription\nRequired\nDefault\nid\nstring\nA unique identifier for the chat completion. Each chunk has the same ID.\nYes\nchoices\narray\nA list of chat completion choices. Can contain more than one elements if\nn\nis greater than 1.\nYes\ncreated\ninteger\nThe Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.\nYes\nmodel\nstring\nThe model to generate the completion.\nYes\nsystem_fingerprint\nstring\nThis fingerprint represents the backend configuration that the model runs with.\nCan be used in conjunction with the\nseed\nrequest parameter to understand when backend changes have been made that might impact determinism.\nNo\nobject\nenum\nThe object type, which is always\nchat.completion.chunk\n.\nPossible values: chat.completion.chunk\nYes\nchatCompletionStreamResponseDelta\nA chat completion delta generated by streamed model responses.\nName\nType\nDescription\nRequired\nDefault\ncontent\nstring\nThe contents of the chunk message.\nNo\nfunction_call\nobject\nDeprecated and replaced by\ntool_calls\n. The name and arguments of a function that should be called, as generated by the model.\nNo\ntool_calls\narray\nNo\nrole\nenum\nThe role of the author of this message.\nPossible values: system, user, assistant, tool\nNo\nrefusal\nstring\nThe refusal message generated by the model.\nNo\nProperties for function_call\narguments\nName\nType\nDescription\nDefault\narguments\nstring\nThe arguments to call the function with, as generated by the model in JSON format. Note that the model doesn't always generate valid JSON, and may generate parameters not defined by your function schema. Validate the arguments in your code before calling your function.\nname\nName\nType\nDescription\nDefault\nname\nstring\nThe name of the function to call.\nchatCompletionMessageToolCallChunk\nName\nType\nDescription\nRequired\nDefault\nindex\ninteger\nYes\nid\nstring\nThe ID of the tool call.\nNo\ntype\nenum\nThe type of the tool. Currently, only\nfunction\nis supported.\nPossible values: function\nNo\nfunction\nobject\nNo\nProperties for function\nname\nName\nType\nDescription\nDefault\nname\nstring\nThe name of the function to call.\narguments\nName\nType\nDescription\nDefault\narguments\nstring\nThe arguments to call the function with, as generated by the model in JSON format. Note that the model doesn't always generate valid JSON, and may generate parameters not defined by your function schema. Validate the arguments in your code before calling your function.\nchatCompletionStreamOptions\nOptions for streaming response. Only set this when you set\nstream: true\n.\nName\nType\nDescription\nRequired\nDefault\ninclude_usage\nboolean\nIf set, an additional chunk will be streamed before the\ndata: [DONE]\nmessage. The\nusage\nfield on this chunk shows the token usage statistics for the entire request, and the\nchoices\nfield will always be an empty array. All other chunks will also include a\nusage\nfield, but with a null value.\nNo\nchatCompletionChoiceLogProbs\nLog probability information for the choice.\nName\nType\nDescription\nRequired\nDefault\ncontent\narray\nA list of message content tokens with log probability information.\nYes\nrefusal\narray\nA list of message refusal tokens with log probability information.\nNo\nchatCompletionTokenLogprob\nName\nType\nDescription\nRequired\nDefault\ntoken\nstring\nThe token.\nYes\nlogprob\nnumber\nThe log probability of this token.\nYes\nbytes\narray\nA list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be\nnull\nif there's no bytes representation for the token.\nYes\ntop_logprobs\narray\nList of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested\ntop_logprobs\nreturned.\nYes\nchatCompletionResponseMessage\nA chat completion message generated by the model.\nName\nType\nDescription\nRequired\nDefault\nrole\nchatCompletionResponseMessageRole\nThe role of the author of the response message.\nYes\nrefusal\nstring\nThe refusal message generated by the model.\nYes\ncontent\nstring\nThe contents of the message.\nYes\ntool_calls\narray\nThe tool calls generated by the model, such as function calls.\nNo\nfunction_call\nchatCompletionFunctionCall\nDeprecated and replaced by\ntool_calls\n. The name and arguments of a function that should be called, as generated by the model.\nNo\ncontext\nazureChatExtensionsMessageContext\nA representation of the additional context information available when Azure OpenAI chat extensions are involved\nin the generation of a corresponding chat completions response. This context information is only populated when\nusing an Azure OpenAI request configured to use a matching extension.\nNo\nchatCompletionResponseMessageRole\nThe role of the author of the response message.\nDescription\n: The role of the author of the response message.\nType\n: string\nDefault\n:\nEnum Values\n:\nassistant\nchatCompletionToolChoiceOption\nControls which (if any) tool is called by the model.\nnone\nmeans the model won't call any tool and instead generates a message.\nauto\nmeans the model can pick between generating a message or calling one or more tools.\nrequired\nmeans the model must call one or more tools. Specifying a particular tool via\n{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}\nforces the model to call that tool.\nnone\nis the default when no tools are present.\nauto\nis the default if tools are present.\nThis component can be one of the following:\nchatCompletionNamedToolChoice\nSpecifies a tool the model should use. Use to force the model to call a specific function.\nName\nType\nDescription\nRequired\nDefault\ntype\nenum\nThe type of the tool. Currently, only\nfunction\nis supported.\nPossible values: function\nYes\nfunction\nobject\nYes\nProperties for function\nname\nName\nType\nDescription\nDefault\nname\nstring\nThe name of the function to call.\nParallelToolCalls\nWhether to enable parallel function calling during tool use.\nNo properties defined for this component.\nchatCompletionMessageToolCalls\nThe tool calls generated by the model, such as function calls.\nNo properties defined for this component.\nchatCompletionFunctionCall\nDeprecated and replaced by\ntool_calls\n. The name and arguments of a function that should be called, as generated by the model.\nName\nType\nDescription\nRequired\nDefault\nname\nstring\nThe name of the function to call.\nYes\narguments\nstring\nThe arguments to call the function with, as generated by the model in JSON format. Note that the model doesn't always generate valid JSON, and may generate parameters not defined by your function schema. Validate the arguments in your code before calling your function.\nYes\ncompletionUsage\nUsage statistics for the completion request.\nName\nType\nDescription\nRequired\nDefault\nprompt_tokens\ninteger\nNumber of tokens in the prompt.\nYes\ncompletion_tokens\ninteger\nNumber of tokens in the generated completion.\nYes\ntotal_tokens\ninteger\nTotal number of tokens used in the request (prompt + completion).\nYes\ncompletion_tokens_details\nobject\nBreakdown of tokens used in a completion.\nNo\nProperties for completion_tokens_details\nreasoning_tokens\nName\nType\nDescription\nDefault\nreasoning_tokens\ninteger\nTokens generated by the model for reasoning.\nchatCompletionTool\nName\nType\nDescription\nRequired\nDefault\ntype\nenum\nThe type of the tool. Currently, only\nfunction\nis supported.\nPossible values: function\nYes\nfunction\nFunctionObject\nYes\nFunctionParameters\nThe parameters the functions accepts, described as a JSON Schema object.\nSee the guide\nfor examples, and the\nJSON Schema reference\nfor documentation about the format.\nOmitting\nparameters\ndefines a function with an empty parameter list.\nNo properties defined for this component.\nFunctionObject\nName\nType\nDescription\nRequired\nDefault\ndescription\nstring\nA description of what the function does, used by the model to choose when and how to call the function.\nNo\nname\nstring\nThe name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.\nYes\nparameters\nFunctionParameters\nThe parameters the functions accepts, described as a JSON Schema object.\nSee the guide\nfor examples, and the\nJSON Schema reference\nfor documentation about the format.\nOmitting\nparameters\ndefines a function with an empty parameter list.\nNo\nstrict\nboolean\nWhether to enable strict schema adherence when generating the function call. If set to true, the model will follow the exact schema defined in the\nparameters\nfield. Only a subset of JSON Schema is supported when\nstrict\nis\ntrue\n.\nNo\nFalse\nResponseFormatText\nName\nType\nDescription\nRequired\nDefault\ntype\nenum\nThe type of response format being defined:\ntext\nPossible values: text\nYes\nResponseFormatJsonObject\nName\nType\nDescription\nRequired\nDefault\ntype\nenum\nThe type of response format being defined:\njson_object\nPossible values: json_object\nYes\nResponseFormatJsonSchemaSchema\nThe schema for the response format, described as a JSON Schema object.\nNo properties defined for this component.\nResponseFormatJsonSchema\nName\nType\nDescription\nRequired\nDefault\ntype\nenum\nThe type of response format being defined:\njson_schema\nPossible values: json_schema\nYes\njson_schema\nobject\nYes\nProperties for json_schema\ndescription\nName\nType\nDescription\nDefault\ndescription\nstring\nA description of what the response format is for, used by the model to determine how to respond in the format.\nname\nName\nType\nDescription\nDefault\nname\nstring\nThe name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.\nschema\nName\nType\nDescription\nDefault\nschema\nResponseFormatJsonSchemaSchema\nThe schema for the response format, described as a JSON Schema object.\nstrict\nName\nType\nDescription\nDefault\nstrict\nboolean\nWhether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the\nschema\nfield. Only a subset of JSON Schema is supported when\nstrict\nis\ntrue\n.\nFalse\nchatCompletionChoiceCommon\nName\nType\nDescription\nRequired\nDefault\nindex\ninteger\nNo\nfinish_reason\nstring\nNo\ncreateTranslationRequest\nTranslation request.\nName\nType\nDescription\nRequired\nDefault\nfile\nstring\nThe audio file to translate.\nYes\nprompt\nstring\nAn optional text to guide the model's style or continue a previous audio segment. The prompt should be in English.\nNo\nresponse_format\naudioResponseFormat\nDefines the format of the output.\nNo\ntemperature\nnumber\nThe sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.\nNo\n0\naudioResponse\nTranslation or transcription response when response_format was json\nName\nType\nDescription\nRequired\nDefault\ntext\nstring\nTranslated or transcribed text.\nYes\naudioVerboseResponse\nTranslation or transcription response when response_format was verbose_json\nName\nType\nDescription\nRequired\nDefault\ntext\nstring\nTranslated or transcribed text.\nYes\ntask\nstring\nType of audio task.\nNo\nlanguage\nstring\nLanguage.\nNo\nduration\nnumber\nDuration.\nNo\nsegments\narray\nNo\naudioResponseFormat\nDefines the format of the output.\nDescription\n: Defines the format of the output.\nType\n: string\nDefault\n:\nEnum Values\n:\njson\ntext\nsrt\nverbose_json\nvtt\ncreateTranscriptionRequest\nTranscription request.\nName\nType\nDescription\nRequired\nDefault\nfile\nstring\nThe audio file object to transcribe.\nYes\nprompt\nstring\nAn optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.\nNo\nresponse_format\naudioResponseFormat\nDefines the format of the output.\nNo\ntemperature\nnumber\nThe sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.\nNo\n0\nlanguage\nstring\nThe language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency.\nNo\naudioSegment\nTranscription or translation segment.\nName\nType\nDescription\nRequired\nDefault\nid\ninteger\nSegment identifier.\nNo\nseek\nnumber\nOffset of the segment.\nNo\nstart\nnumber\nSegment start offset.\nNo\nend\nnumber\nSegment end offset.\nNo\ntext\nstring\nSegment text.\nNo\ntokens\narray\nTokens of the text.\nNo\ntemperature\nnumber\nTemperature.\nNo\navg_logprob\nnumber\nAverage log probability.\nNo\ncompression_ratio\nnumber\nCompression ratio.\nNo\nno_speech_prob\nnumber\nProbability of\nno speech\n.\nNo\nimageQuality\nThe quality of the image that will be generated.\nDescription\n: The quality of the image that will be generated.\nType\n: string\nDefault\n: standard\nEnum Name\n: Quality\nEnum Values\n:\nValue\nDescription\nstandard\nStandard quality creates images with standard quality.\nhd\nHD quality creates images with finer details and greater consistency across the image.\nimagesResponseFormat\nThe format in which the generated images are returned.\nDescription\n: The format in which the generated images are returned.\nType\n: string\nDefault\n: url\nEnum Name\n: ImagesResponseFormat\nEnum Values\n:\nValue\nDescription\nurl\nThe URL that provides temporary access to download the generated images.\nb64_json\nThe generated images are returned as base64 encoded string.\nimageSize\nThe size of the generated images.\nDescription\n: The size of the generated images.\nType\n: string\nDefault\n: 1024x1024\nEnum Name\n: Size\nEnum Values\n:\nValue\nDescription\n1792x1024\nThe desired size of the generated image is 1792x1024 pixels.\n1024x1792\nThe desired size of the generated image is 1024x1792 pixels.\n1024x1024\nThe desired size of the generated image is 1024x1024 pixels.\nimageStyle\nThe style of the generated images.\nDescription\n: The style of the generated images.\nType\n: string\nDefault\n: vivid\nEnum Name\n: Style\nEnum Values\n:\nValue\nDescription\nvivid\nVivid creates images that are hyper-realistic and dramatic.\nnatural\nNatural creates images that are more natural and less hyper-realistic.\nimageGenerationsRequest\nName\nType\nDescription\nRequired\nDefault\nprompt\nstring\nA text description of the desired image(s). The maximum length is 4,000 characters.\nYes\nn\ninteger\nThe number of images to generate.\nNo\n1\nsize\nimageSize\nThe size of the generated images.\nNo\n1024x1024\nresponse_format\nimagesResponseFormat\nThe format in which the generated images are returned.\nNo\nurl\nuser\nstring\nA unique identifier representing your end-user, which can help to monitor and detect abuse.\nNo\nquality\nimageQuality\nThe quality of the image that will be generated.\nNo\nstandard\nstyle\nimageStyle\nThe style of the generated images.\nNo\nvivid\ngenerateImagesResponse\nName\nType\nDescription\nRequired\nDefault\ncreated\ninteger\nThe unix timestamp when the operation was created.\nYes\ndata\narray\nThe result data of the operation, if successful\nYes\nimageResult\nThe image url or encoded image if successful, and an error otherwise.\nName\nType\nDescription\nRequired\nDefault\nurl\nstring\nThe image url.\nNo\nb64_json\nstring\nThe base64 encoded image\nNo\ncontent_filter_results\ndalleContentFilterResults\nInformation about the content filtering results.\nNo\nrevised_prompt\nstring\nThe prompt that was used to generate the image, if there was any revision to the prompt.\nNo\nprompt_filter_results\ndalleFilterResults\nInformation about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about jailbreak content and profanity, if it has been detected, and if it has been filtered or not. And information about customer blocklist, if it has been filtered and its id.\nNo\nCompletions extensions\nCompletions extensions aren't part of the latest GA version of the Azure OpenAI data plane inference spec.\nChatmessage\nThe Chat message object isn't part of the latest GA version of the Azure OpenAI data plane inference spec.\nText to speech (Preview)\nIs not currently part of the latest Azure OpenAI GA version of the Azure OpenAI data plane inference spec. Refer to the latest\npreview\nversion for this capability.\nNext steps\nLearn about\nModels, and fine-tuning with the REST API\n.\nLearn more about the\nunderlying models that power Azure OpenAI\n.\nFeedback\nWas this page helpful?\nYes\nNo\nNo\nNeed help with this topic?\nWant to try using Ask Learn to clarify or guide you through this topic?\nAsk Learn\nAsk Learn\nSuggest a fix?\nWas this page helpful?\nNeed help with this topic?\nWant to try using Ask Learn to clarify or guide you through this topic?\nAsk Learn\nAsk Learn\nSuggest a fix?\nAdditional resources\nIn this article\nWas this page helpful?\nYes\nNo\nNo\nNeed help with this topic?\nWant to try using Ask Learn to clarify or guide you through this topic?\nAsk Learn\nAsk Learn\nSuggest a fix?\nWas this page helpful?\nNeed help with this topic?\nWant to try using Ask Learn to clarify or guide you through this topic?\nAsk Learn\nAsk Learn\nSuggest a fix?\nen-us\nYour Privacy Choices\nTheme\nLight\nDark\nHigh contrast\nAI Disclaimer\nPrevious Versions\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2025",
    "fetched_at": "2025-10-09T03:06:39.753689Z"
  },
  "https://docs.aws.amazon.com/bedrock/latest/userguide/api-methods.html": {
    "provider": "AWS Bedrock",
    "title": "AWS Bedrock API Methods",
    "hash": "eab537d8b5f4df93cb084304f402372e94e77256b132c2454f37645dcc463a6e",
    "content": "Amazon Bedrock",
    "fetched_at": "2025-09-12T03:00:47.262224Z"
  }
}