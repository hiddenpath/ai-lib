/// AI-lib quickstart example
use ai_lib::prelude::*;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸš€ AI-lib v0.5.0 Quickstart");
    println!("============================");

    // ğŸ¯ Simplest v0.5.0 usage - create client with a model ID
    // Configurations are loaded from the embedded aimanifest.yaml
    println!("\nğŸ“‹ Model-Driven simplified usage:");
    let client = AiClientBuilder::new(Provider::OpenAI)
        .with_model("gpt-4o")
        .build()?;
    println!(
        "âœ… Client created via Manifest for model: {}",
        client.default_chat_model()
    );

    // ğŸ”§ Customize behavior while remaining manifest-compatible
    println!("\nğŸ“‹ Advanced Client Configuration:");
    let _custom_client = AiClientBuilder::new(Provider::OpenAI)
        .with_model("gpt-4o")
        .with_timeout(std::time::Duration::from_secs(30))
        .build()?;
    println!("âœ… Custom client created successfully!");

    // ğŸ“ Create a structured chat request
    println!("\nğŸ“‹ Creating a chat request:");
    let request = ChatCompletionRequest::new(
        "gpt-4o".to_string(),
        vec![Message::user("Tell me one interesting fact about Rust.")],
    );
    println!("âœ… Request built for model: {}", request.model);

    // ğŸŒ Multi-provider support (all driven by local or embedded YAML)
    println!("\nğŸ“‹ Switching Providers (Zero code change needed):");

    // Switch to Groq (Llama 3) via manifest
    let groq_client = AiClientBuilder::new(Provider::Groq)
        .with_model("llama-3.3-70b-versatile")
        .build()?;
    println!(
        "âœ… Groq client (Manifest-driven) created: {}",
        groq_client.default_chat_model()
    );

    // Switch to Mistral via manifest
    let mistral_client = AiClientBuilder::new(Provider::Mistral)
        .with_model("mistral-large-latest")
        .build()?;
    println!(
        "âœ… Mistral client (Manifest-driven) created: {}",
        mistral_client.default_chat_model()
    );

    println!("\nğŸ‰ Quickstart completed!");
    println!("\nğŸ’¡ Key points for v0.5.0:");
    println!("   1. Manifest-First: No hardcoded provider logic; all details are in YAML.");
    println!("   2. Model-Centric: Use .with_model() to pick specific capabilities.");
    println!("   3. Unified SSE: Streaming is handled by operators, not provider branches.");
    println!("   4. Zero-Code: Add new providers to aimanifest.yaml without rebuilding the SDK.");

    Ok(())
}
