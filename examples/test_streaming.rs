use ai_lib::types::common::Content;
use ai_lib::{AiClient, ChatCompletionRequest, Message, Provider, Role};
use futures::StreamExt;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸŒŠ Streaming Response Test");
    println!("================");

    // Check Groq API key
    if std::env::var("GROQ_API_KEY").is_err() {
        println!("âŒ GROQ_API_KEY not set");
        return Ok(());
    }

    // Create Groq client
    let client = AiClient::new(Provider::Groq)?;
    println!("âœ… Groq client created successfully");

    // Choose model from env or fallback to a sensible default
    let model = std::env::var("GROQ_MODEL").unwrap_or_else(|_| "llama-3.1-8b-instant".to_string());

    // Create streaming request
    let request = ChatCompletionRequest::new(
        model,
        vec![Message {
            role: Role::User,
            content: Content::Text(
                "Please write a short poem about AI in exactly 4 lines.".to_string(),
            ),
            function_call: None,
        }],
    )
    .with_max_tokens(100)
    .with_temperature(0.7);

    println!("\nğŸ“¤ Sending streaming request...");
    println!("   Model: {}", request.model);
    println!("   Message: {}", request.messages[0].content.as_text());

    // Get streaming response
    match client.chat_completion_stream(request).await {
        Ok(mut stream) => {
            println!("\nğŸŒŠ Starting to receive streaming response:");
            println!("{}", "â”€".repeat(50));

            let mut full_content = String::new();
            let mut chunk_count = 0;

            while let Some(result) = stream.next().await {
                match result {
                    Ok(chunk) => {
                        chunk_count += 1;

                        if let Some(choice) = chunk.choices.first() {
                            if let Some(content) = &choice.delta.content {
                                print!("{}", content);
                                full_content.push_str(content);

                                // Flush output
                                use std::io::{self, Write};
                                io::stdout().flush().unwrap();
                            }

                            // Check if completed
                            if choice.finish_reason.is_some() {
                                println!("\n{}", "â”€".repeat(50));
                                println!("âœ… Streaming response completed!");
                                println!("   Finish reason: {:?}", choice.finish_reason);
                                break;
                            }
                        }
                    }
                    Err(e) => {
                        println!("\nâŒ Streaming response error: {}", e);
                        break;
                    }
                }
            }

            println!("\nğŸ“Š Streaming response statistics:");
            println!("   Chunk count: {}", chunk_count);
            println!("   Total content length: {} characters", full_content.len());
            println!("   Complete content: \"{}\"", full_content.trim());
        }
        Err(e) => {
            println!("âŒ Streaming request failed: {}", e);
        }
    }

    println!("\nğŸ’¡ Advantages of streaming responses:");
    println!("   â€¢ Real-time content generation display");
    println!("   â€¢ Better user experience");
    println!("   â€¢ Can stop generation early");
    println!("   â€¢ Suitable for long text generation");

    Ok(())
}
