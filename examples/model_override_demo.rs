use ai_lib::{AiClient, AiClientBuilder, Provider, ModelOptions};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // æ£€æŸ¥ç¯å¢ƒå˜é‡
    if std::env::var("GROQ_API_KEY").is_err() {
        println!("âŒ è¯·è®¾ç½® GROQ_API_KEY ç¯å¢ƒå˜é‡");
        println!("   ä¾‹å¦‚: export GROQ_API_KEY=your_api_key_here");
        return Ok(());
    }
    
    println!("ğŸš€ æ¨¡å‹è¦†ç›–åŠŸèƒ½æ¼”ç¤º");
    println!("==================");
    println!();
    
    // 1. åŸºç¡€ç”¨æ³• - ä¿æŒåŸæœ‰ç®€æ´æ€§
    println!("ğŸ“‹ 1. åŸºç¡€ç”¨æ³• - ä½¿ç”¨é»˜è®¤æ¨¡å‹");
    let reply = AiClient::quick_chat_text(Provider::Groq, "Hello!").await?;
    println!("   âœ… å“åº”: {}", reply);
    println!();
    
    // 2. æ˜¾å¼æŒ‡å®šæ¨¡å‹
    println!("ğŸ“‹ 2. æ˜¾å¼æŒ‡å®šæ¨¡å‹");
    let reply = AiClient::quick_chat_text_with_model(
        Provider::Groq, 
        "Hello!", 
        "llama-3.1-8b-instant"
    ).await?;
    println!("   âœ… å“åº”: {}", reply);
    println!();
    
    // 3. ä½¿ç”¨ ModelOptions
    println!("ğŸ“‹ 3. ä½¿ç”¨ ModelOptions");
    let options = ModelOptions::default()
        .with_chat_model("llama-3.1-8b-instant");
    
    let reply = AiClient::quick_chat_text_with_options(
        Provider::Groq, 
        "Hello!", 
        options
    ).await?;
    println!("   âœ… å“åº”: {}", reply);
    println!();
    
    // 4. AiClientBuilder è‡ªå®šä¹‰é»˜è®¤æ¨¡å‹
    println!("ğŸ“‹ 4. AiClientBuilder è‡ªå®šä¹‰é»˜è®¤æ¨¡å‹");
    let client = AiClientBuilder::new(Provider::Groq)
        .with_default_chat_model("llama-3.1-8b-instant")
        .build()?;
    
    let request = client.build_simple_request("Hello!");
    println!("   ä½¿ç”¨æ¨¡å‹: {}", request.model);
    let response = client.chat_completion(request).await?;
    match &response.choices[0].message.content {
        ai_lib::types::common::Content::Text(text) => {
            println!("   âœ… å“åº”: {}", text);
        }
        _ => println!("   âœ… å“åº”: {:?}", response.choices[0].message.content),
    }
    println!();
    
    // 5. æ˜¾å¼æŒ‡å®šæ¨¡å‹çš„ build_simple_request
    println!("ğŸ“‹ 5. æ˜¾å¼æŒ‡å®šæ¨¡å‹çš„ build_simple_request");
    let client = AiClient::new(Provider::Groq)?;
    let request = client.build_simple_request_with_model(
        "Hello!",
        "llama-3.1-8b-instant"
    );
    println!("   ä½¿ç”¨æ¨¡å‹: {}", request.model);
    let response = client.chat_completion(request).await?;
    match &response.choices[0].message.content {
        ai_lib::types::common::Content::Text(text) => {
            println!("   âœ… å“åº”: {}", text);
        }
        _ => println!("   âœ… å“åº”: {:?}", response.choices[0].message.content),
    }
    println!();
    
    println!("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼");
    println!("=============");
    println!("âœ… æ‰€æœ‰æ¨¡å‹è¦†ç›–åŠŸèƒ½éƒ½æ­£å¸¸å·¥ä½œ");
    println!("âœ… å‘åå…¼å®¹æ€§å¾—åˆ°ä¿è¯");
    println!("âœ… æä¾›äº†çµæ´»çš„æ¨¡å‹æŒ‡å®šæ–¹å¼");
    
    Ok(())
}
