use ai_lib::{AiClient, Provider, ChatCompletionRequest, Message, Role};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ¤– OpenAI ä¸“é¡¹æµ‹è¯•");
    println!("==================");
    
    // æ£€æŸ¥OpenAI APIå¯†é’¥
    match std::env::var("OPENAI_API_KEY") {
        Ok(key) => {
            let masked = format!("{}...{}", &key[..8], &key[key.len()-4..]);
            println!("ğŸ”‘ OpenAI API Key: {}", masked);
        }
        Err(_) => {
            println!("âŒ æœªè®¾ç½®OPENAI_API_KEY");
            return Ok(());
        }
    }
    
    // åˆ›å»ºOpenAIå®¢æˆ·ç«¯
    println!("\nğŸ“¡ åˆ›å»ºOpenAIå®¢æˆ·ç«¯...");
    let client = match AiClient::new(Provider::OpenAI) {
        Ok(client) => {
            println!("âœ… å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ");
            client
        }
        Err(e) => {
            println!("âŒ å®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {}", e);
            return Ok(());
        }
    };
    
    // æµ‹è¯•æ¨¡å‹åˆ—è¡¨
    println!("\nğŸ“‹ è·å–æ¨¡å‹åˆ—è¡¨...");
    match client.list_models().await {
        Ok(models) => {
            println!("âœ… æˆåŠŸè·å– {} ä¸ªæ¨¡å‹", models.len());
            
            // æ˜¾ç¤ºGPTæ¨¡å‹
            let gpt_models: Vec<_> = models.iter()
                .filter(|m| m.contains("gpt"))
                .take(5)
                .collect();
            println!("   GPTæ¨¡å‹: {:?}", gpt_models);
        }
        Err(e) => {
            println!("âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {}", e);
            return Ok(());
        }
    }
    
    // æµ‹è¯•èŠå¤©å®Œæˆ
    println!("\nğŸ’¬ æµ‹è¯•èŠå¤©å®Œæˆ...");
    let request = ChatCompletionRequest::new(
        "gpt-3.5-turbo".to_string(),
        vec![Message {
            role: Role::User,
            content: "Say 'Hello from OpenAI!' in exactly those words.".to_string(),
        }],
    ).with_max_tokens(20)
     .with_temperature(0.0);  // ä½¿ç”¨0æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
    
    match client.chat_completion(request).await {
        Ok(response) => {
            println!("âœ… èŠå¤©å®ŒæˆæˆåŠŸ!");
            println!("   æ¨¡å‹: {}", response.model);
            println!("   å“åº”: '{}'", response.choices[0].message.content);
            println!("   Tokenä½¿ç”¨: {} (prompt: {}, completion: {})", 
                response.usage.total_tokens,
                response.usage.prompt_tokens,
                response.usage.completion_tokens
            );
            println!("   å®ŒæˆåŸå› : {:?}", response.choices[0].finish_reason);
        }
        Err(e) => {
            println!("âŒ èŠå¤©å®Œæˆå¤±è´¥: {}", e);
            
            // åˆ†æé”™è¯¯ç±»å‹
            let error_str = e.to_string();
            if error_str.contains("400") {
                println!("   â†’ è¿™æ˜¯è¯·æ±‚æ ¼å¼é”™è¯¯");
            } else if error_str.contains("401") {
                println!("   â†’ è¿™æ˜¯è®¤è¯é”™è¯¯ï¼Œæ£€æŸ¥APIå¯†é’¥");
            } else if error_str.contains("429") {
                println!("   â†’ è¿™æ˜¯é€Ÿç‡é™åˆ¶é”™è¯¯");
            } else if error_str.contains("500") {
                println!("   â†’ è¿™æ˜¯æœåŠ¡å™¨é”™è¯¯");
            }
        }
    }
    
    println!("\nğŸ¯ OpenAIæµ‹è¯•å®Œæˆ!");
    
    Ok(())
}